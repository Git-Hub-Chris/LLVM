// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 5
// RUN: %clang_cc1 -triple arm64-none-linux-gnu -target-feature +neon \
// RUN:  -disable-O0-optnone -emit-llvm -o - %s \
// RUN: | opt -S -passes=mem2reg,instcombine | FileCheck %s

// REQUIRES: aarch64-registered-target || arm-registered-target

#include <arm_neon.h>

// CHECK-LABEL: define dso_local <16 x i8> @test_vld1q_dup_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <16 x i8> poison, i8 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <16 x i8> [[TMP1]], <16 x i8> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    ret <16 x i8> [[LANE]]
//
uint8x16_t test_vld1q_dup_u8(uint8_t  *a) {
  return vld1q_dup_u8(a);
}

// CHECK-LABEL: define dso_local <8 x i16> @test_vld1q_dup_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <8 x i16> poison, i16 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <8 x i16> [[TMP1]], <8 x i16> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    ret <8 x i16> [[LANE]]
//
uint16x8_t test_vld1q_dup_u16(uint16_t  *a) {
  return vld1q_dup_u16(a);
}

// CHECK-LABEL: define dso_local <4 x i32> @test_vld1q_dup_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x i32> poison, i32 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <4 x i32> [[TMP1]], <4 x i32> poison, <4 x i32> zeroinitializer
// CHECK-NEXT:    ret <4 x i32> [[LANE]]
//
uint32x4_t test_vld1q_dup_u32(uint32_t  *a) {
  return vld1q_dup_u32(a);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_vld1q_dup_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <2 x i64> [[TMP1]], <2 x i64> poison, <2 x i32> zeroinitializer
// CHECK-NEXT:    ret <2 x i64> [[LANE]]
//
uint64x2_t test_vld1q_dup_u64(uint64_t  *a) {
  return vld1q_dup_u64(a);
}

// CHECK-LABEL: define dso_local <16 x i8> @test_vld1q_dup_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <16 x i8> poison, i8 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <16 x i8> [[TMP1]], <16 x i8> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    ret <16 x i8> [[LANE]]
//
int8x16_t test_vld1q_dup_s8(int8_t  *a) {
  return vld1q_dup_s8(a);
}

// CHECK-LABEL: define dso_local <8 x i16> @test_vld1q_dup_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <8 x i16> poison, i16 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <8 x i16> [[TMP1]], <8 x i16> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    ret <8 x i16> [[LANE]]
//
int16x8_t test_vld1q_dup_s16(int16_t  *a) {
  return vld1q_dup_s16(a);
}

// CHECK-LABEL: define dso_local <4 x i32> @test_vld1q_dup_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x i32> poison, i32 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <4 x i32> [[TMP1]], <4 x i32> poison, <4 x i32> zeroinitializer
// CHECK-NEXT:    ret <4 x i32> [[LANE]]
//
int32x4_t test_vld1q_dup_s32(int32_t  *a) {
  return vld1q_dup_s32(a);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_vld1q_dup_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <2 x i64> [[TMP1]], <2 x i64> poison, <2 x i32> zeroinitializer
// CHECK-NEXT:    ret <2 x i64> [[LANE]]
//
int64x2_t test_vld1q_dup_s64(int64_t  *a) {
  return vld1q_dup_s64(a);
}

// CHECK-LABEL: define dso_local <8 x half> @test_vld1q_dup_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <8 x half> poison, half [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <8 x half> [[TMP1]], <8 x half> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    ret <8 x half> [[LANE]]
//
float16x8_t test_vld1q_dup_f16(float16_t  *a) {
  return vld1q_dup_f16(a);
}

// CHECK-LABEL: define dso_local <4 x float> @test_vld1q_dup_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x float> poison, float [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <4 x float> [[TMP1]], <4 x float> poison, <4 x i32> zeroinitializer
// CHECK-NEXT:    ret <4 x float> [[LANE]]
//
float32x4_t test_vld1q_dup_f32(float32_t  *a) {
  return vld1q_dup_f32(a);
}

// CHECK-LABEL: define dso_local <2 x double> @test_vld1q_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load double, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x double> poison, double [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <2 x double> [[TMP1]], <2 x double> poison, <2 x i32> zeroinitializer
// CHECK-NEXT:    ret <2 x double> [[LANE]]
//
float64x2_t test_vld1q_dup_f64(float64_t  *a) {
  return vld1q_dup_f64(a);
}

// CHECK-LABEL: define dso_local <16 x i8> @test_vld1q_dup_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <16 x i8> poison, i8 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <16 x i8> [[TMP1]], <16 x i8> poison, <16 x i32> zeroinitializer
// CHECK-NEXT:    ret <16 x i8> [[LANE]]
//
poly8x16_t test_vld1q_dup_p8(poly8_t  *a) {
  return vld1q_dup_p8(a);
}

// CHECK-LABEL: define dso_local <8 x i16> @test_vld1q_dup_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <8 x i16> poison, i16 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <8 x i16> [[TMP1]], <8 x i16> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    ret <8 x i16> [[LANE]]
//
poly16x8_t test_vld1q_dup_p16(poly16_t  *a) {
  return vld1q_dup_p16(a);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_vld1q_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <2 x i64> [[TMP1]], <2 x i64> poison, <2 x i32> zeroinitializer
// CHECK-NEXT:    ret <2 x i64> [[LANE]]
//
poly64x2_t test_vld1q_dup_p64(poly64_t  *a) {
  return vld1q_dup_p64(a);
}

// CHECK-LABEL: define dso_local <8 x i8> @test_vld1_dup_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <8 x i8> poison, i8 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <8 x i8> [[TMP1]], <8 x i8> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    ret <8 x i8> [[LANE]]
//
uint8x8_t test_vld1_dup_u8(uint8_t  *a) {
  return vld1_dup_u8(a);
}

// CHECK-LABEL: define dso_local <4 x i16> @test_vld1_dup_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x i16> poison, i16 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> poison, <4 x i32> zeroinitializer
// CHECK-NEXT:    ret <4 x i16> [[LANE]]
//
uint16x4_t test_vld1_dup_u16(uint16_t  *a) {
  return vld1_dup_u16(a);
}

// CHECK-LABEL: define dso_local <2 x i32> @test_vld1_dup_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x i32> poison, i32 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> poison, <2 x i32> zeroinitializer
// CHECK-NEXT:    ret <2 x i32> [[LANE]]
//
uint32x2_t test_vld1_dup_u32(uint32_t  *a) {
  return vld1_dup_u32(a);
}

// CHECK-LABEL: define dso_local <1 x i64> @test_vld1_dup_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <1 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x i64> [[TMP1]]
//
uint64x1_t test_vld1_dup_u64(uint64_t  *a) {
  return vld1_dup_u64(a);
}

// CHECK-LABEL: define dso_local <8 x i8> @test_vld1_dup_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <8 x i8> poison, i8 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <8 x i8> [[TMP1]], <8 x i8> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    ret <8 x i8> [[LANE]]
//
int8x8_t test_vld1_dup_s8(int8_t  *a) {
  return vld1_dup_s8(a);
}

// CHECK-LABEL: define dso_local <4 x i16> @test_vld1_dup_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x i16> poison, i16 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> poison, <4 x i32> zeroinitializer
// CHECK-NEXT:    ret <4 x i16> [[LANE]]
//
int16x4_t test_vld1_dup_s16(int16_t  *a) {
  return vld1_dup_s16(a);
}

// CHECK-LABEL: define dso_local <2 x i32> @test_vld1_dup_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x i32> poison, i32 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <2 x i32> [[TMP1]], <2 x i32> poison, <2 x i32> zeroinitializer
// CHECK-NEXT:    ret <2 x i32> [[LANE]]
//
int32x2_t test_vld1_dup_s32(int32_t  *a) {
  return vld1_dup_s32(a);
}

// CHECK-LABEL: define dso_local <1 x i64> @test_vld1_dup_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <1 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x i64> [[TMP1]]
//
int64x1_t test_vld1_dup_s64(int64_t  *a) {
  return vld1_dup_s64(a);
}

// CHECK-LABEL: define dso_local <4 x half> @test_vld1_dup_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x half> poison, half [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <4 x half> [[TMP1]], <4 x half> poison, <4 x i32> zeroinitializer
// CHECK-NEXT:    ret <4 x half> [[LANE]]
//
float16x4_t test_vld1_dup_f16(float16_t  *a) {
  return vld1_dup_f16(a);
}

// CHECK-LABEL: define dso_local <2 x float> @test_vld1_dup_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[A]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <2 x float> poison, float [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <2 x float> [[TMP1]], <2 x float> poison, <2 x i32> zeroinitializer
// CHECK-NEXT:    ret <2 x float> [[LANE]]
//
float32x2_t test_vld1_dup_f32(float32_t  *a) {
  return vld1_dup_f32(a);
}

// CHECK-LABEL: define dso_local <1 x double> @test_vld1_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load double, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <1 x double> poison, double [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x double> [[TMP1]]
//
float64x1_t test_vld1_dup_f64(float64_t  *a) {
  return vld1_dup_f64(a);
}

// CHECK-LABEL: define dso_local <8 x i8> @test_vld1_dup_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <8 x i8> poison, i8 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <8 x i8> [[TMP1]], <8 x i8> poison, <8 x i32> zeroinitializer
// CHECK-NEXT:    ret <8 x i8> [[LANE]]
//
poly8x8_t test_vld1_dup_p8(poly8_t  *a) {
  return vld1_dup_p8(a);
}

// CHECK-LABEL: define dso_local <4 x i16> @test_vld1_dup_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <4 x i16> poison, i16 [[TMP0]], i64 0
// CHECK-NEXT:    [[LANE:%.*]] = shufflevector <4 x i16> [[TMP1]], <4 x i16> poison, <4 x i32> zeroinitializer
// CHECK-NEXT:    ret <4 x i16> [[LANE]]
//
poly16x4_t test_vld1_dup_p16(poly16_t  *a) {
  return vld1_dup_p16(a);
}

// CHECK-LABEL: define dso_local <1 x i64> @test_vld1_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertelement <1 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x i64> [[TMP1]]
//
poly64x1_t test_vld1_dup_p64(poly64_t  *a) {
  return vld1_dup_p64(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x2_t @test_vld2q_dup_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X2_T]], align 16
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD2_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD2_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD2_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [2 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT3:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK4:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT3]], align 16
// CHECK-NEXT:    [[DOTUNPACK5:%.*]] = insertvalue [2 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK4]], 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [[STRUCT_UINT64X2X2_T]] poison, [2 x <2 x i64>] [[DOTUNPACK5]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X2_T]] [[TMP1]]
//
uint64x2x2_t test_vld2q_dup_u64(uint64_t  *a) {
  return vld2q_dup_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int64x2x2_t @test_vld2q_dup_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X2_T]], align 16
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD2_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD2_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD2_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [2 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT3:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK4:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT3]], align 16
// CHECK-NEXT:    [[DOTUNPACK5:%.*]] = insertvalue [2 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK4]], 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [[STRUCT_INT64X2X2_T]] poison, [2 x <2 x i64>] [[DOTUNPACK5]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X2X2_T]] [[TMP1]]
//
int64x2x2_t test_vld2q_dup_s64(int64_t  *a) {
  return vld2q_dup_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float64x2x2_t @test_vld2q_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x double>, <2 x double> } @llvm.aarch64.neon.ld2r.v2f64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD2_ELT:%.*]] = extractvalue { <2 x double>, <2 x double> } [[VLD2]], 0
// CHECK-NEXT:    store <2 x double> [[VLD2_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_ELT2:%.*]] = extractvalue { <2 x double>, <2 x double> } [[VLD2]], 1
// CHECK-NEXT:    store <2 x double> [[VLD2_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x double>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [2 x <2 x double>] poison, <2 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT3:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK4:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT3]], align 16
// CHECK-NEXT:    [[DOTUNPACK5:%.*]] = insertvalue [2 x <2 x double>] [[TMP0]], <2 x double> [[DOTUNPACK_UNPACK4]], 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [[STRUCT_FLOAT64X2X2_T]] poison, [2 x <2 x double>] [[DOTUNPACK5]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X2_T]] [[TMP1]]
//
float64x2x2_t test_vld2q_dup_f64(float64_t  *a) {
  return vld2q_dup_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x2_t @test_vld2q_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X2_T]], align 16
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD2_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD2_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD2_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [2 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT3:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK4:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT3]], align 16
// CHECK-NEXT:    [[DOTUNPACK5:%.*]] = insertvalue [2 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK4]], 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [[STRUCT_POLY64X2X2_T]] poison, [2 x <2 x i64>] [[DOTUNPACK5]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X2_T]] [[TMP1]]
//
poly64x2x2_t test_vld2q_dup_p64(poly64_t  *a) {
  return vld2q_dup_p64(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x2_t @test_vld2_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[VLD2:%.*]] = call { <1 x double>, <1 x double> } @llvm.aarch64.neon.ld2r.v1f64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD2_ELT:%.*]] = extractvalue { <1 x double>, <1 x double> } [[VLD2]], 0
// CHECK-NEXT:    store <1 x double> [[VLD2_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_ELT2:%.*]] = extractvalue { <1 x double>, <1 x double> } [[VLD2]], 1
// CHECK-NEXT:    store <1 x double> [[VLD2_ELT2]], ptr [[__RET_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x double>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [2 x <1 x double>] poison, <1 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT3:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK4:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT3]], align 8
// CHECK-NEXT:    [[DOTUNPACK5:%.*]] = insertvalue [2 x <1 x double>] [[TMP0]], <1 x double> [[DOTUNPACK_UNPACK4]], 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [[STRUCT_FLOAT64X1X2_T]] poison, [2 x <1 x double>] [[DOTUNPACK5]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X2_T]] [[TMP1]]
//
float64x1x2_t test_vld2_dup_f64(float64_t  *a) {
  return vld2_dup_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x2_t @test_vld2_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X2_T]], align 8
// CHECK-NEXT:    [[VLD2:%.*]] = call { <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld2r.v1i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD2_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD2_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_ELT2:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD2_ELT2]], ptr [[__RET_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [2 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT3:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK4:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT3]], align 8
// CHECK-NEXT:    [[DOTUNPACK5:%.*]] = insertvalue [2 x <1 x i64>] [[TMP0]], <1 x i64> [[DOTUNPACK_UNPACK4]], 1
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [[STRUCT_POLY64X1X2_T]] poison, [2 x <1 x i64>] [[DOTUNPACK5]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X2_T]] [[TMP1]]
//
poly64x1x2_t test_vld2_dup_p64(poly64_t  *a) {
  return vld2_dup_p64(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x3_t @test_vld3q_dup_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X3_T]], align 16
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD3_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [3 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [3 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[DOTUNPACK9:%.*]] = insertvalue [3 x <2 x i64>] [[TMP1]], <2 x i64> [[DOTUNPACK_UNPACK8]], 2
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [[STRUCT_UINT64X2X3_T]] poison, [3 x <2 x i64>] [[DOTUNPACK9]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X3_T]] [[TMP2]]
//
uint64x2x3_t test_vld3q_dup_u64(uint64_t  *a) {
  return vld3q_dup_u64(a);
  // [{{x[0-9]+|sp}}]
}

// CHECK-LABEL: define dso_local %struct.int64x2x3_t @test_vld3q_dup_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X3_T]], align 16
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD3_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [3 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [3 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[DOTUNPACK9:%.*]] = insertvalue [3 x <2 x i64>] [[TMP1]], <2 x i64> [[DOTUNPACK_UNPACK8]], 2
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [[STRUCT_INT64X2X3_T]] poison, [3 x <2 x i64>] [[DOTUNPACK9]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X2X3_T]] [[TMP2]]
//
int64x2x3_t test_vld3q_dup_s64(int64_t  *a) {
  return vld3q_dup_s64(a);
  // [{{x[0-9]+|sp}}]
}

// CHECK-LABEL: define dso_local %struct.float64x2x3_t @test_vld3q_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld3r.v2f64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD3_ELT:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double> } [[VLD3]], 0
// CHECK-NEXT:    store <2 x double> [[VLD3_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_ELT2:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double> } [[VLD3]], 1
// CHECK-NEXT:    store <2 x double> [[VLD3_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_ELT4:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double> } [[VLD3]], 2
// CHECK-NEXT:    store <2 x double> [[VLD3_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x double>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [3 x <2 x double>] poison, <2 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [3 x <2 x double>] [[TMP0]], <2 x double> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[DOTUNPACK9:%.*]] = insertvalue [3 x <2 x double>] [[TMP1]], <2 x double> [[DOTUNPACK_UNPACK8]], 2
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [[STRUCT_FLOAT64X2X3_T]] poison, [3 x <2 x double>] [[DOTUNPACK9]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X3_T]] [[TMP2]]
//
float64x2x3_t test_vld3q_dup_f64(float64_t  *a) {
  return vld3q_dup_f64(a);
  // [{{x[0-9]+|sp}}]
}

// CHECK-LABEL: define dso_local %struct.poly64x2x3_t @test_vld3q_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X3_T]], align 16
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD3_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD3_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [3 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [3 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[DOTUNPACK9:%.*]] = insertvalue [3 x <2 x i64>] [[TMP1]], <2 x i64> [[DOTUNPACK_UNPACK8]], 2
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [[STRUCT_POLY64X2X3_T]] poison, [3 x <2 x i64>] [[DOTUNPACK9]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X3_T]] [[TMP2]]
//
poly64x2x3_t test_vld3q_dup_p64(poly64_t  *a) {
  return vld3q_dup_p64(a);
  // [{{x[0-9]+|sp}}]
}

// CHECK-LABEL: define dso_local %struct.float64x1x3_t @test_vld3_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[VLD3:%.*]] = call { <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld3r.v1f64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD3_ELT:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double> } [[VLD3]], 0
// CHECK-NEXT:    store <1 x double> [[VLD3_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_ELT2:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double> } [[VLD3]], 1
// CHECK-NEXT:    store <1 x double> [[VLD3_ELT2]], ptr [[__RET_REPACK1]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_ELT4:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double> } [[VLD3]], 2
// CHECK-NEXT:    store <1 x double> [[VLD3_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x double>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [3 x <1 x double>] poison, <1 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [3 x <1 x double>] [[TMP0]], <1 x double> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT7]], align 8
// CHECK-NEXT:    [[DOTUNPACK9:%.*]] = insertvalue [3 x <1 x double>] [[TMP1]], <1 x double> [[DOTUNPACK_UNPACK8]], 2
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [[STRUCT_FLOAT64X1X3_T]] poison, [3 x <1 x double>] [[DOTUNPACK9]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X3_T]] [[TMP2]]
//
float64x1x3_t test_vld3_dup_f64(float64_t  *a) {
  return vld3_dup_f64(a);
  // [{{x[0-9]+|sp}}]
}

// CHECK-LABEL: define dso_local %struct.poly64x1x3_t @test_vld3_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X3_T]], align 8
// CHECK-NEXT:    [[VLD3:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld3r.v1i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD3_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD3_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_ELT2:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD3_ELT2]], ptr [[__RET_REPACK1]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_ELT4:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD3_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [3 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [3 x <1 x i64>] [[TMP0]], <1 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT7]], align 8
// CHECK-NEXT:    [[DOTUNPACK9:%.*]] = insertvalue [3 x <1 x i64>] [[TMP1]], <1 x i64> [[DOTUNPACK_UNPACK8]], 2
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [[STRUCT_POLY64X1X3_T]] poison, [3 x <1 x i64>] [[DOTUNPACK9]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X3_T]] [[TMP2]]
//
poly64x1x3_t test_vld3_dup_p64(poly64_t  *a) {
  return vld3_dup_p64(a);
  // [{{x[0-9]+|sp}}]
}

// CHECK-LABEL: define dso_local %struct.uint64x2x4_t @test_vld4q_dup_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X4_T]], align 16
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD4_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_ELT6:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 3
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [4 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [4 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK8]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [4 x <2 x i64>] [[TMP1]], <2 x i64> [[DOTUNPACK_UNPACK10]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [4 x <2 x i64>] [[TMP2]], <2 x i64> [[DOTUNPACK_UNPACK12]], 3
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT64X2X4_T]] poison, [4 x <2 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X4_T]] [[TMP3]]
//
uint64x2x4_t test_vld4q_dup_u64(uint64_t  *a) {
  return vld4q_dup_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int64x2x4_t @test_vld4q_dup_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X4_T]], align 16
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD4_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_ELT6:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 3
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [4 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [4 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK8]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [4 x <2 x i64>] [[TMP1]], <2 x i64> [[DOTUNPACK_UNPACK10]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [4 x <2 x i64>] [[TMP2]], <2 x i64> [[DOTUNPACK_UNPACK12]], 3
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT64X2X4_T]] poison, [4 x <2 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X2X4_T]] [[TMP3]]
//
int64x2x4_t test_vld4q_dup_s64(int64_t  *a) {
  return vld4q_dup_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float64x2x4_t @test_vld4q_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x double>, <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld4r.v2f64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD4_ELT:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4]], 0
// CHECK-NEXT:    store <2 x double> [[VLD4_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_ELT2:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4]], 1
// CHECK-NEXT:    store <2 x double> [[VLD4_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_ELT4:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4]], 2
// CHECK-NEXT:    store <2 x double> [[VLD4_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_ELT6:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4]], 3
// CHECK-NEXT:    store <2 x double> [[VLD4_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x double>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [4 x <2 x double>] poison, <2 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [4 x <2 x double>] [[TMP0]], <2 x double> [[DOTUNPACK_UNPACK8]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [4 x <2 x double>] [[TMP1]], <2 x double> [[DOTUNPACK_UNPACK10]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [4 x <2 x double>] [[TMP2]], <2 x double> [[DOTUNPACK_UNPACK12]], 3
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT64X2X4_T]] poison, [4 x <2 x double>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X4_T]] [[TMP3]]
//
float64x2x4_t test_vld4q_dup_f64(float64_t  *a) {
  return vld4q_dup_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x4_t @test_vld4q_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X4_T]], align 16
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4r.v2i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD4_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_ELT2:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT2]], ptr [[__RET_REPACK1]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_ELT6:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], 3
// CHECK-NEXT:    store <2 x i64> [[VLD4_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [4 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT7]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [4 x <2 x i64>] [[TMP0]], <2 x i64> [[DOTUNPACK_UNPACK8]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [4 x <2 x i64>] [[TMP1]], <2 x i64> [[DOTUNPACK_UNPACK10]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [4 x <2 x i64>] [[TMP2]], <2 x i64> [[DOTUNPACK_UNPACK12]], 3
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY64X2X4_T]] poison, [4 x <2 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X4_T]] [[TMP3]]
//
poly64x2x4_t test_vld4q_dup_p64(poly64_t  *a) {
  return vld4q_dup_p64(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x4_t @test_vld4_dup_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[VLD4:%.*]] = call { <1 x double>, <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld4r.v1f64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD4_ELT:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4]], 0
// CHECK-NEXT:    store <1 x double> [[VLD4_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_ELT2:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4]], 1
// CHECK-NEXT:    store <1 x double> [[VLD4_ELT2]], ptr [[__RET_REPACK1]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_ELT4:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4]], 2
// CHECK-NEXT:    store <1 x double> [[VLD4_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_ELT6:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4]], 3
// CHECK-NEXT:    store <1 x double> [[VLD4_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x double>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [4 x <1 x double>] poison, <1 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT7]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [4 x <1 x double>] [[TMP0]], <1 x double> [[DOTUNPACK_UNPACK8]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [4 x <1 x double>] [[TMP1]], <1 x double> [[DOTUNPACK_UNPACK10]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [4 x <1 x double>] [[TMP2]], <1 x double> [[DOTUNPACK_UNPACK12]], 3
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT64X1X4_T]] poison, [4 x <1 x double>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X4_T]] [[TMP3]]
//
float64x1x4_t test_vld4_dup_f64(float64_t  *a) {
  return vld4_dup_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x4_t @test_vld4_dup_p64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X4_T]], align 8
// CHECK-NEXT:    [[VLD4:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld4r.v1i64.p0(ptr [[A]])
// CHECK-NEXT:    [[VLD4_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD4_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_ELT2:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD4_ELT2]], ptr [[__RET_REPACK1]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_ELT4:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD4_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_ELT6:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4]], 3
// CHECK-NEXT:    store <1 x i64> [[VLD4_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = insertvalue [4 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT7:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK8:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT7]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = insertvalue [4 x <1 x i64>] [[TMP0]], <1 x i64> [[DOTUNPACK_UNPACK8]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [4 x <1 x i64>] [[TMP1]], <1 x i64> [[DOTUNPACK_UNPACK10]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [4 x <1 x i64>] [[TMP2]], <1 x i64> [[DOTUNPACK_UNPACK12]], 3
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY64X1X4_T]] poison, [4 x <1 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X4_T]] [[TMP3]]
//
poly64x1x4_t test_vld4_dup_p64(poly64_t  *a) {
  return vld4_dup_p64(a);
}

// CHECK-LABEL: define dso_local <16 x i8> @test_vld1q_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <16 x i8> [[B]], i8 [[TMP0]], i64 15
// CHECK-NEXT:    ret <16 x i8> [[VLD1_LANE]]
//
uint8x16_t test_vld1q_lane_u8(uint8_t  *a, uint8x16_t b) {
  return vld1q_lane_u8(a, b, 15);
}

// CHECK-LABEL: define dso_local <8 x i16> @test_vld1q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <8 x i16> [[B]], i16 [[TMP0]], i64 7
// CHECK-NEXT:    ret <8 x i16> [[VLD1_LANE]]
//
uint16x8_t test_vld1q_lane_u16(uint16_t  *a, uint16x8_t b) {
  return vld1q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local <4 x i32> @test_vld1q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <4 x i32> [[B]], i32 [[TMP0]], i64 3
// CHECK-NEXT:    ret <4 x i32> [[VLD1_LANE]]
//
uint32x4_t test_vld1q_lane_u32(uint32_t  *a, uint32x4_t b) {
  return vld1q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_vld1q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <2 x i64> [[B]], i64 [[TMP0]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[VLD1_LANE]]
//
uint64x2_t test_vld1q_lane_u64(uint64_t  *a, uint64x2_t b) {
  return vld1q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local <16 x i8> @test_vld1q_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <16 x i8> [[B]], i8 [[TMP0]], i64 15
// CHECK-NEXT:    ret <16 x i8> [[VLD1_LANE]]
//
int8x16_t test_vld1q_lane_s8(int8_t  *a, int8x16_t b) {
  return vld1q_lane_s8(a, b, 15);
}

// CHECK-LABEL: define dso_local <8 x i16> @test_vld1q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <8 x i16> [[B]], i16 [[TMP0]], i64 7
// CHECK-NEXT:    ret <8 x i16> [[VLD1_LANE]]
//
int16x8_t test_vld1q_lane_s16(int16_t  *a, int16x8_t b) {
  return vld1q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local <4 x i32> @test_vld1q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <4 x i32> [[B]], i32 [[TMP0]], i64 3
// CHECK-NEXT:    ret <4 x i32> [[VLD1_LANE]]
//
int32x4_t test_vld1q_lane_s32(int32_t  *a, int32x4_t b) {
  return vld1q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_vld1q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <2 x i64> [[B]], i64 [[TMP0]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[VLD1_LANE]]
//
int64x2_t test_vld1q_lane_s64(int64_t  *a, int64x2_t b) {
  return vld1q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local <8 x half> @test_vld1q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <8 x half> [[B]], half [[TMP0]], i64 7
// CHECK-NEXT:    ret <8 x half> [[VLD1_LANE]]
//
float16x8_t test_vld1q_lane_f16(float16_t  *a, float16x8_t b) {
  return vld1q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local <4 x float> @test_vld1q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[A]], align 4
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <4 x float> [[B]], float [[TMP0]], i64 3
// CHECK-NEXT:    ret <4 x float> [[VLD1_LANE]]
//
float32x4_t test_vld1q_lane_f32(float32_t  *a, float32x4_t b) {
  return vld1q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local <2 x double> @test_vld1q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load double, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <2 x double> [[B]], double [[TMP0]], i64 1
// CHECK-NEXT:    ret <2 x double> [[VLD1_LANE]]
//
float64x2_t test_vld1q_lane_f64(float64_t  *a, float64x2_t b) {
  return vld1q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local <16 x i8> @test_vld1q_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <16 x i8> [[B]], i8 [[TMP0]], i64 15
// CHECK-NEXT:    ret <16 x i8> [[VLD1_LANE]]
//
poly8x16_t test_vld1q_lane_p8(poly8_t  *a, poly8x16_t b) {
  return vld1q_lane_p8(a, b, 15);
}

// CHECK-LABEL: define dso_local <8 x i16> @test_vld1q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <8 x i16> [[B]], i16 [[TMP0]], i64 7
// CHECK-NEXT:    ret <8 x i16> [[VLD1_LANE]]
//
poly16x8_t test_vld1q_lane_p16(poly16_t  *a, poly16x8_t b) {
  return vld1q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_vld1q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <2 x i64> [[B]], i64 [[TMP0]], i64 1
// CHECK-NEXT:    ret <2 x i64> [[VLD1_LANE]]
//
poly64x2_t test_vld1q_lane_p64(poly64_t  *a, poly64x2_t b) {
  return vld1q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local <8 x i8> @test_vld1_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <8 x i8> [[B]], i8 [[TMP0]], i64 7
// CHECK-NEXT:    ret <8 x i8> [[VLD1_LANE]]
//
uint8x8_t test_vld1_lane_u8(uint8_t  *a, uint8x8_t b) {
  return vld1_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local <4 x i16> @test_vld1_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <4 x i16> [[B]], i16 [[TMP0]], i64 3
// CHECK-NEXT:    ret <4 x i16> [[VLD1_LANE]]
//
uint16x4_t test_vld1_lane_u16(uint16_t  *a, uint16x4_t b) {
  return vld1_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local <2 x i32> @test_vld1_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <2 x i32> [[B]], i32 [[TMP0]], i64 1
// CHECK-NEXT:    ret <2 x i32> [[VLD1_LANE]]
//
uint32x2_t test_vld1_lane_u32(uint32_t  *a, uint32x2_t b) {
  return vld1_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local <1 x i64> @test_vld1_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <1 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x i64> [[VLD1_LANE]]
//
uint64x1_t test_vld1_lane_u64(uint64_t  *a, uint64x1_t b) {
  return vld1_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local <8 x i8> @test_vld1_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <8 x i8> [[B]], i8 [[TMP0]], i64 7
// CHECK-NEXT:    ret <8 x i8> [[VLD1_LANE]]
//
int8x8_t test_vld1_lane_s8(int8_t  *a, int8x8_t b) {
  return vld1_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local <4 x i16> @test_vld1_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <4 x i16> [[B]], i16 [[TMP0]], i64 3
// CHECK-NEXT:    ret <4 x i16> [[VLD1_LANE]]
//
int16x4_t test_vld1_lane_s16(int16_t  *a, int16x4_t b) {
  return vld1_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local <2 x i32> @test_vld1_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A]], align 4
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <2 x i32> [[B]], i32 [[TMP0]], i64 1
// CHECK-NEXT:    ret <2 x i32> [[VLD1_LANE]]
//
int32x2_t test_vld1_lane_s32(int32_t  *a, int32x2_t b) {
  return vld1_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local <1 x i64> @test_vld1_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <1 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x i64> [[VLD1_LANE]]
//
int64x1_t test_vld1_lane_s64(int64_t  *a, int64x1_t b) {
  return vld1_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local <4 x half> @test_vld1_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <4 x half> [[B]], half [[TMP0]], i64 3
// CHECK-NEXT:    ret <4 x half> [[VLD1_LANE]]
//
float16x4_t test_vld1_lane_f16(float16_t  *a, float16x4_t b) {
  return vld1_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local <2 x float> @test_vld1_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[A]], align 4
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <2 x float> [[B]], float [[TMP0]], i64 1
// CHECK-NEXT:    ret <2 x float> [[VLD1_LANE]]
//
float32x2_t test_vld1_lane_f32(float32_t  *a, float32x2_t b) {
  return vld1_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local <1 x double> @test_vld1_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load double, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <1 x double> poison, double [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x double> [[VLD1_LANE]]
//
float64x1_t test_vld1_lane_f64(float64_t  *a, float64x1_t b) {
  return vld1_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local <8 x i8> @test_vld1_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A]], align 1
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <8 x i8> [[B]], i8 [[TMP0]], i64 7
// CHECK-NEXT:    ret <8 x i8> [[VLD1_LANE]]
//
poly8x8_t test_vld1_lane_p8(poly8_t  *a, poly8x8_t b) {
  return vld1_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local <4 x i16> @test_vld1_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A]], align 2
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <4 x i16> [[B]], i16 [[TMP0]], i64 3
// CHECK-NEXT:    ret <4 x i16> [[VLD1_LANE]]
//
poly16x4_t test_vld1_lane_p16(poly16_t  *a, poly16x4_t b) {
  return vld1_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local <1 x i64> @test_vld1_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A]], align 8
// CHECK-NEXT:    [[VLD1_LANE:%.*]] = insertelement <1 x i64> poison, i64 [[TMP0]], i64 0
// CHECK-NEXT:    ret <1 x i64> [[VLD1_LANE]]
//
poly64x1_t test_vld1_lane_p64(poly64_t  *a, poly64x1_t b) {
  return vld1_lane_p64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.int8x16x2_t @test_vld2q_lane_s8(
// CHECK-SAME: ptr noundef [[PTR:%.*]], [2 x <16 x i8>] alignstack(16) [[SRC_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_INT8X16X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X16X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X2_T]], align 16
// CHECK-NEXT:    [[SRC_COERCE_ELT:%.*]] = extractvalue [2 x <16 x i8>] [[SRC_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT]], ptr [[SRC]], align 16
// CHECK-NEXT:    [[SRC_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[SRC]], i64 16
// CHECK-NEXT:    [[SRC_COERCE_ELT2:%.*]] = extractvalue [2 x <16 x i8>] [[SRC_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT2]], ptr [[SRC_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[SRC]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld2lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], i64 15, ptr [[PTR]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <16 x i8>, <16 x i8> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <16 x i8>] [[TMP2]], <16 x i8> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT8X16X2_T]] poison, [2 x <16 x i8>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT8X16X2_T]] [[TMP3]]
//
int8x16x2_t test_vld2q_lane_s8(int8_t const * ptr, int8x16x2_t src) {
  return vld2q_lane_s8(ptr, src, 15);
}

// CHECK-LABEL: define dso_local %struct.uint8x16x2_t @test_vld2q_lane_u8(
// CHECK-SAME: ptr noundef [[PTR:%.*]], [2 x <16 x i8>] alignstack(16) [[SRC_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_UINT8X16X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X16X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X2_T]], align 16
// CHECK-NEXT:    [[SRC_COERCE_ELT:%.*]] = extractvalue [2 x <16 x i8>] [[SRC_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT]], ptr [[SRC]], align 16
// CHECK-NEXT:    [[SRC_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[SRC]], i64 16
// CHECK-NEXT:    [[SRC_COERCE_ELT2:%.*]] = extractvalue [2 x <16 x i8>] [[SRC_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT2]], ptr [[SRC_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[SRC]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld2lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], i64 15, ptr [[PTR]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <16 x i8>, <16 x i8> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <16 x i8>] [[TMP2]], <16 x i8> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT8X16X2_T]] poison, [2 x <16 x i8>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT8X16X2_T]] [[TMP3]]
//
uint8x16x2_t test_vld2q_lane_u8(uint8_t const * ptr, uint8x16x2_t src) {
  return vld2q_lane_u8(ptr, src, 15);
}

// CHECK-LABEL: define dso_local %struct.poly8x16x2_t @test_vld2q_lane_p8(
// CHECK-SAME: ptr noundef [[PTR:%.*]], [2 x <16 x i8>] alignstack(16) [[SRC_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_POLY8X16X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X16X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X2_T]], align 16
// CHECK-NEXT:    [[SRC_COERCE_ELT:%.*]] = extractvalue [2 x <16 x i8>] [[SRC_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT]], ptr [[SRC]], align 16
// CHECK-NEXT:    [[SRC_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[SRC]], i64 16
// CHECK-NEXT:    [[SRC_COERCE_ELT2:%.*]] = extractvalue [2 x <16 x i8>] [[SRC_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT2]], ptr [[SRC_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[SRC]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld2lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], i64 15, ptr [[PTR]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <16 x i8>, <16 x i8> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <16 x i8>] [[TMP2]], <16 x i8> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY8X16X2_T]] poison, [2 x <16 x i8>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY8X16X2_T]] [[TMP3]]
//
poly8x16x2_t test_vld2q_lane_p8(poly8_t const * ptr, poly8x16x2_t src) {
  return vld2q_lane_p8(ptr, src, 15);
}

// CHECK-LABEL: define dso_local %struct.int8x16x3_t @test_vld3q_lane_s8(
// CHECK-SAME: ptr noundef [[PTR:%.*]], [3 x <16 x i8>] alignstack(16) [[SRC_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_INT8X16X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X16X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X3_T]], align 16
// CHECK-NEXT:    [[SRC_COERCE_ELT:%.*]] = extractvalue [3 x <16 x i8>] [[SRC_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT]], ptr [[SRC]], align 16
// CHECK-NEXT:    [[SRC_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[SRC]], i64 16
// CHECK-NEXT:    [[SRC_COERCE_ELT2:%.*]] = extractvalue [3 x <16 x i8>] [[SRC_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT2]], ptr [[SRC_REPACK1]], align 16
// CHECK-NEXT:    [[SRC_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[SRC]], i64 32
// CHECK-NEXT:    [[SRC_COERCE_ELT4:%.*]] = extractvalue [3 x <16 x i8>] [[SRC_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT4]], ptr [[SRC_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[SRC]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld3lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], i64 15, ptr [[PTR]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <16 x i8>] [[TMP3]], <16 x i8> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <16 x i8>] [[TMP4]], <16 x i8> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT8X16X3_T]] poison, [3 x <16 x i8>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT8X16X3_T]] [[TMP5]]
//
int8x16x3_t test_vld3q_lane_s8(int8_t const * ptr, int8x16x3_t src) {
  return vld3q_lane_s8(ptr, src, 15);
}

// CHECK-LABEL: define dso_local %struct.uint8x16x3_t @test_vld3q_lane_u8(
// CHECK-SAME: ptr noundef [[PTR:%.*]], [3 x <16 x i8>] alignstack(16) [[SRC_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[SRC:%.*]] = alloca [[STRUCT_UINT8X16X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X16X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X3_T]], align 16
// CHECK-NEXT:    [[SRC_COERCE_ELT:%.*]] = extractvalue [3 x <16 x i8>] [[SRC_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT]], ptr [[SRC]], align 16
// CHECK-NEXT:    [[SRC_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[SRC]], i64 16
// CHECK-NEXT:    [[SRC_COERCE_ELT2:%.*]] = extractvalue [3 x <16 x i8>] [[SRC_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT2]], ptr [[SRC_REPACK1]], align 16
// CHECK-NEXT:    [[SRC_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[SRC]], i64 32
// CHECK-NEXT:    [[SRC_COERCE_ELT4:%.*]] = extractvalue [3 x <16 x i8>] [[SRC_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[SRC_COERCE_ELT4]], ptr [[SRC_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[SRC]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld3lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], i64 15, ptr [[PTR]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <16 x i8>] [[TMP3]], <16 x i8> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <16 x i8>] [[TMP4]], <16 x i8> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT8X16X3_T]] poison, [3 x <16 x i8>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT8X16X3_T]] [[TMP5]]
//
uint8x16x3_t test_vld3q_lane_u8(uint8_t const * ptr, uint8x16x3_t src) {
  return vld3q_lane_u8(ptr, src, 15);
}

// CHECK-LABEL: define dso_local %struct.uint16x8x2_t @test_vld2q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X8X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld2lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <8 x i16>, <8 x i16> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <8 x i16>] [[TMP2]], <8 x i16> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT16X8X2_T]] poison, [2 x <8 x i16>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT16X8X2_T]] [[TMP3]]
//
uint16x8x2_t test_vld2q_lane_u16(uint16_t  *a, uint16x8x2_t b) {
  return vld2q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.uint32x4x2_t @test_vld2q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X4X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld2lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <4 x i32>, <4 x i32> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <4 x i32> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <4 x i32>, <4 x i32> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <4 x i32> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i32>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <4 x i32>] poison, <4 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <4 x i32>] [[TMP2]], <4 x i32> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT32X4X2_T]] poison, [2 x <4 x i32>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT32X4X2_T]] [[TMP3]]
//
uint32x4x2_t test_vld2q_lane_u32(uint32_t  *a, uint32x4x2_t b) {
  return vld2q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x2_t @test_vld2q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <2 x i64>] [[TMP2]], <2 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT64X2X2_T]] poison, [2 x <2 x i64>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X2_T]] [[TMP3]]
//
uint64x2x2_t test_vld2q_lane_u64(uint64_t  *a, uint64x2x2_t b) {
  return vld2q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.int16x8x2_t @test_vld2q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X8X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld2lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <8 x i16>, <8 x i16> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <8 x i16>] [[TMP2]], <8 x i16> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT16X8X2_T]] poison, [2 x <8 x i16>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT16X8X2_T]] [[TMP3]]
//
int16x8x2_t test_vld2q_lane_s16(int16_t  *a, int16x8x2_t b) {
  return vld2q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.int32x4x2_t @test_vld2q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X4X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld2lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <4 x i32>, <4 x i32> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <4 x i32> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <4 x i32>, <4 x i32> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <4 x i32> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i32>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <4 x i32>] poison, <4 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <4 x i32>] [[TMP2]], <4 x i32> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT32X4X2_T]] poison, [2 x <4 x i32>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT32X4X2_T]] [[TMP3]]
//
int32x4x2_t test_vld2q_lane_s32(int32_t  *a, int32x4x2_t b) {
  return vld2q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.int64x2x2_t @test_vld2q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <2 x i64>] [[TMP2]], <2 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT64X2X2_T]] poison, [2 x <2 x i64>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X2X2_T]] [[TMP3]]
//
int64x2x2_t test_vld2q_lane_s64(int64_t  *a, int64x2x2_t b) {
  return vld2q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.float16x8x2_t @test_vld2q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <8 x half>, <8 x half> } @llvm.aarch64.neon.ld2lane.v8f16.p0(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <8 x half>, <8 x half> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <8 x half> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <8 x half>, <8 x half> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <8 x half> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x half>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <8 x half>] poison, <8 x half> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <8 x half>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <8 x half>] [[TMP2]], <8 x half> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT16X8X2_T]] poison, [2 x <8 x half>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X8X2_T]] [[TMP3]]
//
float16x8x2_t test_vld2q_lane_f16(float16_t  *a, float16x8x2_t b) {
  return vld2q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.float32x4x2_t @test_vld2q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <4 x float>, <4 x float> } @llvm.aarch64.neon.ld2lane.v4f32.p0(<4 x float> [[TMP0]], <4 x float> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <4 x float>, <4 x float> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <4 x float> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <4 x float>, <4 x float> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <4 x float> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x float>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <4 x float>] poison, <4 x float> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <4 x float>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <4 x float>] [[TMP2]], <4 x float> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT32X4X2_T]] poison, [2 x <4 x float>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X4X2_T]] [[TMP3]]
//
float32x4x2_t test_vld2q_lane_f32(float32_t  *a, float32x4x2_t b) {
  return vld2q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.float64x2x2_t @test_vld2q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <2 x double>, <2 x double> } @llvm.aarch64.neon.ld2lane.v2f64.p0(<2 x double> [[TMP0]], <2 x double> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <2 x double>, <2 x double> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <2 x double> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <2 x double>, <2 x double> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <2 x double> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x double>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <2 x double>] poison, <2 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <2 x double>] [[TMP2]], <2 x double> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT64X2X2_T]] poison, [2 x <2 x double>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X2_T]] [[TMP3]]
//
float64x2x2_t test_vld2q_lane_f64(float64_t  *a, float64x2x2_t b) {
  return vld2q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.poly16x8x2_t @test_vld2q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X8X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld2lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <8 x i16>, <8 x i16> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <8 x i16>] [[TMP2]], <8 x i16> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY16X8X2_T]] poison, [2 x <8 x i16>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY16X8X2_T]] [[TMP3]]
//
poly16x8x2_t test_vld2q_lane_p16(poly16_t  *a, poly16x8x2_t b) {
  return vld2q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x2_t @test_vld2q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X2_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X2_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD2_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <2 x i64>, <2 x i64> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT5]], align 16
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <2 x i64>] [[TMP2]], <2 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY64X2X2_T]] poison, [2 x <2 x i64>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X2_T]] [[TMP3]]
//
poly64x2x2_t test_vld2q_lane_p64(poly64_t  *a, poly64x2x2_t b) {
  return vld2q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.uint8x8x2_t @test_vld2_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X8X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld2lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <8 x i8>, <8 x i8> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <8 x i8>] [[TMP2]], <8 x i8> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT8X8X2_T]] poison, [2 x <8 x i8>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT8X8X2_T]] [[TMP3]]
//
uint8x8x2_t test_vld2_lane_u8(uint8_t  *a, uint8x8x2_t b) {
  return vld2_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.uint16x4x2_t @test_vld2_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X4X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld2lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <4 x i16>, <4 x i16> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <4 x i16>] [[TMP2]], <4 x i16> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT16X4X2_T]] poison, [2 x <4 x i16>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT16X4X2_T]] [[TMP3]]
//
uint16x4x2_t test_vld2_lane_u16(uint16_t  *a, uint16x4x2_t b) {
  return vld2_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.uint32x2x2_t @test_vld2_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X2X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld2lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <2 x i32>, <2 x i32> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <2 x i32> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <2 x i32>, <2 x i32> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <2 x i32> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i32>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <2 x i32>] poison, <2 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <2 x i32>] [[TMP2]], <2 x i32> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT32X2X2_T]] poison, [2 x <2 x i32>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT32X2X2_T]] [[TMP3]]
//
uint32x2x2_t test_vld2_lane_u32(uint32_t  *a, uint32x2x2_t b) {
  return vld2_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.uint64x1x2_t @test_vld2_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X1X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld2lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <1 x i64>] [[TMP2]], <1 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_UINT64X1X2_T]] poison, [2 x <1 x i64>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X1X2_T]] [[TMP3]]
//
uint64x1x2_t test_vld2_lane_u64(uint64_t  *a, uint64x1x2_t b) {
  return vld2_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.int8x8x2_t @test_vld2_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X8X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld2lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <8 x i8>, <8 x i8> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <8 x i8>] [[TMP2]], <8 x i8> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT8X8X2_T]] poison, [2 x <8 x i8>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT8X8X2_T]] [[TMP3]]
//
int8x8x2_t test_vld2_lane_s8(int8_t  *a, int8x8x2_t b) {
  return vld2_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.int16x4x2_t @test_vld2_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X4X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld2lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <4 x i16>, <4 x i16> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <4 x i16>] [[TMP2]], <4 x i16> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT16X4X2_T]] poison, [2 x <4 x i16>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT16X4X2_T]] [[TMP3]]
//
int16x4x2_t test_vld2_lane_s16(int16_t  *a, int16x4x2_t b) {
  return vld2_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.int32x2x2_t @test_vld2_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X2X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld2lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <2 x i32>, <2 x i32> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <2 x i32> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <2 x i32>, <2 x i32> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <2 x i32> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i32>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <2 x i32>] poison, <2 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <2 x i32>] [[TMP2]], <2 x i32> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT32X2X2_T]] poison, [2 x <2 x i32>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT32X2X2_T]] [[TMP3]]
//
int32x2x2_t test_vld2_lane_s32(int32_t  *a, int32x2x2_t b) {
  return vld2_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.int64x1x2_t @test_vld2_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X1X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld2lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <1 x i64>] [[TMP2]], <1 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_INT64X1X2_T]] poison, [2 x <1 x i64>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X1X2_T]] [[TMP3]]
//
int64x1x2_t test_vld2_lane_s64(int64_t  *a, int64x1x2_t b) {
  return vld2_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.float16x4x2_t @test_vld2_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x half>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <4 x half>, <4 x half> } @llvm.aarch64.neon.ld2lane.v4f16.p0(<4 x half> [[TMP0]], <4 x half> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <4 x half>, <4 x half> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <4 x half> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <4 x half>, <4 x half> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <4 x half> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x half>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <4 x half>] poison, <4 x half> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <4 x half>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <4 x half>] [[TMP2]], <4 x half> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT16X4X2_T]] poison, [2 x <4 x half>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X4X2_T]] [[TMP3]]
//
float16x4x2_t test_vld2_lane_f16(float16_t  *a, float16x4x2_t b) {
  return vld2_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.float32x2x2_t @test_vld2_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x float>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <2 x float>, <2 x float> } @llvm.aarch64.neon.ld2lane.v2f32.p0(<2 x float> [[TMP0]], <2 x float> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <2 x float>, <2 x float> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <2 x float> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <2 x float>, <2 x float> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <2 x float> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x float>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <2 x float>] poison, <2 x float> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <2 x float>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <2 x float>] [[TMP2]], <2 x float> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT32X2X2_T]] poison, [2 x <2 x float>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X2X2_T]] [[TMP3]]
//
float32x2x2_t test_vld2_lane_f32(float32_t  *a, float32x2x2_t b) {
  return vld2_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.float64x1x2_t @test_vld2_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x double>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <1 x double>, <1 x double> } @llvm.aarch64.neon.ld2lane.v1f64.p0(<1 x double> [[TMP0]], <1 x double> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <1 x double>, <1 x double> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <1 x double> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <1 x double>, <1 x double> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <1 x double> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x double>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <1 x double>] poison, <1 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <1 x double>] [[TMP2]], <1 x double> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_FLOAT64X1X2_T]] poison, [2 x <1 x double>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X2_T]] [[TMP3]]
//
float64x1x2_t test_vld2_lane_f64(float64_t  *a, float64x1x2_t b) {
  return vld2_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.poly8x8x2_t @test_vld2_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X8X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld2lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <8 x i8>, <8 x i8> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <8 x i8>] [[TMP2]], <8 x i8> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY8X8X2_T]] poison, [2 x <8 x i8>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY8X8X2_T]] [[TMP3]]
//
poly8x8x2_t test_vld2_lane_p8(poly8_t  *a, poly8x8x2_t b) {
  return vld2_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.poly16x4x2_t @test_vld2_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X4X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld2lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <4 x i16>, <4 x i16> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <4 x i16>] [[TMP2]], <4 x i16> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY16X4X2_T]] poison, [2 x <4 x i16>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY16X4X2_T]] [[TMP3]]
//
poly16x4x2_t test_vld2_lane_p16(poly16_t  *a, poly16x4x2_t b) {
  return vld2_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x2_t @test_vld2_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X2_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X2_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[VLD2_LANE:%.*]] = call { <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld2lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD2_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD2_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD2_LANE_ELT4:%.*]] = extractvalue { <1 x i64>, <1 x i64> } [[VLD2_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD2_LANE_ELT4]], ptr [[__RET_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(16) [[__RET]], i64 16, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = insertvalue [2 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT5:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK6:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT5]], align 8
// CHECK-NEXT:    [[DOTUNPACK7:%.*]] = insertvalue [2 x <1 x i64>] [[TMP2]], <1 x i64> [[DOTUNPACK_UNPACK6]], 1
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [[STRUCT_POLY64X1X2_T]] poison, [2 x <1 x i64>] [[DOTUNPACK7]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X2_T]] [[TMP3]]
//
poly64x1x2_t test_vld2_lane_p64(poly64_t  *a, poly64x1x2_t b) {
  return vld2_lane_p64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.uint16x8x3_t @test_vld3q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X8X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld3lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <8 x i16>] [[TMP3]], <8 x i16> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <8 x i16>] [[TMP4]], <8 x i16> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT16X8X3_T]] poison, [3 x <8 x i16>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT16X8X3_T]] [[TMP5]]
//
uint16x8x3_t test_vld3q_lane_u16(uint16_t  *a, uint16x8x3_t b) {
  return vld3q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.uint32x4x3_t @test_vld3q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X4X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld3lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <4 x i32> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <4 x i32> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <4 x i32> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i32>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <4 x i32>] poison, <4 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <4 x i32>] [[TMP3]], <4 x i32> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <4 x i32>] [[TMP4]], <4 x i32> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT32X4X3_T]] poison, [3 x <4 x i32>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT32X4X3_T]] [[TMP5]]
//
uint32x4x3_t test_vld3q_lane_u32(uint32_t  *a, uint32x4x3_t b) {
  return vld3q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x3_t @test_vld3q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <2 x i64>] [[TMP3]], <2 x i64> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <2 x i64>] [[TMP4]], <2 x i64> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT64X2X3_T]] poison, [3 x <2 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X3_T]] [[TMP5]]
//
uint64x2x3_t test_vld3q_lane_u64(uint64_t  *a, uint64x2x3_t b) {
  return vld3q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.int16x8x3_t @test_vld3q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X8X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld3lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <8 x i16>] [[TMP3]], <8 x i16> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <8 x i16>] [[TMP4]], <8 x i16> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT16X8X3_T]] poison, [3 x <8 x i16>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT16X8X3_T]] [[TMP5]]
//
int16x8x3_t test_vld3q_lane_s16(int16_t  *a, int16x8x3_t b) {
  return vld3q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.int32x4x3_t @test_vld3q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X4X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld3lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <4 x i32> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <4 x i32> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <4 x i32> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i32>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <4 x i32>] poison, <4 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <4 x i32>] [[TMP3]], <4 x i32> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <4 x i32>] [[TMP4]], <4 x i32> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT32X4X3_T]] poison, [3 x <4 x i32>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT32X4X3_T]] [[TMP5]]
//
int32x4x3_t test_vld3q_lane_s32(int32_t  *a, int32x4x3_t b) {
  return vld3q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.int64x2x3_t @test_vld3q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <2 x i64>] [[TMP3]], <2 x i64> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <2 x i64>] [[TMP4]], <2 x i64> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT64X2X3_T]] poison, [3 x <2 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X2X3_T]] [[TMP5]]
//
int64x2x3_t test_vld3q_lane_s64(int64_t  *a, int64x2x3_t b) {
  return vld3q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.float16x8x3_t @test_vld3q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <8 x half>, <8 x half>, <8 x half> } @llvm.aarch64.neon.ld3lane.v8f16.p0(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <8 x half>, <8 x half>, <8 x half> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <8 x half> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <8 x half>, <8 x half>, <8 x half> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <8 x half> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <8 x half>, <8 x half>, <8 x half> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <8 x half> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x half>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <8 x half>] poison, <8 x half> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <8 x half>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <8 x half>] [[TMP3]], <8 x half> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <8 x half>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <8 x half>] [[TMP4]], <8 x half> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_FLOAT16X8X3_T]] poison, [3 x <8 x half>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X8X3_T]] [[TMP5]]
//
float16x8x3_t test_vld3q_lane_f16(float16_t  *a, float16x8x3_t b) {
  return vld3q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.float32x4x3_t @test_vld3q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <4 x float>, <4 x float>, <4 x float> } @llvm.aarch64.neon.ld3lane.v4f32.p0(<4 x float> [[TMP0]], <4 x float> [[TMP1]], <4 x float> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <4 x float>, <4 x float>, <4 x float> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <4 x float> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <4 x float>, <4 x float>, <4 x float> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <4 x float> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <4 x float>, <4 x float>, <4 x float> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <4 x float> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x float>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <4 x float>] poison, <4 x float> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <4 x float>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <4 x float>] [[TMP3]], <4 x float> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <4 x float>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <4 x float>] [[TMP4]], <4 x float> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_FLOAT32X4X3_T]] poison, [3 x <4 x float>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X4X3_T]] [[TMP5]]
//
float32x4x3_t test_vld3q_lane_f32(float32_t  *a, float32x4x3_t b) {
  return vld3q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.float64x2x3_t @test_vld3q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld3lane.v2f64.p0(<2 x double> [[TMP0]], <2 x double> [[TMP1]], <2 x double> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <2 x double> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <2 x double> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <2 x double> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x double>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <2 x double>] poison, <2 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <2 x double>] [[TMP3]], <2 x double> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <2 x double>] [[TMP4]], <2 x double> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_FLOAT64X2X3_T]] poison, [3 x <2 x double>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X3_T]] [[TMP5]]
//
float64x2x3_t test_vld3q_lane_f64(float64_t  *a, float64x2x3_t b) {
  return vld3q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.poly8x16x3_t @test_vld3q_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X16X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld3lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], i64 15, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <16 x i8> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <16 x i8>] [[TMP3]], <16 x i8> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <16 x i8>] [[TMP4]], <16 x i8> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_POLY8X16X3_T]] poison, [3 x <16 x i8>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY8X16X3_T]] [[TMP5]]
//
poly8x16x3_t test_vld3q_lane_p8(poly8_t  *a, poly8x16x3_t b) {
  return vld3q_lane_p8(a, b, 15);
}

// CHECK-LABEL: define dso_local %struct.poly16x8x3_t @test_vld3q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X8X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld3lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <8 x i16> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <8 x i16>] [[TMP3]], <8 x i16> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <8 x i16>] [[TMP4]], <8 x i16> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_POLY16X8X3_T]] poison, [3 x <8 x i16>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY16X8X3_T]] [[TMP5]]
//
poly16x8x3_t test_vld3q_lane_p16(poly16_t  *a, poly16x8x3_t b) {
  return vld3q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x3_t @test_vld3q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X3_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X3_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(48) [[__RET]], i64 48, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT9]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <2 x i64>] [[TMP3]], <2 x i64> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT11]], align 16
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <2 x i64>] [[TMP4]], <2 x i64> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_POLY64X2X3_T]] poison, [3 x <2 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X3_T]] [[TMP5]]
//
poly64x2x3_t test_vld3q_lane_p64(poly64_t  *a, poly64x2x3_t b) {
  return vld3q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.uint8x8x3_t @test_vld3_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X8X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld3lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <8 x i8>] [[TMP3]], <8 x i8> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <8 x i8>] [[TMP4]], <8 x i8> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT8X8X3_T]] poison, [3 x <8 x i8>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT8X8X3_T]] [[TMP5]]
//
uint8x8x3_t test_vld3_lane_u8(uint8_t  *a, uint8x8x3_t b) {
  return vld3_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.uint16x4x3_t @test_vld3_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X4X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld3lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <4 x i16>] [[TMP3]], <4 x i16> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <4 x i16>] [[TMP4]], <4 x i16> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT16X4X3_T]] poison, [3 x <4 x i16>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT16X4X3_T]] [[TMP5]]
//
uint16x4x3_t test_vld3_lane_u16(uint16_t  *a, uint16x4x3_t b) {
  return vld3_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.uint32x2x3_t @test_vld3_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X2X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld3lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <2 x i32> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <2 x i32> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <2 x i32> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i32>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <2 x i32>] poison, <2 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <2 x i32>] [[TMP3]], <2 x i32> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <2 x i32>] [[TMP4]], <2 x i32> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT32X2X3_T]] poison, [3 x <2 x i32>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT32X2X3_T]] [[TMP5]]
//
uint32x2x3_t test_vld3_lane_u32(uint32_t  *a, uint32x2x3_t b) {
  return vld3_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.uint64x1x3_t @test_vld3_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X1X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld3lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <1 x i64>] [[TMP3]], <1 x i64> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <1 x i64>] [[TMP4]], <1 x i64> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_UINT64X1X3_T]] poison, [3 x <1 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X1X3_T]] [[TMP5]]
//
uint64x1x3_t test_vld3_lane_u64(uint64_t  *a, uint64x1x3_t b) {
  return vld3_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.int8x8x3_t @test_vld3_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X8X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld3lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <8 x i8>] [[TMP3]], <8 x i8> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <8 x i8>] [[TMP4]], <8 x i8> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT8X8X3_T]] poison, [3 x <8 x i8>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT8X8X3_T]] [[TMP5]]
//
int8x8x3_t test_vld3_lane_s8(int8_t  *a, int8x8x3_t b) {
  return vld3_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.int16x4x3_t @test_vld3_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X4X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld3lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <4 x i16>] [[TMP3]], <4 x i16> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <4 x i16>] [[TMP4]], <4 x i16> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT16X4X3_T]] poison, [3 x <4 x i16>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT16X4X3_T]] [[TMP5]]
//
int16x4x3_t test_vld3_lane_s16(int16_t  *a, int16x4x3_t b) {
  return vld3_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.int32x2x3_t @test_vld3_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X2X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld3lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <2 x i32> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <2 x i32> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <2 x i32> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i32>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <2 x i32>] poison, <2 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <2 x i32>] [[TMP3]], <2 x i32> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <2 x i32>] [[TMP4]], <2 x i32> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT32X2X3_T]] poison, [3 x <2 x i32>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT32X2X3_T]] [[TMP5]]
//
int32x2x3_t test_vld3_lane_s32(int32_t  *a, int32x2x3_t b) {
  return vld3_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.int64x1x3_t @test_vld3_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X1X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld3lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <1 x i64>] [[TMP3]], <1 x i64> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <1 x i64>] [[TMP4]], <1 x i64> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_INT64X1X3_T]] poison, [3 x <1 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X1X3_T]] [[TMP5]]
//
int64x1x3_t test_vld3_lane_s64(int64_t  *a, int64x1x3_t b) {
  return vld3_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.float16x4x3_t @test_vld3_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x half>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x half>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <4 x half>, <4 x half>, <4 x half> } @llvm.aarch64.neon.ld3lane.v4f16.p0(<4 x half> [[TMP0]], <4 x half> [[TMP1]], <4 x half> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <4 x half>, <4 x half>, <4 x half> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <4 x half> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <4 x half>, <4 x half>, <4 x half> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <4 x half> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <4 x half>, <4 x half>, <4 x half> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <4 x half> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x half>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <4 x half>] poison, <4 x half> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <4 x half>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <4 x half>] [[TMP3]], <4 x half> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <4 x half>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <4 x half>] [[TMP4]], <4 x half> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_FLOAT16X4X3_T]] poison, [3 x <4 x half>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X4X3_T]] [[TMP5]]
//
float16x4x3_t test_vld3_lane_f16(float16_t  *a, float16x4x3_t b) {
  return vld3_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.float32x2x3_t @test_vld3_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x float>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x float>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <2 x float>, <2 x float>, <2 x float> } @llvm.aarch64.neon.ld3lane.v2f32.p0(<2 x float> [[TMP0]], <2 x float> [[TMP1]], <2 x float> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <2 x float>, <2 x float>, <2 x float> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <2 x float> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <2 x float>, <2 x float>, <2 x float> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <2 x float> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <2 x float>, <2 x float>, <2 x float> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <2 x float> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x float>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <2 x float>] poison, <2 x float> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <2 x float>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <2 x float>] [[TMP3]], <2 x float> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <2 x float>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <2 x float>] [[TMP4]], <2 x float> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_FLOAT32X2X3_T]] poison, [3 x <2 x float>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X2X3_T]] [[TMP5]]
//
float32x2x3_t test_vld3_lane_f32(float32_t  *a, float32x2x3_t b) {
  return vld3_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.float64x1x3_t @test_vld3_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x double>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld3lane.v1f64.p0(<1 x double> [[TMP0]], <1 x double> [[TMP1]], <1 x double> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <1 x double> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <1 x double> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <1 x double> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x double>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <1 x double>] poison, <1 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <1 x double>] [[TMP3]], <1 x double> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <1 x double>] [[TMP4]], <1 x double> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_FLOAT64X1X3_T]] poison, [3 x <1 x double>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X3_T]] [[TMP5]]
//
float64x1x3_t test_vld3_lane_f64(float64_t  *a, float64x1x3_t b) {
  return vld3_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.poly8x8x3_t @test_vld3_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X8X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld3lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <8 x i8> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <8 x i8>] [[TMP3]], <8 x i8> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <8 x i8>] [[TMP4]], <8 x i8> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_POLY8X8X3_T]] poison, [3 x <8 x i8>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY8X8X3_T]] [[TMP5]]
//
poly8x8x3_t test_vld3_lane_p8(poly8_t  *a, poly8x8x3_t b) {
  return vld3_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.poly16x4x3_t @test_vld3_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X4X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld3lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <4 x i16> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <4 x i16>] [[TMP3]], <4 x i16> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <4 x i16>] [[TMP4]], <4 x i16> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_POLY16X4X3_T]] poison, [3 x <4 x i16>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY16X4X3_T]] [[TMP5]]
//
poly16x4x3_t test_vld3_lane_p16(poly16_t  *a, poly16x4x3_t b) {
  return vld3_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x3_t @test_vld3_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X3_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X3_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[VLD3_LANE:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld3lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD3_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD3_LANE_ELT6:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT6]], ptr [[__RET_REPACK5]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD3_LANE_ELT8:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3_LANE]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD3_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(24) [[__RET]], i64 24, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = insertvalue [3 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT9:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK10:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT9]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [3 x <1 x i64>] [[TMP3]], <1 x i64> [[DOTUNPACK_UNPACK10]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT11:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK12:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT11]], align 8
// CHECK-NEXT:    [[DOTUNPACK13:%.*]] = insertvalue [3 x <1 x i64>] [[TMP4]], <1 x i64> [[DOTUNPACK_UNPACK12]], 2
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [[STRUCT_POLY64X1X3_T]] poison, [3 x <1 x i64>] [[DOTUNPACK13]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X3_T]] [[TMP5]]
//
poly64x1x3_t test_vld3_lane_p64(poly64_t  *a, poly64x1x3_t b) {
  return vld3_lane_p64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.uint8x16x4_t @test_vld4q_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X16X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X16X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld4lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], <16 x i8> [[TMP3]], i64 15, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <16 x i8>] [[TMP4]], <16 x i8> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <16 x i8>] [[TMP5]], <16 x i8> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <16 x i8>] [[TMP6]], <16 x i8> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT8X16X4_T]] poison, [4 x <16 x i8>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT8X16X4_T]] [[TMP7]]
//
uint8x16x4_t test_vld4q_lane_u8(uint8_t  *a, uint8x16x4_t b) {
  return vld4q_lane_u8(a, b, 15);
}

// CHECK-LABEL: define dso_local %struct.uint16x8x4_t @test_vld4q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X8X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld4lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], <8 x i16> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <8 x i16>] [[TMP4]], <8 x i16> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <8 x i16>] [[TMP5]], <8 x i16> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <8 x i16>] [[TMP6]], <8 x i16> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT16X8X4_T]] poison, [4 x <8 x i16>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT16X8X4_T]] [[TMP7]]
//
uint16x8x4_t test_vld4q_lane_u16(uint16_t  *a, uint16x8x4_t b) {
  return vld4q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.uint32x4x4_t @test_vld4q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X4X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i32>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld4lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], <4 x i32> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i32>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <4 x i32>] poison, <4 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <4 x i32>] [[TMP4]], <4 x i32> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <4 x i32>] [[TMP5]], <4 x i32> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <4 x i32>] [[TMP6]], <4 x i32> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT32X4X4_T]] poison, [4 x <4 x i32>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT32X4X4_T]] [[TMP7]]
//
uint32x4x4_t test_vld4q_lane_u32(uint32_t  *a, uint32x4x4_t b) {
  return vld4q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x4_t @test_vld4q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <2 x i64>] [[TMP4]], <2 x i64> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <2 x i64>] [[TMP5]], <2 x i64> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <2 x i64>] [[TMP6]], <2 x i64> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT64X2X4_T]] poison, [4 x <2 x i64>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X4_T]] [[TMP7]]
//
uint64x2x4_t test_vld4q_lane_u64(uint64_t  *a, uint64x2x4_t b) {
  return vld4q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.int8x16x4_t @test_vld4q_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X16X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X16X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld4lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], <16 x i8> [[TMP3]], i64 15, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <16 x i8>] [[TMP4]], <16 x i8> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <16 x i8>] [[TMP5]], <16 x i8> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <16 x i8>] [[TMP6]], <16 x i8> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT8X16X4_T]] poison, [4 x <16 x i8>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT8X16X4_T]] [[TMP7]]
//
int8x16x4_t test_vld4q_lane_s8(int8_t  *a, int8x16x4_t b) {
  return vld4q_lane_s8(a, b, 15);
}

// CHECK-LABEL: define dso_local %struct.int16x8x4_t @test_vld4q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X8X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld4lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], <8 x i16> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <8 x i16>] [[TMP4]], <8 x i16> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <8 x i16>] [[TMP5]], <8 x i16> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <8 x i16>] [[TMP6]], <8 x i16> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT16X8X4_T]] poison, [4 x <8 x i16>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT16X8X4_T]] [[TMP7]]
//
int16x8x4_t test_vld4q_lane_s16(int16_t  *a, int16x8x4_t b) {
  return vld4q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.int32x4x4_t @test_vld4q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X4X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i32>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld4lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], <4 x i32> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <4 x i32> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i32>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <4 x i32>] poison, <4 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <4 x i32>] [[TMP4]], <4 x i32> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <4 x i32>] [[TMP5]], <4 x i32> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <4 x i32>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <4 x i32>] [[TMP6]], <4 x i32> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT32X4X4_T]] poison, [4 x <4 x i32>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT32X4X4_T]] [[TMP7]]
//
int32x4x4_t test_vld4q_lane_s32(int32_t  *a, int32x4x4_t b) {
  return vld4q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.int64x2x4_t @test_vld4q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <2 x i64>] [[TMP4]], <2 x i64> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <2 x i64>] [[TMP5]], <2 x i64> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <2 x i64>] [[TMP6]], <2 x i64> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT64X2X4_T]] poison, [4 x <2 x i64>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X2X4_T]] [[TMP7]]
//
int64x2x4_t test_vld4q_lane_s64(int64_t  *a, int64x2x4_t b) {
  return vld4q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.float16x8x4_t @test_vld4q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <8 x half>, <8 x half>, <8 x half>, <8 x half> } @llvm.aarch64.neon.ld4lane.v8f16.p0(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], <8 x half> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <8 x half>, <8 x half>, <8 x half>, <8 x half> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <8 x half> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <8 x half>, <8 x half>, <8 x half>, <8 x half> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <8 x half> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <8 x half>, <8 x half>, <8 x half>, <8 x half> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <8 x half> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <8 x half>, <8 x half>, <8 x half>, <8 x half> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <8 x half> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x half>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <8 x half>] poison, <8 x half> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <8 x half>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <8 x half>] [[TMP4]], <8 x half> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <8 x half>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <8 x half>] [[TMP5]], <8 x half> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <8 x half>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <8 x half>] [[TMP6]], <8 x half> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_FLOAT16X8X4_T]] poison, [4 x <8 x half>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X8X4_T]] [[TMP7]]
//
float16x8x4_t test_vld4q_lane_f16(float16_t  *a, float16x8x4_t b) {
  return vld4q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.float32x4x4_t @test_vld4q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <4 x float>, <4 x float>, <4 x float>, <4 x float> } @llvm.aarch64.neon.ld4lane.v4f32.p0(<4 x float> [[TMP0]], <4 x float> [[TMP1]], <4 x float> [[TMP2]], <4 x float> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <4 x float>, <4 x float>, <4 x float>, <4 x float> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <4 x float> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <4 x float>, <4 x float>, <4 x float>, <4 x float> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <4 x float> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <4 x float>, <4 x float>, <4 x float>, <4 x float> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <4 x float> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <4 x float>, <4 x float>, <4 x float>, <4 x float> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <4 x float> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x float>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <4 x float>] poison, <4 x float> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <4 x float>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <4 x float>] [[TMP4]], <4 x float> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <4 x float>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <4 x float>] [[TMP5]], <4 x float> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <4 x float>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <4 x float>] [[TMP6]], <4 x float> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_FLOAT32X4X4_T]] poison, [4 x <4 x float>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X4X4_T]] [[TMP7]]
//
float32x4x4_t test_vld4q_lane_f32(float32_t  *a, float32x4x4_t b) {
  return vld4q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.float64x2x4_t @test_vld4q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <2 x double>, <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld4lane.v2f64.p0(<2 x double> [[TMP0]], <2 x double> [[TMP1]], <2 x double> [[TMP2]], <2 x double> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <2 x double> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <2 x double> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <2 x double> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <2 x double> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x double>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <2 x double>] poison, <2 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <2 x double>] [[TMP4]], <2 x double> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <2 x double>] [[TMP5]], <2 x double> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <2 x double>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <2 x double>] [[TMP6]], <2 x double> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_FLOAT64X2X4_T]] poison, [4 x <2 x double>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X4_T]] [[TMP7]]
//
float64x2x4_t test_vld4q_lane_f64(float64_t  *a, float64x2x4_t b) {
  return vld4q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.poly8x16x4_t @test_vld4q_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X16X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld4lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], <16 x i8> [[TMP3]], i64 15, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <16 x i8> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <16 x i8>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <16 x i8>] poison, <16 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <16 x i8>] [[TMP4]], <16 x i8> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <16 x i8>] [[TMP5]], <16 x i8> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <16 x i8>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <16 x i8>] [[TMP6]], <16 x i8> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_POLY8X16X4_T]] poison, [4 x <16 x i8>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY8X16X4_T]] [[TMP7]]
//
poly8x16x4_t test_vld4q_lane_p8(poly8_t  *a, poly8x16x4_t b) {
  return vld4q_lane_p8(a, b, 15);
}

// CHECK-LABEL: define dso_local %struct.poly16x8x4_t @test_vld4q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X8X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld4lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], <8 x i16> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <8 x i16> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i16>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <8 x i16>] poison, <8 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <8 x i16>] [[TMP4]], <8 x i16> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <8 x i16>] [[TMP5]], <8 x i16> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <8 x i16>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <8 x i16>] [[TMP6]], <8 x i16> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_POLY16X8X4_T]] poison, [4 x <8 x i16>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY16X8X4_T]] [[TMP7]]
//
poly16x8x4_t test_vld4q_lane_p16(poly16_t  *a, poly16x8x4_t b) {
  return vld4q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x4_t @test_vld4q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X4_T]], align 16
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X4_T]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 16
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 32
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 16
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 48
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <2 x i64> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[RETVAL]], ptr noundef nonnull align 16 dereferenceable(64) [[__RET]], i64 64, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <2 x i64>] poison, <2 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT13]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <2 x i64>] [[TMP4]], <2 x i64> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 32
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT15]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <2 x i64>] [[TMP5]], <2 x i64> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 48
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <2 x i64>, ptr [[DOTUNPACK_ELT17]], align 16
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <2 x i64>] [[TMP6]], <2 x i64> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_POLY64X2X4_T]] poison, [4 x <2 x i64>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X4_T]] [[TMP7]]
//
poly64x2x4_t test_vld4q_lane_p64(poly64_t  *a, poly64x2x4_t b) {
  return vld4q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.uint8x8x4_t @test_vld4_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X8X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld4lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], <8 x i8> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <8 x i8>] [[TMP4]], <8 x i8> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <8 x i8>] [[TMP5]], <8 x i8> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <8 x i8>] [[TMP6]], <8 x i8> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT8X8X4_T]] poison, [4 x <8 x i8>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT8X8X4_T]] [[TMP7]]
//
uint8x8x4_t test_vld4_lane_u8(uint8_t  *a, uint8x8x4_t b) {
  return vld4_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.uint16x4x4_t @test_vld4_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X4X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld4lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], <4 x i16> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <4 x i16>] [[TMP4]], <4 x i16> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <4 x i16>] [[TMP5]], <4 x i16> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <4 x i16>] [[TMP6]], <4 x i16> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT16X4X4_T]] poison, [4 x <4 x i16>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT16X4X4_T]] [[TMP7]]
//
uint16x4x4_t test_vld4_lane_u16(uint16_t  *a, uint16x4x4_t b) {
  return vld4_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.uint32x2x4_t @test_vld4_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X2X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld4lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], <2 x i32> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i32>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <2 x i32>] poison, <2 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <2 x i32>] [[TMP4]], <2 x i32> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <2 x i32>] [[TMP5]], <2 x i32> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <2 x i32>] [[TMP6]], <2 x i32> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT32X2X4_T]] poison, [4 x <2 x i32>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT32X2X4_T]] [[TMP7]]
//
uint32x2x4_t test_vld4_lane_u32(uint32_t  *a, uint32x2x4_t b) {
  return vld4_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.uint64x1x4_t @test_vld4_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X1X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld4lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], <1 x i64> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <1 x i64>] [[TMP4]], <1 x i64> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <1 x i64>] [[TMP5]], <1 x i64> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <1 x i64>] [[TMP6]], <1 x i64> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_UINT64X1X4_T]] poison, [4 x <1 x i64>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_UINT64X1X4_T]] [[TMP7]]
//
uint64x1x4_t test_vld4_lane_u64(uint64_t  *a, uint64x1x4_t b) {
  return vld4_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.int8x8x4_t @test_vld4_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X8X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld4lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], <8 x i8> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <8 x i8>] [[TMP4]], <8 x i8> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <8 x i8>] [[TMP5]], <8 x i8> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <8 x i8>] [[TMP6]], <8 x i8> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT8X8X4_T]] poison, [4 x <8 x i8>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT8X8X4_T]] [[TMP7]]
//
int8x8x4_t test_vld4_lane_s8(int8_t  *a, int8x8x4_t b) {
  return vld4_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.int16x4x4_t @test_vld4_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X4X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld4lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], <4 x i16> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <4 x i16>] [[TMP4]], <4 x i16> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <4 x i16>] [[TMP5]], <4 x i16> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <4 x i16>] [[TMP6]], <4 x i16> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT16X4X4_T]] poison, [4 x <4 x i16>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT16X4X4_T]] [[TMP7]]
//
int16x4x4_t test_vld4_lane_s16(int16_t  *a, int16x4x4_t b) {
  return vld4_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.int32x2x4_t @test_vld4_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X2X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld4lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], <2 x i32> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <2 x i32> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x i32>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <2 x i32>] poison, <2 x i32> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <2 x i32>] [[TMP4]], <2 x i32> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <2 x i32>] [[TMP5]], <2 x i32> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <2 x i32>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <2 x i32>] [[TMP6]], <2 x i32> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT32X2X4_T]] poison, [4 x <2 x i32>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT32X2X4_T]] [[TMP7]]
//
int32x2x4_t test_vld4_lane_s32(int32_t  *a, int32x2x4_t b) {
  return vld4_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.int64x1x4_t @test_vld4_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X1X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld4lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], <1 x i64> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <1 x i64>] [[TMP4]], <1 x i64> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <1 x i64>] [[TMP5]], <1 x i64> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <1 x i64>] [[TMP6]], <1 x i64> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_INT64X1X4_T]] poison, [4 x <1 x i64>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_INT64X1X4_T]] [[TMP7]]
//
int64x1x4_t test_vld4_lane_s64(int64_t  *a, int64x1x4_t b) {
  return vld4_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.float16x4x4_t @test_vld4_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x half>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x half>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x half>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <4 x half>, <4 x half>, <4 x half>, <4 x half> } @llvm.aarch64.neon.ld4lane.v4f16.p0(<4 x half> [[TMP0]], <4 x half> [[TMP1]], <4 x half> [[TMP2]], <4 x half> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <4 x half>, <4 x half>, <4 x half>, <4 x half> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <4 x half> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <4 x half>, <4 x half>, <4 x half>, <4 x half> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <4 x half> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <4 x half>, <4 x half>, <4 x half>, <4 x half> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <4 x half> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <4 x half>, <4 x half>, <4 x half>, <4 x half> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <4 x half> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x half>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <4 x half>] poison, <4 x half> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <4 x half>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <4 x half>] [[TMP4]], <4 x half> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <4 x half>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <4 x half>] [[TMP5]], <4 x half> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <4 x half>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <4 x half>] [[TMP6]], <4 x half> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_FLOAT16X4X4_T]] poison, [4 x <4 x half>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X4X4_T]] [[TMP7]]
//
float16x4x4_t test_vld4_lane_f16(float16_t  *a, float16x4x4_t b) {
  return vld4_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.float32x2x4_t @test_vld4_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x float>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x float>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x float>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <2 x float>, <2 x float>, <2 x float>, <2 x float> } @llvm.aarch64.neon.ld4lane.v2f32.p0(<2 x float> [[TMP0]], <2 x float> [[TMP1]], <2 x float> [[TMP2]], <2 x float> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <2 x float>, <2 x float>, <2 x float>, <2 x float> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <2 x float> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <2 x float>, <2 x float>, <2 x float>, <2 x float> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <2 x float> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <2 x float>, <2 x float>, <2 x float>, <2 x float> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <2 x float> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <2 x float>, <2 x float>, <2 x float>, <2 x float> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <2 x float> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <2 x float>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <2 x float>] poison, <2 x float> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <2 x float>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <2 x float>] [[TMP4]], <2 x float> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <2 x float>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <2 x float>] [[TMP5]], <2 x float> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <2 x float>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <2 x float>] [[TMP6]], <2 x float> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_FLOAT32X2X4_T]] poison, [4 x <2 x float>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X2X4_T]] [[TMP7]]
//
float32x2x4_t test_vld4_lane_f32(float32_t  *a, float32x2x4_t b) {
  return vld4_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local %struct.float64x1x4_t @test_vld4_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x double>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x double>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <1 x double>, <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld4lane.v1f64.p0(<1 x double> [[TMP0]], <1 x double> [[TMP1]], <1 x double> [[TMP2]], <1 x double> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <1 x double> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <1 x double> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <1 x double> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <1 x double> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x double>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <1 x double>] poison, <1 x double> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <1 x double>] [[TMP4]], <1 x double> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <1 x double>] [[TMP5]], <1 x double> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <1 x double>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <1 x double>] [[TMP6]], <1 x double> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_FLOAT64X1X4_T]] poison, [4 x <1 x double>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X4_T]] [[TMP7]]
//
float64x1x4_t test_vld4_lane_f64(float64_t  *a, float64x1x4_t b) {
  return vld4_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local %struct.poly8x8x4_t @test_vld4_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X8X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld4lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], <8 x i8> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <8 x i8> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <8 x i8>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <8 x i8>] poison, <8 x i8> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <8 x i8>] [[TMP4]], <8 x i8> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <8 x i8>] [[TMP5]], <8 x i8> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <8 x i8>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <8 x i8>] [[TMP6]], <8 x i8> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_POLY8X8X4_T]] poison, [4 x <8 x i8>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY8X8X4_T]] [[TMP7]]
//
poly8x8x4_t test_vld4_lane_p8(poly8_t  *a, poly8x8x4_t b) {
  return vld4_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local %struct.poly16x4x4_t @test_vld4_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X4X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld4lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], <4 x i16> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <4 x i16> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <4 x i16>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <4 x i16>] poison, <4 x i16> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <4 x i16>] [[TMP4]], <4 x i16> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <4 x i16>] [[TMP5]], <4 x i16> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <4 x i16>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <4 x i16>] [[TMP6]], <4 x i16> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_POLY16X4X4_T]] poison, [4 x <4 x i16>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY16X4X4_T]] [[TMP7]]
//
poly16x4x4_t test_vld4_lane_p16(poly16_t  *a, poly16x4x4_t b) {
  return vld4_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x4_t @test_vld4_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X4_T]], align 8
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X4_T]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[VLD4_LANE:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld4lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], <1 x i64> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    [[VLD4_LANE_ELT:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 0
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[__RET_REPACK7:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 8
// CHECK-NEXT:    [[VLD4_LANE_ELT8:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 1
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT8]], ptr [[__RET_REPACK7]], align 8
// CHECK-NEXT:    [[__RET_REPACK9:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 16
// CHECK-NEXT:    [[VLD4_LANE_ELT10:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 2
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT10]], ptr [[__RET_REPACK9]], align 8
// CHECK-NEXT:    [[__RET_REPACK11:%.*]] = getelementptr inbounds nuw i8, ptr [[__RET]], i64 24
// CHECK-NEXT:    [[VLD4_LANE_ELT12:%.*]] = extractvalue { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4_LANE]], 3
// CHECK-NEXT:    store <1 x i64> [[VLD4_LANE_ELT12]], ptr [[__RET_REPACK11]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[RETVAL]], ptr noundef nonnull align 8 dereferenceable(32) [[__RET]], i64 32, i1 false)
// CHECK-NEXT:    [[DOTUNPACK_UNPACK:%.*]] = load <1 x i64>, ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = insertvalue [4 x <1 x i64>] poison, <1 x i64> [[DOTUNPACK_UNPACK]], 0
// CHECK-NEXT:    [[DOTUNPACK_ELT13:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 8
// CHECK-NEXT:    [[DOTUNPACK_UNPACK14:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT13]], align 8
// CHECK-NEXT:    [[TMP5:%.*]] = insertvalue [4 x <1 x i64>] [[TMP4]], <1 x i64> [[DOTUNPACK_UNPACK14]], 1
// CHECK-NEXT:    [[DOTUNPACK_ELT15:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 16
// CHECK-NEXT:    [[DOTUNPACK_UNPACK16:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT15]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = insertvalue [4 x <1 x i64>] [[TMP5]], <1 x i64> [[DOTUNPACK_UNPACK16]], 2
// CHECK-NEXT:    [[DOTUNPACK_ELT17:%.*]] = getelementptr inbounds nuw i8, ptr [[RETVAL]], i64 24
// CHECK-NEXT:    [[DOTUNPACK_UNPACK18:%.*]] = load <1 x i64>, ptr [[DOTUNPACK_ELT17]], align 8
// CHECK-NEXT:    [[DOTUNPACK19:%.*]] = insertvalue [4 x <1 x i64>] [[TMP6]], <1 x i64> [[DOTUNPACK_UNPACK18]], 3
// CHECK-NEXT:    [[TMP7:%.*]] = insertvalue [[STRUCT_POLY64X1X4_T]] poison, [4 x <1 x i64>] [[DOTUNPACK19]], 0
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X4_T]] [[TMP7]]
//
poly64x1x4_t test_vld4_lane_p64(poly64_t  *a, poly64x1x4_t b) {
  return vld4_lane_p64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <16 x i8> [[B]], i64 15
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[A]], align 1
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_u8(uint8_t  *a, uint8x16_t b) {
  vst1q_lane_u8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i16> [[B]], i64 7
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_u16(uint16_t  *a, uint16x8_t b) {
  vst1q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i32> [[B]], i64 3
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_u32(uint32_t  *a, uint32x4_t b) {
  vst1q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[B]], i64 1
// CHECK-NEXT:    store i64 [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_u64(uint64_t  *a, uint64x2_t b) {
  vst1q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <16 x i8> [[B]], i64 15
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[A]], align 1
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_s8(int8_t  *a, int8x16_t b) {
  vst1q_lane_s8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i16> [[B]], i64 7
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_s16(int16_t  *a, int16x8_t b) {
  vst1q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i32> [[B]], i64 3
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_s32(int32_t  *a, int32x4_t b) {
  vst1q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[B]], i64 1
// CHECK-NEXT:    store i64 [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_s64(int64_t  *a, int64x2_t b) {
  vst1q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x half> [[B]], i64 7
// CHECK-NEXT:    store half [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_f16(float16_t  *a, float16x8_t b) {
  vst1q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x float> [[B]], i64 3
// CHECK-NEXT:    store float [[TMP0]], ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_f32(float32_t  *a, float32x4_t b) {
  vst1q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x double> [[B]], i64 1
// CHECK-NEXT:    store double [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_f64(float64_t  *a, float64x2_t b) {
  vst1q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <16 x i8> [[B]], i64 15
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[A]], align 1
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_p8(poly8_t  *a, poly8x16_t b) {
  vst1q_lane_p8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i16> [[B]], i64 7
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_p16(poly16_t  *a, poly16x8_t b) {
  vst1q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst1q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i64> [[B]], i64 1
// CHECK-NEXT:    store i64 [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1q_lane_p64(poly64_t  *a, poly64x2_t b) {
  vst1q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i8> [[B]], i64 7
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[A]], align 1
// CHECK-NEXT:    ret void
//
void test_vst1_lane_u8(uint8_t  *a, uint8x8_t b) {
  vst1_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i16> [[B]], i64 3
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1_lane_u16(uint16_t  *a, uint16x4_t b) {
  vst1_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i32> [[B]], i64 1
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
void test_vst1_lane_u32(uint32_t  *a, uint32x2_t b) {
  vst1_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <1 x i64> [[B]], i64 0
// CHECK-NEXT:    store i64 [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1_lane_u64(uint64_t  *a, uint64x1_t b) {
  vst1_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i8> [[B]], i64 7
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[A]], align 1
// CHECK-NEXT:    ret void
//
void test_vst1_lane_s8(int8_t  *a, int8x8_t b) {
  vst1_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i16> [[B]], i64 3
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1_lane_s16(int16_t  *a, int16x4_t b) {
  vst1_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x i32> [[B]], i64 1
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
void test_vst1_lane_s32(int32_t  *a, int32x2_t b) {
  vst1_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <1 x i64> [[B]], i64 0
// CHECK-NEXT:    store i64 [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1_lane_s64(int64_t  *a, int64x1_t b) {
  vst1_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x half> [[B]], i64 3
// CHECK-NEXT:    store half [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1_lane_f16(float16_t  *a, float16x4_t b) {
  vst1_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <2 x float> [[B]], i64 1
// CHECK-NEXT:    store float [[TMP0]], ptr [[A]], align 4
// CHECK-NEXT:    ret void
//
void test_vst1_lane_f32(float32_t  *a, float32x2_t b) {
  vst1_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <1 x double> [[B]], i64 0
// CHECK-NEXT:    store double [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1_lane_f64(float64_t  *a, float64x1_t b) {
  vst1_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <8 x i8> [[B]], i64 7
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[A]], align 1
// CHECK-NEXT:    ret void
//
void test_vst1_lane_p8(poly8_t  *a, poly8x8_t b) {
  vst1_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <4 x i16> [[B]], i64 3
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[A]], align 2
// CHECK-NEXT:    ret void
//
void test_vst1_lane_p16(poly16_t  *a, poly16x4_t b) {
  vst1_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst1_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[TMP0:%.*]] = extractelement <1 x i64> [[B]], i64 0
// CHECK-NEXT:    store i64 [[TMP0]], ptr [[A]], align 8
// CHECK-NEXT:    ret void
//
void test_vst1_lane_p64(poly64_t  *a, poly64x1_t b) {
  vst1_lane_p64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_u8(uint8_t  *a, uint8x16x2_t b) {
  vst2q_lane_u8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_u16(uint16_t  *a, uint16x8x2_t b) {
  vst2q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_u32(uint32_t  *a, uint32x4x2_t b) {
  vst2q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_u64(uint64_t  *a, uint64x2x2_t b) {
  vst2q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_s8(int8_t  *a, int8x16x2_t b) {
  vst2q_lane_s8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_s16(int16_t  *a, int16x8x2_t b) {
  vst2q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_s32(int32_t  *a, int32x4x2_t b) {
  vst2q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_s64(int64_t  *a, int64x2x2_t b) {
  vst2q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v8f16.p0(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_f16(float16_t  *a, float16x8x2_t b) {
  vst2q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v4f32.p0(<4 x float> [[TMP0]], <4 x float> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_f32(float32_t  *a, float32x4x2_t b) {
  vst2q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v2f64.p0(<2 x double> [[TMP0]], <2 x double> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_f64(float64_t  *a, float64x2x2_t b) {
  vst2q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_p8(poly8_t  *a, poly8x16x2_t b) {
  vst2q_lane_p8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_p16(poly16_t  *a, poly16x8x2_t b) {
  vst2q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst2q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X2_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(32) [[__S1]], ptr noundef nonnull align 16 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2q_lane_p64(poly64_t  *a, poly64x2x2_t b) {
  vst2q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_u8(uint8_t  *a, uint8x8x2_t b) {
  vst2_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_u16(uint16_t  *a, uint16x4x2_t b) {
  vst2_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_u32(uint32_t  *a, uint32x2x2_t b) {
  vst2_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_u64(uint64_t  *a, uint64x1x2_t b) {
  vst2_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_s8(int8_t  *a, int8x8x2_t b) {
  vst2_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_s16(int16_t  *a, int16x4x2_t b) {
  vst2_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_s32(int32_t  *a, int32x2x2_t b) {
  vst2_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_s64(int64_t  *a, int64x1x2_t b) {
  vst2_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x half>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v4f16.p0(<4 x half> [[TMP0]], <4 x half> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_f16(float16_t  *a, float16x4x2_t b) {
  vst2_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <2 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <2 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x float>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v2f32.p0(<2 x float> [[TMP0]], <2 x float> [[TMP1]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_f32(float32_t  *a, float32x2x2_t b) {
  vst2_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x double>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v1f64.p0(<1 x double> [[TMP0]], <1 x double> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_f64(float64_t  *a, float64x1x2_t b) {
  vst2_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_p8(poly8_t  *a, poly8x8x2_t b) {
  vst2_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_p16(poly16_t  *a, poly16x4x2_t b) {
  vst2_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst2_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X2_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [2 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(16) [[__S1]], ptr noundef nonnull align 8 dereferenceable(16) [[B]], i64 16, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst2_lane_p64(poly64_t  *a, poly64x1x2_t b) {
  vst2_lane_p64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_u8(uint8_t  *a, uint8x16x3_t b) {
  vst3q_lane_u8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_u16(uint16_t  *a, uint16x8x3_t b) {
  vst3q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_u32(uint32_t  *a, uint32x4x3_t b) {
  vst3q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_u64(uint64_t  *a, uint64x2x3_t b) {
  vst3q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_s8(int8_t  *a, int8x16x3_t b) {
  vst3q_lane_s8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_s16(int16_t  *a, int16x8x3_t b) {
  vst3q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_s32(int32_t  *a, int32x4x3_t b) {
  vst3q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_s64(int64_t  *a, int64x2x3_t b) {
  vst3q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v8f16.p0(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_f16(float16_t  *a, float16x8x3_t b) {
  vst3q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v4f32.p0(<4 x float> [[TMP0]], <4 x float> [[TMP1]], <4 x float> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_f32(float32_t  *a, float32x4x3_t b) {
  vst3q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v2f64.p0(<2 x double> [[TMP0]], <2 x double> [[TMP1]], <2 x double> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_f64(float64_t  *a, float64x2x3_t b) {
  vst3q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_p8(poly8_t  *a, poly8x16x3_t b) {
  vst3q_lane_p8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_p16(poly16_t  *a, poly16x8x3_t b) {
  vst3q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst3q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X3_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(48) [[__S1]], ptr noundef nonnull align 16 dereferenceable(48) [[B]], i64 48, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3q_lane_p64(poly64_t  *a, poly64x2x3_t b) {
  vst3q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_u8(uint8_t  *a, uint8x8x3_t b) {
  vst3_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_u16(uint16_t  *a, uint16x4x3_t b) {
  vst3_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_u32(uint32_t  *a, uint32x2x3_t b) {
  vst3_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_u64(uint64_t  *a, uint64x1x3_t b) {
  vst3_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_s8(int8_t  *a, int8x8x3_t b) {
  vst3_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_s16(int16_t  *a, int16x4x3_t b) {
  vst3_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_s32(int32_t  *a, int32x2x3_t b) {
  vst3_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_s64(int64_t  *a, int64x1x3_t b) {
  vst3_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x half>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x half>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v4f16.p0(<4 x half> [[TMP0]], <4 x half> [[TMP1]], <4 x half> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_f16(float16_t  *a, float16x4x3_t b) {
  vst3_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <2 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <2 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <2 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x float>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x float>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v2f32.p0(<2 x float> [[TMP0]], <2 x float> [[TMP1]], <2 x float> [[TMP2]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_f32(float32_t  *a, float32x2x3_t b) {
  vst3_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x double>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v1f64.p0(<1 x double> [[TMP0]], <1 x double> [[TMP1]], <1 x double> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_f64(float64_t  *a, float64x1x3_t b) {
  vst3_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_p8(poly8_t  *a, poly8x8x3_t b) {
  vst3_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_p16(poly16_t  *a, poly16x4x3_t b) {
  vst3_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst3_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X3_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [3 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(24) [[__S1]], ptr noundef nonnull align 8 dereferenceable(24) [[B]], i64 24, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst3_lane_p64(poly64_t  *a, poly64x1x3_t b) {
  vst3_lane_p64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], <16 x i8> [[TMP3]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_u8(uint8_t  *a, uint8x16x4_t b) {
  vst4q_lane_u8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], <8 x i16> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_u16(uint16_t  *a, uint16x8x4_t b) {
  vst4q_lane_u16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i32>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], <4 x i32> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_u32(uint32_t  *a, uint32x4x4_t b) {
  vst4q_lane_u32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_u64(uint64_t  *a, uint64x2x4_t b) {
  vst4q_lane_u64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], <16 x i8> [[TMP3]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_s8(int8_t  *a, int8x16x4_t b) {
  vst4q_lane_s8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], <8 x i16> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_s16(int16_t  *a, int16x8x4_t b) {
  vst4q_lane_s16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i32>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v4i32.p0(<4 x i32> [[TMP0]], <4 x i32> [[TMP1]], <4 x i32> [[TMP2]], <4 x i32> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_s32(int32_t  *a, int32x4x4_t b) {
  vst4q_lane_s32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_s64(int64_t  *a, int64x2x4_t b) {
  vst4q_lane_s64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x half>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x half> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v8f16.p0(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], <8 x half> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_f16(float16_t  *a, float16x8x4_t b) {
  vst4q_lane_f16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x float>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x float> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v4f32.p0(<4 x float> [[TMP0]], <4 x float> [[TMP1]], <4 x float> [[TMP2]], <4 x float> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_f32(float32_t  *a, float32x4x4_t b) {
  vst4q_lane_f32(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x double>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x double> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v2f64.p0(<2 x double> [[TMP0]], <2 x double> [[TMP1]], <2 x double> [[TMP2]], <2 x double> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_f64(float64_t  *a, float64x2x4_t b) {
  vst4q_lane_f64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <16 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <16 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v16i8.p0(<16 x i8> [[TMP0]], <16 x i8> [[TMP1]], <16 x i8> [[TMP2]], <16 x i8> [[TMP3]], i64 15, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_p8(poly8_t  *a, poly8x16x4_t b) {
  vst4q_lane_p8(a, b, 15);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v8i16.p0(<8 x i16> [[TMP0]], <8 x i16> [[TMP1]], <8 x i16> [[TMP2]], <8 x i16> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_p16(poly16_t  *a, poly16x8x4_t b) {
  vst4q_lane_p16(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst4q_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X4_T]], align 16
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT]], ptr [[B]], align 16
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 16
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 32
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 16
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 48
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 16
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 16 dereferenceable(64) [[__S1]], ptr noundef nonnull align 16 dereferenceable(64) [[B]], i64 64, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 48
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v2i64.p0(<2 x i64> [[TMP0]], <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4q_lane_p64(poly64_t  *a, poly64x2x4_t b) {
  vst4q_lane_p64(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], <8 x i8> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_u8(uint8_t  *a, uint8x8x4_t b) {
  vst4_lane_u8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], <4 x i16> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_u16(uint16_t  *a, uint16x4x4_t b) {
  vst4_lane_u16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], <2 x i32> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_u32(uint32_t  *a, uint32x2x4_t b) {
  vst4_lane_u32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], <1 x i64> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_u64(uint64_t  *a, uint64x1x4_t b) {
  vst4_lane_u64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], <8 x i8> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_s8(int8_t  *a, int8x8x4_t b) {
  vst4_lane_s8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], <4 x i16> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_s16(int16_t  *a, int16x4x4_t b) {
  vst4_lane_s16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x i32>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x i32> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x i32>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v2i32.p0(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]], <2 x i32> [[TMP2]], <2 x i32> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_s32(int32_t  *a, int32x2x4_t b) {
  vst4_lane_s32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], <1 x i64> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_s64(int64_t  *a, int64x1x4_t b) {
  vst4_lane_s64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x half>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x half> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x half>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x half>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x half>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v4f16.p0(<4 x half> [[TMP0]], <4 x half> [[TMP1]], <4 x half> [[TMP2]], <4 x half> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_f16(float16_t  *a, float16x4x4_t b) {
  vst4_lane_f16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 0
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 1
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 2
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <2 x float>] [[B_COERCE]], 3
// CHECK-NEXT:    store <2 x float> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x float>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x float>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x float>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v2f32.p0(<2 x float> [[TMP0]], <2 x float> [[TMP1]], <2 x float> [[TMP2]], <2 x float> [[TMP3]], i64 1, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_f32(float32_t  *a, float32x2x4_t b) {
  vst4_lane_f32(a, b, 1);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x double>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x double> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x double>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x double>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v1f64.p0(<1 x double> [[TMP0]], <1 x double> [[TMP1]], <1 x double> [[TMP2]], <1 x double> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_f64(float64_t  *a, float64x1x4_t b) {
  vst4_lane_f64(a, b, 0);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 0
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 1
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 2
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <8 x i8>] [[B_COERCE]], 3
// CHECK-NEXT:    store <8 x i8> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v8i8.p0(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]], <8 x i8> [[TMP2]], <8 x i8> [[TMP3]], i64 7, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_p8(poly8_t  *a, poly8x8x4_t b) {
  vst4_lane_p8(a, b, 7);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 0
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 1
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 2
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <4 x i16>] [[B_COERCE]], 3
// CHECK-NEXT:    store <4 x i16> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v4i16.p0(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]], <4 x i16> [[TMP2]], <4 x i16> [[TMP3]], i64 3, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_p16(poly16_t  *a, poly16x4x4_t b) {
  vst4_lane_p16(a, b, 3);
}

// CHECK-LABEL: define dso_local void @test_vst4_lane_p64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X4_T]], align 8
// CHECK-NEXT:    [[B_COERCE_ELT:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 0
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT]], ptr [[B]], align 8
// CHECK-NEXT:    [[B_REPACK1:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 8
// CHECK-NEXT:    [[B_COERCE_ELT2:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 1
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT2]], ptr [[B_REPACK1]], align 8
// CHECK-NEXT:    [[B_REPACK3:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 16
// CHECK-NEXT:    [[B_COERCE_ELT4:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 2
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT4]], ptr [[B_REPACK3]], align 8
// CHECK-NEXT:    [[B_REPACK5:%.*]] = getelementptr inbounds nuw i8, ptr [[B]], i64 24
// CHECK-NEXT:    [[B_COERCE_ELT6:%.*]] = extractvalue [4 x <1 x i64>] [[B_COERCE]], 3
// CHECK-NEXT:    store <1 x i64> [[B_COERCE_ELT6]], ptr [[B_REPACK5]], align 8
// CHECK-NEXT:    call void @llvm.memcpy.p0.p0.i64(ptr noundef nonnull align 8 dereferenceable(32) [[__S1]], ptr noundef nonnull align 8 dereferenceable(32) [[B]], i64 32, i1 false)
// CHECK-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds nuw i8, ptr [[__S1]], i64 24
// CHECK-NEXT:    [[TMP3:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4lane.v1i64.p0(<1 x i64> [[TMP0]], <1 x i64> [[TMP1]], <1 x i64> [[TMP2]], <1 x i64> [[TMP3]], i64 0, ptr [[A]])
// CHECK-NEXT:    ret void
//
void test_vst4_lane_p64(poly64_t  *a, poly64x1x4_t b) {
  vst4_lane_p64(a, b, 0);
}
