// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 5
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=x86_64-- -target-feature +avx -emit-llvm -o - | FileCheck %s --check-prefix=CHECK-X32
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386-- -target-feature +avx -emit-llvm -o - | FileCheck %s --check-prefix=CHECK-X64
// FIXME: The shufflevector instructions in test_cmpgt_sd are relying on O3 here.


#include <immintrin.h>

//
// Test LLVM IR codegen of cmpXY instructions
//

// CHECK-LABEL: define dso_local <2 x double> @test_cmp_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP0]], <2 x double> [[TMP1]], i8 13)
// CHECK-NEXT:    ret <2 x double> [[TMP2]]
// CHECK-X32-LABEL: define dso_local <2 x double> @test_cmp_sd(
// CHECK-X32-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP0]], <2 x double> [[TMP1]], i8 13)
// CHECK-X32-NEXT:    ret <2 x double> [[TMP2]]
//
// CHECK-X64-LABEL: define dso_local <2 x double> @test_cmp_sd(
// CHECK-X64-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP0]], <2 x double> [[TMP1]], i8 13)
// CHECK-X64-NEXT:    ret <2 x double> [[TMP2]]
//
__m128d test_cmp_sd(__m128d a, __m128d b) {
  // Expects that the third argument in LLVM IR is immediate expression
  return _mm_cmp_sd(a, b, _CMP_GE_OS);
}

// CHECK-LABEL: define dso_local <4 x float> @test_cmp_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP0]], <4 x float> [[TMP1]], i8 13)
// CHECK-NEXT:    ret <4 x float> [[TMP2]]
// CHECK-X32-LABEL: define dso_local <4 x float> @test_cmp_ss(
// CHECK-X32-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP0]], <4 x float> [[TMP1]], i8 13)
// CHECK-X32-NEXT:    ret <4 x float> [[TMP2]]
//
// CHECK-X64-LABEL: define dso_local <4 x float> @test_cmp_ss(
// CHECK-X64-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP0]], <4 x float> [[TMP1]], i8 13)
// CHECK-X64-NEXT:    ret <4 x float> [[TMP2]]
//
__m128 test_cmp_ss(__m128 a, __m128 b) {
  // Expects that the third argument in LLVM IR is immediate expression
  return _mm_cmp_ss(a, b, _CMP_GE_OS);
}

// CHECK-LABEL: define dso_local <4 x float> @test_cmpgt_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 1)
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x float> [[SHUFFLE_I]]
// CHECK-X32-LABEL: define dso_local <4 x float> @test_cmpgt_ss(
// CHECK-X32-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 1)
// CHECK-X32-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X32-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
// CHECK-X64-LABEL: define dso_local <4 x float> @test_cmpgt_ss(
// CHECK-X64-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 1)
// CHECK-X64-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X64-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
__m128 test_cmpgt_ss(__m128 a, __m128 b) {
  return _mm_cmpgt_ss(a, b);
}

// CHECK-LABEL: define dso_local <4 x float> @test_cmpge_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 2)
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x float> [[SHUFFLE_I]]
// CHECK-X32-LABEL: define dso_local <4 x float> @test_cmpge_ss(
// CHECK-X32-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 2)
// CHECK-X32-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X32-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
// CHECK-X64-LABEL: define dso_local <4 x float> @test_cmpge_ss(
// CHECK-X64-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 2)
// CHECK-X64-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X64-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
__m128 test_cmpge_ss(__m128 a, __m128 b) {
  return _mm_cmpge_ss(a, b);
}

// CHECK-LABEL: define dso_local <4 x float> @test_cmpngt_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 5)
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x float> [[SHUFFLE_I]]
// CHECK-X32-LABEL: define dso_local <4 x float> @test_cmpngt_ss(
// CHECK-X32-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 5)
// CHECK-X32-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X32-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
// CHECK-X64-LABEL: define dso_local <4 x float> @test_cmpngt_ss(
// CHECK-X64-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 5)
// CHECK-X64-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X64-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
__m128 test_cmpngt_ss(__m128 a, __m128 b) {
  return _mm_cmpngt_ss(a, b);
}

// CHECK-LABEL: define dso_local <4 x float> @test_cmpnge_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 6)
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-NEXT:    ret <4 x float> [[SHUFFLE_I]]
// CHECK-X32-LABEL: define dso_local <4 x float> @test_cmpnge_ss(
// CHECK-X32-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X32-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 6)
// CHECK-X32-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X32-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
// CHECK-X64-LABEL: define dso_local <4 x float> @test_cmpnge_ss(
// CHECK-X64-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-X64-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 6)
// CHECK-X64-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP5]], <4 x i32> <i32 4, i32 1, i32 2, i32 3>
// CHECK-X64-NEXT:    ret <4 x float> [[SHUFFLE_I]]
//
__m128 test_cmpnge_ss(__m128 a, __m128 b) {
  return _mm_cmpnge_ss(a, b);
}

// CHECK-LABEL: define dso_local <2 x double> @test_cmpgt_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 1)
// CHECK-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    ret <2 x double> [[TMP7]]
// CHECK-X32-LABEL: define dso_local <2 x double> @test_cmpgt_sd(
// CHECK-X32-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 1)
// CHECK-X32-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X32-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X32-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X32-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X32-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    ret <2 x double> [[TMP7]]
//
// CHECK-X64-LABEL: define dso_local <2 x double> @test_cmpgt_sd(
// CHECK-X64-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 1)
// CHECK-X64-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X64-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X64-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X64-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X64-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    ret <2 x double> [[TMP7]]
//
__m128d test_cmpgt_sd(__m128d a, __m128d b) {
  return _mm_cmpgt_sd(a, b);
}

// CHECK-LABEL: define dso_local <2 x double> @test_cmpge_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 2)
// CHECK-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    ret <2 x double> [[TMP7]]
// CHECK-X32-LABEL: define dso_local <2 x double> @test_cmpge_sd(
// CHECK-X32-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 2)
// CHECK-X32-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X32-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X32-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X32-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X32-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    ret <2 x double> [[TMP7]]
//
// CHECK-X64-LABEL: define dso_local <2 x double> @test_cmpge_sd(
// CHECK-X64-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 2)
// CHECK-X64-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X64-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X64-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X64-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X64-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    ret <2 x double> [[TMP7]]
//
__m128d test_cmpge_sd(__m128d a, __m128d b) {
  return _mm_cmpge_sd(a, b);
}

// CHECK-LABEL: define dso_local <2 x double> @test_cmpngt_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 5)
// CHECK-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    ret <2 x double> [[TMP7]]
// CHECK-X32-LABEL: define dso_local <2 x double> @test_cmpngt_sd(
// CHECK-X32-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 5)
// CHECK-X32-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X32-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X32-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X32-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X32-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    ret <2 x double> [[TMP7]]
//
// CHECK-X64-LABEL: define dso_local <2 x double> @test_cmpngt_sd(
// CHECK-X64-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 5)
// CHECK-X64-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X64-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X64-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X64-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X64-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    ret <2 x double> [[TMP7]]
//
__m128d test_cmpngt_sd(__m128d a, __m128d b) {
  return _mm_cmpngt_sd(a, b);
}

// CHECK-LABEL: define dso_local <2 x double> @test_cmpnge_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 6)
// CHECK-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    ret <2 x double> [[TMP7]]
// CHECK-X32-LABEL: define dso_local <2 x double> @test_cmpnge_sd(
// CHECK-X32-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X32-NEXT:  [[ENTRY:.*:]]
// CHECK-X32-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X32-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X32-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 6)
// CHECK-X32-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X32-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X32-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X32-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X32-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X32-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X32-NEXT:    ret <2 x double> [[TMP7]]
//
// CHECK-X64-LABEL: define dso_local <2 x double> @test_cmpnge_sd(
// CHECK-X64-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-X64-NEXT:  [[ENTRY:.*:]]
// CHECK-X64-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-X64-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-X64-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 6)
// CHECK-X64-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// CHECK-X64-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// CHECK-X64-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-X64-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// CHECK-X64-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// CHECK-X64-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-X64-NEXT:    ret <2 x double> [[TMP7]]
//
__m128d test_cmpnge_sd(__m128d a, __m128d b) {
  return _mm_cmpnge_sd(a, b);
}
