// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 4
// RUN: %clang_cc1 -triple arm64-none-linux-gnu -target-feature +neon \
// RUN:     -S \
// RUN:  -flax-vector-conversions=none -emit-llvm -o - %s -fsanitize=memory \
// RUN: | FileCheck %s

// REQUIRES: aarch64-registered-target || arm-registered-target

// Forked from aarch64-neon-intrinsics.c

#include <arm_neon.h>

// CHECK-LABEL: define dso_local noundef <16 x i8> @test_vld1q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4:[0-9]+]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2:![0-9]+]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7:[0-9]+]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <16 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <16 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <16 x i8>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <16 x i8>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <16 x i8> [[TMP32]]
//
uint8x16_t test_vld1q_u8(uint8_t const *a) {
  return vld1q_u8(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i16> @test_vld1q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i16>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i16>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i16>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i16> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i16> [[TMP32]]
//
uint16x8_t test_vld1q_u16(uint16_t const *a) {
  return vld1q_u16(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i32> @test_vld1q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i32>, ptr [[TMP12]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i32>, ptr [[TMP21]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <4 x i32> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i32>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i32>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <4 x i32> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i32>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i32>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i32> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i32> [[TMP32]]
//
uint32x4_t test_vld1q_u32(uint32_t const *a) {
  return vld1q_u32(a);
}

// CHECK-LABEL: define dso_local noundef <2 x i64> @test_vld1q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x i64>, ptr [[TMP12]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i64>, ptr [[TMP21]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x i64>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i64>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x i64>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i64> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x i64> [[TMP32]]
//
uint64x2_t test_vld1q_u64(uint64_t const *a) {
  return vld1q_u64(a);
}

// CHECK-LABEL: define dso_local noundef <16 x i8> @test_vld1q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <16 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <16 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <16 x i8>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <16 x i8>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <16 x i8> [[TMP32]]
//
int8x16_t test_vld1q_s8(int8_t const *a) {
  return vld1q_s8(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i16> @test_vld1q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i16>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i16>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i16>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i16> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i16> [[TMP32]]
//
int16x8_t test_vld1q_s16(int16_t const *a) {
  return vld1q_s16(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i32> @test_vld1q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i32>, ptr [[TMP12]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i32>, ptr [[TMP21]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <4 x i32> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i32>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i32>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <4 x i32> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i32>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i32>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i32> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i32> [[TMP32]]
//
int32x4_t test_vld1q_s32(int32_t const *a) {
  return vld1q_s32(a);
}

// CHECK-LABEL: define dso_local noundef <2 x i64> @test_vld1q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x i64>, ptr [[TMP12]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i64>, ptr [[TMP21]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x i64>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i64>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x i64>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i64> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x i64> [[TMP32]]
//
int64x2_t test_vld1q_s64(int64_t const *a) {
  return vld1q_s64(a);
}

// CHECK-LABEL: define dso_local noundef <8 x half> @test_vld1q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x half>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x half>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x half>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i16> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x half> [[TMP32]]
//
float16x8_t test_vld1q_f16(float16_t const *a) {
  return vld1q_f16(a);
}

// CHECK-LABEL: define dso_local noundef <4 x float> @test_vld1q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x float>, ptr [[TMP12]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i32>, ptr [[TMP21]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x float>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i32>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x float>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i32>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i32> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x float> [[TMP32]]
//
float32x4_t test_vld1q_f32(float32_t const *a) {
  return vld1q_f32(a);
}

// CHECK-LABEL: define dso_local noundef <2 x double> @test_vld1q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x double>, ptr [[TMP12]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i64>, ptr [[TMP21]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x double>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i64>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x double>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i64> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x double> [[TMP32]]
//
float64x2_t test_vld1q_f64(float64_t const *a) {
  return vld1q_f64(a);
}

// CHECK-LABEL: define dso_local noundef <16 x i8> @test_vld1q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <16 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <16 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <16 x i8>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <16 x i8>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <16 x i8> [[TMP32]]
//
poly8x16_t test_vld1q_p8(poly8_t const *a) {
  return vld1q_p8(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i16> @test_vld1q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i16>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD1]], ptr [[TMP24]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP18]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i16>, ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD2]], ptr [[TMP31]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP25]], ptr [[TMP]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i16>, ptr [[TMP]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP35]], align 16
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i16> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i128 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i16> [[TMP32]]
//
poly16x8_t test_vld1q_p16(poly16_t const *a) {
  return vld1q_p16(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i8> @test_vld1_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i8>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i8> [[TMP32]]
//
uint8x8_t test_vld1_u8(uint8_t const *a) {
  return vld1_u8(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i16> @test_vld1_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i16>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i16>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i16>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i16> [[TMP32]]
//
uint16x4_t test_vld1_u16(uint16_t const *a) {
  return vld1_u16(a);
}

// CHECK-LABEL: define dso_local noundef <2 x i32> @test_vld1_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x i32>, ptr [[TMP12]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i32>, ptr [[TMP21]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x i32>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x i32>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i32> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x i32> [[TMP32]]
//
uint32x2_t test_vld1_u32(uint32_t const *a) {
  return vld1_u32(a);
}

// CHECK-LABEL: define dso_local noundef <1 x i64> @test_vld1_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <1 x i64>, ptr [[TMP12]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <1 x i64>, ptr [[TMP21]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <1 x i64>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <1 x i64>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <1 x i64> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <1 x i64> [[TMP32]]
//
uint64x1_t test_vld1_u64(uint64_t const *a) {
  return vld1_u64(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i8> @test_vld1_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i8>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i8> [[TMP32]]
//
int8x8_t test_vld1_s8(int8_t const *a) {
  return vld1_s8(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i16> @test_vld1_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i16>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i16>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i16>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i16> [[TMP32]]
//
int16x4_t test_vld1_s16(int16_t const *a) {
  return vld1_s16(a);
}

// CHECK-LABEL: define dso_local noundef <2 x i32> @test_vld1_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x i32>, ptr [[TMP12]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i32>, ptr [[TMP21]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x i32>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x i32>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i32> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x i32> [[TMP32]]
//
int32x2_t test_vld1_s32(int32_t const *a) {
  return vld1_s32(a);
}

// CHECK-LABEL: define dso_local noundef <1 x i64> @test_vld1_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <1 x i64>, ptr [[TMP12]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <1 x i64>, ptr [[TMP21]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <1 x i64>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <1 x i64>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <1 x i64> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <1 x i64> [[TMP32]]
//
int64x1_t test_vld1_s64(int64_t const *a) {
  return vld1_s64(a);
}

// CHECK-LABEL: define dso_local noundef <4 x half> @test_vld1_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x half>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x half>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x half>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x half> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x half>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x half> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x half>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x half> [[TMP32]]
//
float16x4_t test_vld1_f16(float16_t const *a) {
  return vld1_f16(a);
}

// CHECK-LABEL: define dso_local noundef <2 x float> @test_vld1_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x float>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x float>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x float>, ptr [[TMP12]], align 4
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i32>, ptr [[TMP21]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <2 x float> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x float>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <2 x float> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x float>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i32> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x float> [[TMP32]]
//
float32x2_t test_vld1_f32(float32_t const *a) {
  return vld1_f32(a);
}

// CHECK-LABEL: define dso_local noundef <1 x double> @test_vld1_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <1 x double>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <1 x double>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <1 x double>, ptr [[TMP12]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <1 x i64>, ptr [[TMP21]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <1 x double> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <1 x double>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <1 x double> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <1 x double>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <1 x i64> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <1 x double> [[TMP32]]
//
float64x1_t test_vld1_f64(float64_t const *a) {
  return vld1_f64(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i8> @test_vld1_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i8>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i8> [[TMP32]]
//
poly8x8_t test_vld1_p8(poly8_t const *a) {
  return vld1_p8(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i16> @test_vld1_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i16>, ptr [[TMP12]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i16>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i16>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i16> [[TMP32]]
//
poly16x4_t test_vld1_p16(poly16_t const *a) {
  return vld1_p16(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i8> @test_vld1_u8_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i8>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i8> [[TMP32]]
//
uint8x8_t test_vld1_u8_void(void *a) {
  return vld1_u8(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i16> @test_vld1_u16_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i16>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i16>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i16>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i16> [[TMP32]]
//
uint16x4_t test_vld1_u16_void(void *a) {
  return vld1_u16(a);
}

// CHECK-LABEL: define dso_local noundef <2 x i32> @test_vld1_u32_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x i32>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i32>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x i32>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x i32>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i32> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x i32> [[TMP32]]
//
uint32x2_t test_vld1_u32_void(void *a) {
  return vld1_u32(a);
}

// CHECK-LABEL: define dso_local noundef <1 x i64> @test_vld1_u64_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <1 x i64>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <1 x i64>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <1 x i64>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <1 x i64>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <1 x i64> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <1 x i64> [[TMP32]]
//
uint64x1_t test_vld1_u64_void(void *a) {
  return vld1_u64(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i8> @test_vld1_s8_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i8>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i8> [[TMP32]]
//
int8x8_t test_vld1_s8_void(void *a) {
  return vld1_s8(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i16> @test_vld1_s16_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i16>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i16>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i16>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i16> [[TMP32]]
//
int16x4_t test_vld1_s16_void(void *a) {
  return vld1_s16(a);
}

// CHECK-LABEL: define dso_local noundef <2 x i32> @test_vld1_s32_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x i32>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i32>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x i32>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x i32>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i32> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x i32> [[TMP32]]
//
int32x2_t test_vld1_s32_void(void *a) {
  return vld1_s32(a);
}

// CHECK-LABEL: define dso_local noundef <1 x i64> @test_vld1_s64_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <1 x i64>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <1 x i64>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <1 x i64>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <1 x i64>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <1 x i64> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <1 x i64> [[TMP32]]
//
int64x1_t test_vld1_s64_void(void *a) {
  return vld1_s64(a);
}

// CHECK-LABEL: define dso_local noundef <4 x half> @test_vld1_f16_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x half>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x half>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x half>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x half> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x half>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x half> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x half>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x half> [[TMP32]]
//
float16x4_t test_vld1_f16_void(void *a) {
  return vld1_f16(a);
}

// CHECK-LABEL: define dso_local noundef <2 x float> @test_vld1_f32_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <2 x float>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <2 x float>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <2 x float>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <2 x i32>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <2 x float> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <2 x float>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <2 x float> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <2 x float>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <2 x i32> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <2 x float> [[TMP32]]
//
float32x2_t test_vld1_f32_void(void *a) {
  return vld1_f32(a);
}

// CHECK-LABEL: define dso_local noundef <1 x double> @test_vld1_f64_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <1 x double>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <1 x double>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <1 x double>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <1 x i64>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <1 x double> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <1 x double>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <1 x double> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <1 x double>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <1 x i64> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <1 x double> [[TMP32]]
//
float64x1_t test_vld1_f64_void(void *a) {
  return vld1_f64(a);
}

// CHECK-LABEL: define dso_local noundef <8 x i8> @test_vld1_p8_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <8 x i8>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <8 x i8>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <8 x i8>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <8 x i8> [[TMP32]]
//
poly8x8_t test_vld1_p8_void(void *a) {
  return vld1_p8(a);
}

// CHECK-LABEL: define dso_local noundef <4 x i16> @test_vld1_p16_void(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[TMP18:%.*]] = load <4 x i16>, ptr [[TMP12]], align 1
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[TMP12]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load <4 x i16>, ptr [[TMP21]], align 1
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD1]], ptr [[TMP24]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP18]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = load <4 x i16>, ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP30:%.*]] = xor i64 [[TMP29]], 193514046488576
// CHECK-NEXT:    [[TMP31:%.*]] = inttoptr i64 [[TMP30]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD2]], ptr [[TMP31]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP25]], ptr [[TMP]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP32:%.*]] = load <4 x i16>, ptr [[TMP]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = ptrtoint ptr [[TMP]] to i64
// CHECK-NEXT:    [[TMP34:%.*]] = xor i64 [[TMP33]], 193514046488576
// CHECK-NEXT:    [[TMP35:%.*]] = inttoptr i64 [[TMP34]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP35]], align 8
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <4 x i16> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP4:%.*]] = icmp ne i64 [[TMP36]], 0
// CHECK-NEXT:    br i1 [[_MSCMP4]], label [[TMP37:%.*]], label [[TMP38:%.*]], !prof [[PROF2]]
// CHECK:       37:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       38:
// CHECK-NEXT:    ret <4 x i16> [[TMP32]]
//
poly16x4_t test_vld1_p16_void(void *a) {
  return vld1_p16(a);
}

// CHECK-LABEL: define dso_local %struct.uint8x16x2_t @test_vld2q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X16X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld2.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT8X16X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT8X16X2_T]] [[TMP22]]
//
uint8x16x2_t test_vld2q_u8(uint8_t const *a) {
  return vld2q_u8(a);
}

// CHECK-LABEL: define dso_local %struct.uint16x8x2_t @test_vld2q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X8X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld2.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT16X8X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT16X8X2_T]] [[TMP22]]
//
uint16x8x2_t test_vld2q_u16(uint16_t const *a) {
  return vld2q_u16(a);
}

// CHECK-LABEL: define dso_local %struct.uint32x4x2_t @test_vld2q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X4X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld2.v4i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x i32>, <4 x i32> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT32X4X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT32X4X2_T]] [[TMP22]]
//
uint32x4x2_t test_vld2q_u32(uint32_t const *a) {
  return vld2q_u32(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x2_t @test_vld2q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT64X2X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X2_T]] [[TMP22]]
//
uint64x2x2_t test_vld2q_u64(uint64_t const *a) {
  return vld2q_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int8x16x2_t @test_vld2q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X16X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld2.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT8X16X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT8X16X2_T]] [[TMP22]]
//
int8x16x2_t test_vld2q_s8(int8_t const *a) {
  return vld2q_s8(a);
}

// CHECK-LABEL: define dso_local %struct.int16x8x2_t @test_vld2q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X8X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld2.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT16X8X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT16X8X2_T]] [[TMP22]]
//
int16x8x2_t test_vld2q_s16(int16_t const *a) {
  return vld2q_s16(a);
}

// CHECK-LABEL: define dso_local %struct.int32x4x2_t @test_vld2q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X4X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld2.v4i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x i32>, <4 x i32> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT32X4X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT32X4X2_T]] [[TMP22]]
//
int32x4x2_t test_vld2q_s32(int32_t const *a) {
  return vld2q_s32(a);
}

// CHECK-LABEL: define dso_local %struct.int64x2x2_t @test_vld2q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld2.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT64X2X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT64X2X2_T]] [[TMP22]]
//
int64x2x2_t test_vld2q_s64(int64_t const *a) {
  return vld2q_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float16x8x2_t @test_vld2q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <8 x half>, <8 x half> } @llvm.aarch64.neon.ld2.v8f16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x half>, <8 x half> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT16X8X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X8X2_T]] [[TMP22]]
//
float16x8x2_t test_vld2q_f16(float16_t const *a) {
  return vld2q_f16(a);
}

// CHECK-LABEL: define dso_local %struct.float32x4x2_t @test_vld2q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <4 x float>, <4 x float> } @llvm.aarch64.neon.ld2.v4f32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x float>, <4 x float> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT32X4X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X4X2_T]] [[TMP22]]
//
float32x4x2_t test_vld2q_f32(float32_t const *a) {
  return vld2q_f32(a);
}

// CHECK-LABEL: define dso_local %struct.float64x2x2_t @test_vld2q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x double>, <2 x double> } @llvm.aarch64.neon.ld2.v2f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x double>, <2 x double> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X2X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X2_T]] [[TMP22]]
//
float64x2x2_t test_vld2q_f64(float64_t const *a) {
  return vld2q_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly8x16x2_t @test_vld2q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X16X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld2.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY8X16X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY8X16X2_T]] [[TMP22]]
//
poly8x16x2_t test_vld2q_p8(poly8_t const *a) {
  return vld2q_p8(a);
}

// CHECK-LABEL: define dso_local %struct.poly16x8x2_t @test_vld2q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X8X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld2.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16> } [[VLD2]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY16X8X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY16X8X2_T]] [[TMP22]]
//
poly16x8x2_t test_vld2q_p16(poly16_t const *a) {
  return vld2q_p16(a);
}

// CHECK-LABEL: define dso_local %struct.uint8x8x2_t @test_vld2_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X8X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld2.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT8X8X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT8X8X2_T]] [[TMP22]]
//
uint8x8x2_t test_vld2_u8(uint8_t const *a) {
  return vld2_u8(a);
}

// CHECK-LABEL: define dso_local %struct.uint16x4x2_t @test_vld2_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X4X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld2.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT16X4X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT16X4X2_T]] [[TMP22]]
//
uint16x4x2_t test_vld2_u16(uint16_t const *a) {
  return vld2_u16(a);
}

// CHECK-LABEL: define dso_local %struct.uint32x2x2_t @test_vld2_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X2X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld2.v2i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x i32>, <2 x i32> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT32X2X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT32X2X2_T]] [[TMP22]]
//
uint32x2x2_t test_vld2_u32(uint32_t const *a) {
  return vld2_u32(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x1x2_t @test_vld2_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X1X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld2.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT64X1X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT64X1X2_T]] [[TMP22]]
//
uint64x1x2_t test_vld2_u64(uint64_t const *a) {
  return vld2_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int8x8x2_t @test_vld2_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X8X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld2.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT8X8X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT8X8X2_T]] [[TMP22]]
//
int8x8x2_t test_vld2_s8(int8_t const *a) {
  return vld2_s8(a);
}

// CHECK-LABEL: define dso_local %struct.int16x4x2_t @test_vld2_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X4X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld2.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT16X4X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT16X4X2_T]] [[TMP22]]
//
int16x4x2_t test_vld2_s16(int16_t const *a) {
  return vld2_s16(a);
}

// CHECK-LABEL: define dso_local %struct.int32x2x2_t @test_vld2_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X2X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld2.v2i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x i32>, <2 x i32> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT32X2X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT32X2X2_T]] [[TMP22]]
//
int32x2x2_t test_vld2_s32(int32_t const *a) {
  return vld2_s32(a);
}

// CHECK-LABEL: define dso_local %struct.int64x1x2_t @test_vld2_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X1X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld2.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT64X1X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT64X1X2_T]] [[TMP22]]
//
int64x1x2_t test_vld2_s64(int64_t const *a) {
  return vld2_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float16x4x2_t @test_vld2_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <4 x half>, <4 x half> } @llvm.aarch64.neon.ld2.v4f16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x half>, <4 x half> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT16X4X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X4X2_T]] [[TMP22]]
//
float16x4x2_t test_vld2_f16(float16_t const *a) {
  return vld2_f16(a);
}

// CHECK-LABEL: define dso_local %struct.float32x2x2_t @test_vld2_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <2 x float>, <2 x float> } @llvm.aarch64.neon.ld2.v2f32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x float>, <2 x float> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT32X2X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X2X2_T]] [[TMP22]]
//
float32x2x2_t test_vld2_f32(float32_t const *a) {
  return vld2_f32(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x2_t @test_vld2_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <1 x double>, <1 x double> } @llvm.aarch64.neon.ld2.v1f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x double>, <1 x double> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X1X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X2_T]] [[TMP22]]
//
float64x1x2_t test_vld2_f64(float64_t const *a) {
  return vld2_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly8x8x2_t @test_vld2_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X8X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld2.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY8X8X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY8X8X2_T]] [[TMP22]]
//
poly8x8x2_t test_vld2_p8(poly8_t const *a) {
  return vld2_p8(a);
}

// CHECK-LABEL: define dso_local %struct.poly16x4x2_t @test_vld2_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X4X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD2:%.*]] = call { <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld2.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16> } [[VLD2]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY16X4X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY16X4X2_T]] [[TMP22]]
//
poly16x4x2_t test_vld2_p16(poly16_t const *a) {
  return vld2_p16(a);
}

// CHECK-LABEL: define dso_local %struct.uint8x16x3_t @test_vld3q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X16X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld3.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT8X16X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT8X16X3_T]] [[TMP22]]
//
uint8x16x3_t test_vld3q_u8(uint8_t const *a) {
  return vld3q_u8(a);
}

// CHECK-LABEL: define dso_local %struct.uint16x8x3_t @test_vld3q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X8X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld3.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT16X8X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT16X8X3_T]] [[TMP22]]
//
uint16x8x3_t test_vld3q_u16(uint16_t const *a) {
  return vld3q_u16(a);
}

// CHECK-LABEL: define dso_local %struct.uint32x4x3_t @test_vld3q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X4X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld3.v4i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT32X4X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT32X4X3_T]] [[TMP22]]
//
uint32x4x3_t test_vld3q_u32(uint32_t const *a) {
  return vld3q_u32(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x3_t @test_vld3q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT64X2X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X3_T]] [[TMP22]]
//
uint64x2x3_t test_vld3q_u64(uint64_t const *a) {
  return vld3q_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int8x16x3_t @test_vld3q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X16X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld3.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT8X16X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT8X16X3_T]] [[TMP22]]
//
int8x16x3_t test_vld3q_s8(int8_t const *a) {
  return vld3q_s8(a);
}

// CHECK-LABEL: define dso_local %struct.int16x8x3_t @test_vld3q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X8X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld3.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT16X8X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT16X8X3_T]] [[TMP22]]
//
int16x8x3_t test_vld3q_s16(int16_t const *a) {
  return vld3q_s16(a);
}

// CHECK-LABEL: define dso_local %struct.int32x4x3_t @test_vld3q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X4X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld3.v4i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT32X4X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT32X4X3_T]] [[TMP22]]
//
int32x4x3_t test_vld3q_s32(int32_t const *a) {
  return vld3q_s32(a);
}

// CHECK-LABEL: define dso_local %struct.int64x2x3_t @test_vld3q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld3.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT64X2X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT64X2X3_T]] [[TMP22]]
//
int64x2x3_t test_vld3q_s64(int64_t const *a) {
  return vld3q_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float16x8x3_t @test_vld3q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <8 x half>, <8 x half>, <8 x half> } @llvm.aarch64.neon.ld3.v8f16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x half>, <8 x half>, <8 x half> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT16X8X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X8X3_T]] [[TMP22]]
//
float16x8x3_t test_vld3q_f16(float16_t const *a) {
  return vld3q_f16(a);
}

// CHECK-LABEL: define dso_local %struct.float32x4x3_t @test_vld3q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <4 x float>, <4 x float>, <4 x float> } @llvm.aarch64.neon.ld3.v4f32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x float>, <4 x float>, <4 x float> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT32X4X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X4X3_T]] [[TMP22]]
//
float32x4x3_t test_vld3q_f32(float32_t const *a) {
  return vld3q_f32(a);
}

// CHECK-LABEL: define dso_local %struct.float64x2x3_t @test_vld3q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld3.v2f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x double>, <2 x double>, <2 x double> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X2X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X3_T]] [[TMP22]]
//
float64x2x3_t test_vld3q_f64(float64_t const *a) {
  return vld3q_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly8x16x3_t @test_vld3q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X16X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld3.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY8X16X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY8X16X3_T]] [[TMP22]]
//
poly8x16x3_t test_vld3q_p8(poly8_t const *a) {
  return vld3q_p8(a);
}

// CHECK-LABEL: define dso_local %struct.poly16x8x3_t @test_vld3q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X8X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld3.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16> } [[VLD3]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY16X8X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY16X8X3_T]] [[TMP22]]
//
poly16x8x3_t test_vld3q_p16(poly16_t const *a) {
  return vld3q_p16(a);
}

// CHECK-LABEL: define dso_local %struct.uint8x8x3_t @test_vld3_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X8X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld3.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT8X8X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT8X8X3_T]] [[TMP22]]
//
uint8x8x3_t test_vld3_u8(uint8_t const *a) {
  return vld3_u8(a);
}

// CHECK-LABEL: define dso_local %struct.uint16x4x3_t @test_vld3_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X4X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld3.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT16X4X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT16X4X3_T]] [[TMP22]]
//
uint16x4x3_t test_vld3_u16(uint16_t const *a) {
  return vld3_u16(a);
}

// CHECK-LABEL: define dso_local %struct.uint32x2x3_t @test_vld3_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X2X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld3.v2i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT32X2X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT32X2X3_T]] [[TMP22]]
//
uint32x2x3_t test_vld3_u32(uint32_t const *a) {
  return vld3_u32(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x1x3_t @test_vld3_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X1X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld3.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT64X1X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT64X1X3_T]] [[TMP22]]
//
uint64x1x3_t test_vld3_u64(uint64_t const *a) {
  return vld3_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int8x8x3_t @test_vld3_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X8X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld3.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT8X8X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT8X8X3_T]] [[TMP22]]
//
int8x8x3_t test_vld3_s8(int8_t const *a) {
  return vld3_s8(a);
}

// CHECK-LABEL: define dso_local %struct.int16x4x3_t @test_vld3_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X4X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld3.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT16X4X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT16X4X3_T]] [[TMP22]]
//
int16x4x3_t test_vld3_s16(int16_t const *a) {
  return vld3_s16(a);
}

// CHECK-LABEL: define dso_local %struct.int32x2x3_t @test_vld3_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X2X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld3.v2i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT32X2X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT32X2X3_T]] [[TMP22]]
//
int32x2x3_t test_vld3_s32(int32_t const *a) {
  return vld3_s32(a);
}

// CHECK-LABEL: define dso_local %struct.int64x1x3_t @test_vld3_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X1X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld3.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT64X1X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT64X1X3_T]] [[TMP22]]
//
int64x1x3_t test_vld3_s64(int64_t const *a) {
  return vld3_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float16x4x3_t @test_vld3_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <4 x half>, <4 x half>, <4 x half> } @llvm.aarch64.neon.ld3.v4f16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x half>, <4 x half>, <4 x half> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT16X4X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X4X3_T]] [[TMP22]]
//
float16x4x3_t test_vld3_f16(float16_t const *a) {
  return vld3_f16(a);
}

// CHECK-LABEL: define dso_local %struct.float32x2x3_t @test_vld3_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <2 x float>, <2 x float>, <2 x float> } @llvm.aarch64.neon.ld3.v2f32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x float>, <2 x float>, <2 x float> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT32X2X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X2X3_T]] [[TMP22]]
//
float32x2x3_t test_vld3_f32(float32_t const *a) {
  return vld3_f32(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x3_t @test_vld3_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld3.v1f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x double>, <1 x double>, <1 x double> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X1X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X3_T]] [[TMP22]]
//
float64x1x3_t test_vld3_f64(float64_t const *a) {
  return vld3_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly8x8x3_t @test_vld3_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X8X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld3.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY8X8X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY8X8X3_T]] [[TMP22]]
//
poly8x8x3_t test_vld3_p8(poly8_t const *a) {
  return vld3_p8(a);
}

// CHECK-LABEL: define dso_local %struct.poly16x4x3_t @test_vld3_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X4X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD3:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld3.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16> } [[VLD3]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY16X4X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY16X4X3_T]] [[TMP22]]
//
poly16x4x3_t test_vld3_p16(poly16_t const *a) {
  return vld3_p16(a);
}

// CHECK-LABEL: define dso_local %struct.uint8x16x4_t @test_vld4q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X16X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld4.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT8X16X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT8X16X4_T]] [[TMP22]]
//
uint8x16x4_t test_vld4q_u8(uint8_t const *a) {
  return vld4q_u8(a);
}

// CHECK-LABEL: define dso_local %struct.uint16x8x4_t @test_vld4q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X8X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld4.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT16X8X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT16X8X4_T]] [[TMP22]]
//
uint16x8x4_t test_vld4q_u16(uint16_t const *a) {
  return vld4q_u16(a);
}

// CHECK-LABEL: define dso_local %struct.uint32x4x4_t @test_vld4q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X4X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld4.v4i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT32X4X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT32X4X4_T]] [[TMP22]]
//
uint32x4x4_t test_vld4q_u32(uint32_t const *a) {
  return vld4q_u32(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x2x4_t @test_vld4q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X2X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT64X2X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT64X2X4_T]] [[TMP22]]
//
uint64x2x4_t test_vld4q_u64(uint64_t const *a) {
  return vld4q_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int8x16x4_t @test_vld4q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X16X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld4.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT8X16X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT8X16X4_T]] [[TMP22]]
//
int8x16x4_t test_vld4q_s8(int8_t const *a) {
  return vld4q_s8(a);
}

// CHECK-LABEL: define dso_local %struct.int16x8x4_t @test_vld4q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X8X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld4.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT16X8X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT16X8X4_T]] [[TMP22]]
//
int16x8x4_t test_vld4q_s16(int16_t const *a) {
  return vld4q_s16(a);
}

// CHECK-LABEL: define dso_local %struct.int32x4x4_t @test_vld4q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X4X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } @llvm.aarch64.neon.ld4.v4i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT32X4X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT32X4X4_T]] [[TMP22]]
//
int32x4x4_t test_vld4q_s32(int32_t const *a) {
  return vld4q_s32(a);
}

// CHECK-LABEL: define dso_local %struct.int64x2x4_t @test_vld4q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X2X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld4.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT64X2X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT64X2X4_T]] [[TMP22]]
//
int64x2x4_t test_vld4q_s64(int64_t const *a) {
  return vld4q_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float16x8x4_t @test_vld4q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <8 x half>, <8 x half>, <8 x half>, <8 x half> } @llvm.aarch64.neon.ld4.v8f16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x half>, <8 x half>, <8 x half>, <8 x half> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT16X8X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X8X4_T]] [[TMP22]]
//
float16x8x4_t test_vld4q_f16(float16_t const *a) {
  return vld4q_f16(a);
}

// CHECK-LABEL: define dso_local %struct.float32x4x4_t @test_vld4q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <4 x float>, <4 x float>, <4 x float>, <4 x float> } @llvm.aarch64.neon.ld4.v4f32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i32>, <4 x i32>, <4 x i32>, <4 x i32> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <4 x float>, <4 x float>, <4 x float>, <4 x float> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT32X4X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <4 x i32>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <4 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X4X4_T]] [[TMP22]]
//
float32x4x4_t test_vld4q_f32(float32_t const *a) {
  return vld4q_f32(a);
}

// CHECK-LABEL: define dso_local %struct.float64x2x4_t @test_vld4q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x double>, <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld4.v2f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X2X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X4_T]] [[TMP22]]
//
float64x2x4_t test_vld4q_f64(float64_t const *a) {
  return vld4q_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly8x16x4_t @test_vld4q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X16X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } @llvm.aarch64.neon.ld4.v16i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <16 x i8>, <16 x i8>, <16 x i8>, <16 x i8> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY8X16X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <16 x i8>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <16 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY8X16X4_T]] [[TMP22]]
//
poly8x16x4_t test_vld4q_p8(poly8_t const *a) {
  return vld4q_p8(a);
}

// CHECK-LABEL: define dso_local %struct.poly16x8x4_t @test_vld4q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X8X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } @llvm.aarch64.neon.ld4.v8i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <8 x i16>, <8 x i16>, <8 x i16>, <8 x i16> } [[VLD4]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY16X8X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <8 x i16>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <8 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY16X8X4_T]] [[TMP22]]
//
poly16x8x4_t test_vld4q_p16(poly16_t const *a) {
  return vld4q_p16(a);
}

// CHECK-LABEL: define dso_local %struct.uint8x8x4_t @test_vld4_u8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT8X8X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld4.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT8X8X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT8X8X4_T]] [[TMP22]]
//
uint8x8x4_t test_vld4_u8(uint8_t const *a) {
  return vld4_u8(a);
}

// CHECK-LABEL: define dso_local %struct.uint16x4x4_t @test_vld4_u16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT16X4X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld4.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT16X4X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT16X4X4_T]] [[TMP22]]
//
uint16x4x4_t test_vld4_u16(uint16_t const *a) {
  return vld4_u16(a);
}

// CHECK-LABEL: define dso_local %struct.uint32x2x4_t @test_vld4_u32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT32X2X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld4.v2i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT32X2X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT32X2X4_T]] [[TMP22]]
//
uint32x2x4_t test_vld4_u32(uint32_t const *a) {
  return vld4_u32(a);
}

// CHECK-LABEL: define dso_local %struct.uint64x1x4_t @test_vld4_u64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_UINT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_UINT64X1X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld4.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_UINT64X1X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_UINT64X1X4_T]] [[TMP22]]
//
uint64x1x4_t test_vld4_u64(uint64_t const *a) {
  return vld4_u64(a);
}

// CHECK-LABEL: define dso_local %struct.int8x8x4_t @test_vld4_s8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT8X8X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld4.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT8X8X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT8X8X4_T]] [[TMP22]]
//
int8x8x4_t test_vld4_s8(int8_t const *a) {
  return vld4_s8(a);
}

// CHECK-LABEL: define dso_local %struct.int16x4x4_t @test_vld4_s16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT16X4X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld4.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT16X4X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT16X4X4_T]] [[TMP22]]
//
int16x4x4_t test_vld4_s16(int16_t const *a) {
  return vld4_s16(a);
}

// CHECK-LABEL: define dso_local %struct.int32x2x4_t @test_vld4_s32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT32X2X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } @llvm.aarch64.neon.ld4.v2i32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT32X2X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT32X2X4_T]] [[TMP22]]
//
int32x2x4_t test_vld4_s32(int32_t const *a) {
  return vld4_s32(a);
}

// CHECK-LABEL: define dso_local %struct.int64x1x4_t @test_vld4_s64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_INT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_INT64X1X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld4.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_INT64X1X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_INT64X1X4_T]] [[TMP22]]
//
int64x1x4_t test_vld4_s64(int64_t const *a) {
  return vld4_s64(a);
}

// CHECK-LABEL: define dso_local %struct.float16x4x4_t @test_vld4_f16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <4 x half>, <4 x half>, <4 x half>, <4 x half> } @llvm.aarch64.neon.ld4.v4f16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x half>, <4 x half>, <4 x half>, <4 x half> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT16X4X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT16X4X4_T]] [[TMP22]]
//
float16x4x4_t test_vld4_f16(float16_t const *a) {
  return vld4_f16(a);
}

// CHECK-LABEL: define dso_local %struct.float32x2x4_t @test_vld4_f32(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <2 x float>, <2 x float>, <2 x float>, <2 x float> } @llvm.aarch64.neon.ld4.v2f32.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i32>, <2 x i32>, <2 x i32>, <2 x i32> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <2 x float>, <2 x float>, <2 x float>, <2 x float> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT32X2X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i32>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <2 x i32>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT32X2X4_T]] [[TMP22]]
//
float32x2x4_t test_vld4_f32(float32_t const *a) {
  return vld4_f32(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x4_t @test_vld4_f64(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <1 x double>, <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld4.v1f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X1X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X4_T]] [[TMP22]]
//
float64x1x4_t test_vld4_f64(float64_t const *a) {
  return vld4_f64(a);
}

// CHECK-LABEL: define dso_local %struct.poly8x8x4_t @test_vld4_p8(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY8X8X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } @llvm.aarch64.neon.ld4.v8i8.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <8 x i8>, <8 x i8>, <8 x i8>, <8 x i8> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY8X8X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <8 x i8>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <8 x i8>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY8X8X4_T]] [[TMP22]]
//
poly8x8x4_t test_vld4_p8(poly8_t const *a) {
  return vld4_p8(a);
}

// CHECK-LABEL: define dso_local %struct.poly16x4x4_t @test_vld4_p16(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY16X4X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD4:%.*]] = call { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } @llvm.aarch64.neon.ld4.v4i16.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <4 x i16>, <4 x i16>, <4 x i16>, <4 x i16> } [[VLD4]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY16X4X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <4 x i16>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <4 x i16>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY16X4X4_T]] [[TMP22]]
//
poly16x4x4_t test_vld4_p16(poly16_t const *a) {
  return vld4_p16(a);
}

// CHECK-LABEL: define dso_local void @test_vst1q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <16 x i8> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <16 x i8> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <16 x i8>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <16 x i8>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <16 x i8>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP30:%.*]], label [[TMP31:%.*]], !prof [[PROF2]]
// CHECK:       30:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       31:
// CHECK-NEXT:    [[TMP32:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP33:%.*]] = xor i64 [[TMP32]], 193514046488576
// CHECK-NEXT:    [[TMP34:%.*]] = inttoptr i64 [[TMP33]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD2]], ptr [[TMP34]], align 1
// CHECK-NEXT:    store <16 x i8> [[TMP26]], ptr [[TMP22]], align 1
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_u8(uint8_t *a, uint8x16_t b) {
  vst1q_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <8 x i16> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <8 x i16> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <8 x i16>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <8 x i16>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <8 x i16> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <8 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <8 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <8 x i16> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_u16(uint16_t *a, uint16x8_t b) {
  vst1q_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <4 x i32> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <4 x i32> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <4 x i32>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <4 x i32>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <4 x i32> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i32>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <4 x i32> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <4 x i32>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <4 x i32> [[TMP32]], ptr [[TMP38]], align 4
// CHECK-NEXT:    store <4 x i32> [[TMP33]], ptr [[TMP22]], align 4
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_u32(uint32_t *a, uint32x4_t b) {
  vst1q_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <2 x i64> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <2 x i64>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i64>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <2 x i64> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <2 x i64>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <2 x i64> [[TMP32]], ptr [[TMP38]], align 8
// CHECK-NEXT:    store <2 x i64> [[TMP33]], ptr [[TMP22]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_u64(uint64_t *a, uint64x2_t b) {
  vst1q_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <16 x i8> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <16 x i8> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <16 x i8>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <16 x i8>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <16 x i8>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP30:%.*]], label [[TMP31:%.*]], !prof [[PROF2]]
// CHECK:       30:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       31:
// CHECK-NEXT:    [[TMP32:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP33:%.*]] = xor i64 [[TMP32]], 193514046488576
// CHECK-NEXT:    [[TMP34:%.*]] = inttoptr i64 [[TMP33]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD2]], ptr [[TMP34]], align 1
// CHECK-NEXT:    store <16 x i8> [[TMP26]], ptr [[TMP22]], align 1
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_s8(int8_t *a, int8x16_t b) {
  vst1q_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <8 x i16> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <8 x i16> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <8 x i16>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <8 x i16>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <8 x i16> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <8 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <8 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <8 x i16> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_s16(int16_t *a, int16x8_t b) {
  vst1q_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <4 x i32>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <4 x i32> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <4 x i32> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <4 x i32>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <4 x i32>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <4 x i32> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <4 x i32>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i32>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <4 x i32> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <4 x i32>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <4 x i32> [[TMP32]], ptr [[TMP38]], align 4
// CHECK-NEXT:    store <4 x i32> [[TMP33]], ptr [[TMP22]], align 4
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_s32(int32_t *a, int32x4_t b) {
  vst1q_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <2 x i64> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <2 x i64>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <2 x i64>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i64>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <2 x i64> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <2 x i64>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <2 x i64> [[TMP32]], ptr [[TMP38]], align 8
// CHECK-NEXT:    store <2 x i64> [[TMP33]], ptr [[TMP22]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_s64(int64_t *a, int64x2_t b) {
  vst1q_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <8 x i16> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <8 x i16>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <8 x half>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <8 x i16> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x half> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <8 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x half>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <8 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <8 x half> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_f16(float16_t *a, float16x8_t b) {
  vst1q_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <4 x i32> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <4 x i32>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <4 x i32> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <4 x float>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i32>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <4 x i32> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x float> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <4 x i32>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x float>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <4 x i32> [[TMP32]], ptr [[TMP38]], align 4
// CHECK-NEXT:    store <4 x float> [[TMP33]], ptr [[TMP22]], align 4
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_f32(float32_t *a, float32x4_t b) {
  vst1q_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <2 x i64> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <2 x i64>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <2 x i64> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <2 x double>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i64>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <2 x i64> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x double> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <2 x i64>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x double>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <2 x i64> [[TMP32]], ptr [[TMP38]], align 8
// CHECK-NEXT:    store <2 x double> [[TMP33]], ptr [[TMP22]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_f64(float64_t *a, float64x2_t b) {
  vst1q_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], <16 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <16 x i8>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <16 x i8> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <16 x i8> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <16 x i8>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <16 x i8>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <16 x i8> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <16 x i8>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <16 x i8>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP30:%.*]], label [[TMP31:%.*]], !prof [[PROF2]]
// CHECK:       30:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       31:
// CHECK-NEXT:    [[TMP32:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP33:%.*]] = xor i64 [[TMP32]], 193514046488576
// CHECK-NEXT:    [[TMP34:%.*]] = inttoptr i64 [[TMP33]] to ptr
// CHECK-NEXT:    store <16 x i8> [[_MSLD2]], ptr [[TMP34]], align 1
// CHECK-NEXT:    store <16 x i8> [[TMP26]], ptr [[TMP22]], align 1
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_p8(poly8_t *a, poly8x16_t b) {
  vst1q_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP5]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <8 x i16> zeroinitializer, ptr [[TMP11]], align 16
// CHECK-NEXT:    store <8 x i16> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP14]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <8 x i16>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <8 x i16>, ptr [[TMP18]], align 16
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <8 x i16> [[_MSLD]], ptr [[TMP21]], align 16
// CHECK-NEXT:    store <8 x i16> [[TMP15]], ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <8 x i16>, ptr [[__S1]], align 16
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i16>, ptr [[TMP29]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <8 x i16> [[_MSLD2]] to <16 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[TMP26]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x i8> [[TMP30]] to <8 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <8 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <8 x i16> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_p16(poly16_t *a, poly16x8_t b) {
  vst1q_p16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <8 x i8> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <8 x i8> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <8 x i8>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <8 x i8>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP30:%.*]], label [[TMP31:%.*]], !prof [[PROF2]]
// CHECK:       30:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       31:
// CHECK-NEXT:    [[TMP32:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP33:%.*]] = xor i64 [[TMP32]], 193514046488576
// CHECK-NEXT:    [[TMP34:%.*]] = inttoptr i64 [[TMP33]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP34]], align 1
// CHECK-NEXT:    store <8 x i8> [[TMP26]], ptr [[TMP22]], align 1
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_u8(uint8_t *a, uint8x8_t b) {
  vst1_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <4 x i16> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <4 x i16> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <4 x i16>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <4 x i16>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <4 x i16> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <4 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <4 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <4 x i16> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_u16(uint16_t *a, uint16x4_t b) {
  vst1_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <2 x i32> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <2 x i32> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <2 x i32>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <2 x i32>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <2 x i32> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <2 x i32>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <2 x i32> [[TMP32]], ptr [[TMP38]], align 4
// CHECK-NEXT:    store <2 x i32> [[TMP33]], ptr [[TMP22]], align 4
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_u32(uint32_t *a, uint32x2_t b) {
  vst1_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <1 x i64> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <1 x i64> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <1 x i64>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <1 x i64>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <1 x i64> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <1 x i64>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <1 x i64> [[TMP32]], ptr [[TMP38]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP33]], ptr [[TMP22]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_u64(uint64_t *a, uint64x1_t b) {
  vst1_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <8 x i8> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <8 x i8> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <8 x i8>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <8 x i8>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP30:%.*]], label [[TMP31:%.*]], !prof [[PROF2]]
// CHECK:       30:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       31:
// CHECK-NEXT:    [[TMP32:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP33:%.*]] = xor i64 [[TMP32]], 193514046488576
// CHECK-NEXT:    [[TMP34:%.*]] = inttoptr i64 [[TMP33]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP34]], align 1
// CHECK-NEXT:    store <8 x i8> [[TMP26]], ptr [[TMP22]], align 1
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_s8(int8_t *a, int8x8_t b) {
  vst1_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <4 x i16> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <4 x i16> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <4 x i16>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <4 x i16>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <4 x i16> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <4 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <4 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <4 x i16> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_s16(int16_t *a, int16x4_t b) {
  vst1_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x i32> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <2 x i32>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <2 x i32> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <2 x i32> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <2 x i32>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <2 x i32>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <2 x i32>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <2 x i32> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <2 x i32>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <2 x i32> [[TMP32]], ptr [[TMP38]], align 4
// CHECK-NEXT:    store <2 x i32> [[TMP33]], ptr [[TMP22]], align 4
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_s32(int32_t *a, int32x2_t b) {
  vst1_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <1 x i64>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <1 x i64> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <1 x i64> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <1 x i64>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <1 x i64>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <1 x i64>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <1 x i64> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <1 x i64>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <1 x i64> [[TMP32]], ptr [[TMP38]], align 8
// CHECK-NEXT:    store <1 x i64> [[TMP33]], ptr [[TMP22]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_s64(int64_t *a, int64x1_t b) {
  vst1_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x half>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <4 x half>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <4 x i16> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <4 x half> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <4 x half>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <4 x i16>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <4 x half> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <4 x half>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <4 x i16> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x half> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <4 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x half>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <4 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <4 x half> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_f16(float16_t *a, float16x4_t b) {
  vst1_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], <2 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x float>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <2 x float>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <2 x i32> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <2 x float> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <2 x float>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <2 x i32>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <2 x i32> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <2 x float> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <2 x float>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <2 x i32>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <2 x i32> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x float> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <2 x i32>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x float>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <2 x i32> [[TMP32]], ptr [[TMP38]], align 4
// CHECK-NEXT:    store <2 x float> [[TMP33]], ptr [[TMP22]], align 4
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_f32(float32_t *a, float32x2_t b) {
  vst1_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], <1 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <1 x double>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <1 x double>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <1 x i64> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <1 x double> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <1 x double>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <1 x i64>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <1 x i64> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <1 x double> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <1 x double>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <1 x i64>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <1 x i64> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x double> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <1 x i64>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x double>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <1 x i64> [[TMP32]], ptr [[TMP38]], align 8
// CHECK-NEXT:    store <1 x double> [[TMP33]], ptr [[TMP22]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_f64(float64_t *a, float64x1_t b) {
  vst1_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x i8> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <8 x i8>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <8 x i8> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <8 x i8> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <8 x i8>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <8 x i8>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <8 x i8> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <8 x i8>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <8 x i8>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP30:%.*]], label [[TMP31:%.*]], !prof [[PROF2]]
// CHECK:       30:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       31:
// CHECK-NEXT:    [[TMP32:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP33:%.*]] = xor i64 [[TMP32]], 193514046488576
// CHECK-NEXT:    [[TMP34:%.*]] = inttoptr i64 [[TMP33]] to ptr
// CHECK-NEXT:    store <8 x i8> [[_MSLD2]], ptr [[TMP34]], align 1
// CHECK-NEXT:    store <8 x i8> [[TMP26]], ptr [[TMP22]], align 1
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_p8(poly8_t *a, poly8x8_t b) {
  vst1_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], <4 x i16> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca <4 x i16>, align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    store <4 x i16> zeroinitializer, ptr [[TMP11]], align 8
// CHECK-NEXT:    store <4 x i16> [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP12:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP13:%.*]] = xor i64 [[TMP12]], 193514046488576
// CHECK-NEXT:    [[TMP14:%.*]] = inttoptr i64 [[TMP13]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP14]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[TMP15:%.*]] = load <4 x i16>, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = ptrtoint ptr [[B_ADDR]] to i64
// CHECK-NEXT:    [[TMP17:%.*]] = xor i64 [[TMP16]], 193514046488576
// CHECK-NEXT:    [[TMP18:%.*]] = inttoptr i64 [[TMP17]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load <4 x i16>, ptr [[TMP18]], align 8
// CHECK-NEXT:    [[TMP19:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP20:%.*]] = xor i64 [[TMP19]], 193514046488576
// CHECK-NEXT:    [[TMP21:%.*]] = inttoptr i64 [[TMP20]] to ptr
// CHECK-NEXT:    store <4 x i16> [[_MSLD]], ptr [[TMP21]], align 8
// CHECK-NEXT:    store <4 x i16> [[TMP15]], ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load i64, ptr [[TMP25]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = load <4 x i16>, ptr [[__S1]], align 8
// CHECK-NEXT:    [[TMP27:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP28:%.*]] = xor i64 [[TMP27]], 193514046488576
// CHECK-NEXT:    [[TMP29:%.*]] = inttoptr i64 [[TMP28]] to ptr
// CHECK-NEXT:    [[_MSLD2:%.*]] = load <4 x i16>, ptr [[TMP29]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <4 x i16> [[_MSLD2]] to <8 x i8>
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[TMP26]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i8> [[TMP30]] to <4 x i16>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD1]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP34:%.*]], label [[TMP35:%.*]], !prof [[PROF2]]
// CHECK:       34:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       35:
// CHECK-NEXT:    [[TMP36:%.*]] = ptrtoint ptr [[TMP22]] to i64
// CHECK-NEXT:    [[TMP37:%.*]] = xor i64 [[TMP36]], 193514046488576
// CHECK-NEXT:    [[TMP38:%.*]] = inttoptr i64 [[TMP37]] to ptr
// CHECK-NEXT:    store <4 x i16> [[TMP32]], ptr [[TMP38]], align 2
// CHECK-NEXT:    store <4 x i16> [[TMP33]], ptr [[TMP22]], align 2
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_p16(poly16_t *a, poly16x4_t b) {
  vst1_p16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = bitcast <16 x i8> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP29]], 0
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <16 x i8> [[_MSLD4]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP30]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP31:%.*]], label [[TMP32:%.*]], !prof [[PROF2]]
// CHECK:       31:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       32:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_u8(uint8_t *a, uint8x16x2_t b) {
  vst2q_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v8i16.p0(<8 x i16> [[TMP34]], <8 x i16> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_u16(uint16_t *a, uint16x8x2_t b) {
  vst2q_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <4 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <4 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i32>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i32> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <4 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i32> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x i32>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x i32>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i32> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v4i32.p0(<4 x i32> [[TMP34]], <4 x i32> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_u32(uint32_t *a, uint32x4x2_t b) {
  vst2q_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v2i64.p0(<2 x i64> [[TMP34]], <2 x i64> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_u64(uint64_t *a, uint64x2x2_t b) {
  vst2q_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = bitcast <16 x i8> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP29]], 0
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <16 x i8> [[_MSLD4]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP30]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP31:%.*]], label [[TMP32:%.*]], !prof [[PROF2]]
// CHECK:       31:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       32:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_s8(int8_t *a, int8x16x2_t b) {
  vst2q_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v8i16.p0(<8 x i16> [[TMP34]], <8 x i16> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_s16(int16_t *a, int16x8x2_t b) {
  vst2q_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <4 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <4 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i32>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i32> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <4 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i32> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x i32>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x i32>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i32> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v4i32.p0(<4 x i32> [[TMP34]], <4 x i32> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_s32(int32_t *a, int32x4x2_t b) {
  vst2q_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v2i64.p0(<2 x i64> [[TMP34]], <2 x i64> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_s64(int64_t *a, int64x2x2_t b) {
  vst2q_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <8 x half>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <8 x half>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x half>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x half> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <8 x half>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x half> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x half>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x half>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v8f16.p0(<8 x half> [[TMP34]], <8 x half> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_f16(float16_t *a, float16x8x2_t b) {
  vst2q_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <4 x float>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <4 x float>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x float>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x float> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <4 x float>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x float> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x float>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x float>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i32> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v4f32.p0(<4 x float> [[TMP34]], <4 x float> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_f32(float32_t *a, float32x4x2_t b) {
  vst2q_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <2 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x double>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x double> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x double> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x double>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x double>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v2f64.p0(<2 x double> [[TMP34]], <2 x double> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_f64(float64_t *a, float64x2x2_t b) {
  vst2q_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[TMP29:%.*]] = bitcast <16 x i8> [[_MSLD3]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP29]], 0
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <16 x i8> [[_MSLD4]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP30]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP31:%.*]], label [[TMP32:%.*]], !prof [[PROF2]]
// CHECK:       31:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       32:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_p8(poly8_t *a, poly8x16x2_t b) {
  vst2q_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v8i16.p0(<8 x i16> [[TMP34]], <8 x i16> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2q_p16(poly16_t *a, poly16x8x2_t b) {
  vst2q_p16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP29]], 0
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <8 x i8> [[_MSLD4]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP30]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP31:%.*]], label [[TMP32:%.*]], !prof [[PROF2]]
// CHECK:       31:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       32:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_u8(uint8_t *a, uint8x8x2_t b) {
  vst2_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v4i16.p0(<4 x i16> [[TMP34]], <4 x i16> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_u16(uint16_t *a, uint16x4x2_t b) {
  vst2_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <2 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i32>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i32> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i32> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x i32>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x i32>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i32> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v2i32.p0(<2 x i32> [[TMP34]], <2 x i32> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_u32(uint32_t *a, uint32x2x2_t b) {
  vst2_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v1i64.p0(<1 x i64> [[TMP34]], <1 x i64> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_u64(uint64_t *a, uint64x1x2_t b) {
  vst2_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP29]], 0
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <8 x i8> [[_MSLD4]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP30]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP31:%.*]], label [[TMP32:%.*]], !prof [[PROF2]]
// CHECK:       31:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       32:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_s8(int8_t *a, int8x8x2_t b) {
  vst2_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v4i16.p0(<4 x i16> [[TMP34]], <4 x i16> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_s16(int16_t *a, int16x4x2_t b) {
  vst2_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <2 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i32>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i32> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i32> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x i32>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x i32>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i32> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v2i32.p0(<2 x i32> [[TMP34]], <2 x i32> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_s32(int32_t *a, int32x2x2_t b) {
  vst2_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v1i64.p0(<1 x i64> [[TMP34]], <1 x i64> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_s64(int64_t *a, int64x1x2_t b) {
  vst2_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <4 x half>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <4 x half>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x half>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x half> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <4 x half>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x half> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x half>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x half>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v4f16.p0(<4 x half> [[TMP34]], <4 x half> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_f16(float16_t *a, float16x4x2_t b) {
  vst2_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <2 x float>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x float>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x float>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x float> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x float>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x float> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x float>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x float>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i32> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v2f32.p0(<2 x float> [[TMP34]], <2 x float> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_f32(float32_t *a, float32x2x2_t b) {
  vst2_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <1 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <1 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x double>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x double> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <1 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x double> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x double>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x double>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v1f64.p0(<1 x double> [[TMP34]], <1 x double> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_f64(float64_t *a, float64x1x2_t b) {
  vst2_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[TMP29:%.*]] = bitcast <8 x i8> [[_MSLD3]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP29]], 0
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast <8 x i8> [[_MSLD4]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP30]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP31:%.*]], label [[TMP32:%.*]], !prof [[PROF2]]
// CHECK:       31:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       32:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_p8(poly8_t *a, poly8x8x2_t b) {
  vst2_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst2_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st2.v4i16.p0(<4 x i16> [[TMP34]], <4 x i16> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst2_p16(poly16_t *a, poly16x4x2_t b) {
  vst2_p16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <16 x i8>, ptr [[TMP32]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[_MSLD5]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP33]], 0
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[_MSLD6]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP34]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[_MSLD7]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP35]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP36:%.*]], label [[TMP37:%.*]], !prof [[PROF2]]
// CHECK:       36:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       37:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], <16 x i8> [[TMP29]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_u8(uint8_t *a, uint8x16x3_t b) {
  vst3q_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x i16>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i16> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i16> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i16> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v8i16.p0(<8 x i16> [[TMP40]], <8 x i16> [[TMP42]], <8 x i16> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_u16(uint16_t *a, uint16x8x3_t b) {
  vst3q_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <4 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i32>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i32> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i32> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i32>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i32> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x i32>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x i32>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <4 x i32>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <4 x i32>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <4 x i32> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <4 x i32> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <4 x i32> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v4i32.p0(<4 x i32> [[TMP40]], <4 x i32> [[TMP42]], <4 x i32> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_u32(uint32_t *a, uint32x4x3_t b) {
  vst3q_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x i64>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i64> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i64> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i64> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v2i64.p0(<2 x i64> [[TMP40]], <2 x i64> [[TMP42]], <2 x i64> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_u64(uint64_t *a, uint64x2x3_t b) {
  vst3q_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <16 x i8>, ptr [[TMP32]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[_MSLD5]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP33]], 0
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[_MSLD6]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP34]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[_MSLD7]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP35]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP36:%.*]], label [[TMP37:%.*]], !prof [[PROF2]]
// CHECK:       36:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       37:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], <16 x i8> [[TMP29]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_s8(int8_t *a, int8x16x3_t b) {
  vst3q_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x i16>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i16> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i16> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i16> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v8i16.p0(<8 x i16> [[TMP40]], <8 x i16> [[TMP42]], <8 x i16> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_s16(int16_t *a, int16x8x3_t b) {
  vst3q_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <4 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i32>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i32> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i32> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <4 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i32>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i32> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x i32>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x i32>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <4 x i32>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <4 x i32>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <4 x i32> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <4 x i32> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <4 x i32> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v4i32.p0(<4 x i32> [[TMP40]], <4 x i32> [[TMP42]], <4 x i32> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_s32(int32_t *a, int32x4x3_t b) {
  vst3q_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x i64>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i64> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i64> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i64> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v2i64.p0(<2 x i64> [[TMP40]], <2 x i64> [[TMP42]], <2 x i64> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_s64(int64_t *a, int64x2x3_t b) {
  vst3q_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <8 x half>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <8 x half>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x half>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x half> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <8 x half>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x half> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <8 x half>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x half>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x half> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x half>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x half>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x half>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i16> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i16> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i16> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v8f16.p0(<8 x half> [[TMP40]], <8 x half> [[TMP42]], <8 x half> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_f16(float16_t *a, float16x8x3_t b) {
  vst3q_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <4 x float>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <4 x float>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x float>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x float> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <4 x float>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x float> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <4 x float>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x float>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i32>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x float> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x float>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x float>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <4 x i32>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <4 x float>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <4 x i32> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <4 x i32> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <4 x i32> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v4f32.p0(<4 x float> [[TMP40]], <4 x float> [[TMP42]], <4 x float> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_f32(float32_t *a, float32x4x3_t b) {
  vst3q_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <2 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x double>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x double> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x double> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x double> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x double>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x double>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x double>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i64> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i64> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i64> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v2f64.p0(<2 x double> [[TMP40]], <2 x double> [[TMP42]], <2 x double> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_f64(float64_t *a, float64x2x3_t b) {
  vst3q_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <16 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <16 x i8>, ptr [[TMP32]], align 16
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[_MSLD5]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP33]], 0
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[_MSLD6]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP34]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[_MSLD7]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP35]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP36:%.*]], label [[TMP37:%.*]], !prof [[PROF2]]
// CHECK:       36:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       37:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], <16 x i8> [[TMP29]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_p8(poly8_t *a, poly8x16x3_t b) {
  vst3q_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <8 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x i16>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i16> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i16> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i16> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v8i16.p0(<8 x i16> [[TMP40]], <8 x i16> [[TMP42]], <8 x i16> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3q_p16(poly16_t *a, poly16x8x3_t b) {
  vst3q_p16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i8>, ptr [[TMP32]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[_MSLD5]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP33]], 0
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[_MSLD6]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP34]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[_MSLD7]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP35]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP36:%.*]], label [[TMP37:%.*]], !prof [[PROF2]]
// CHECK:       36:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       37:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], <8 x i8> [[TMP29]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_u8(uint8_t *a, uint8x8x3_t b) {
  vst3_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x i16>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <4 x i16> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <4 x i16> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <4 x i16> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v4i16.p0(<4 x i16> [[TMP40]], <4 x i16> [[TMP42]], <4 x i16> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_u16(uint16_t *a, uint16x4x3_t b) {
  vst3_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <2 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i32>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i32> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i32> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i32>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i32> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x i32>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x i32>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <2 x i32>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <2 x i32>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i32> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i32> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i32> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v2i32.p0(<2 x i32> [[TMP40]], <2 x i32> [[TMP42]], <2 x i32> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_u32(uint32_t *a, uint32x2x3_t b) {
  vst3_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x i64>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <1 x i64> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <1 x i64> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <1 x i64> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v1i64.p0(<1 x i64> [[TMP40]], <1 x i64> [[TMP42]], <1 x i64> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_u64(uint64_t *a, uint64x1x3_t b) {
  vst3_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i8>, ptr [[TMP32]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[_MSLD5]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP33]], 0
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[_MSLD6]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP34]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[_MSLD7]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP35]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP36:%.*]], label [[TMP37:%.*]], !prof [[PROF2]]
// CHECK:       36:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       37:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], <8 x i8> [[TMP29]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_s8(int8_t *a, int8x8x3_t b) {
  vst3_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x i16>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <4 x i16> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <4 x i16> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <4 x i16> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v4i16.p0(<4 x i16> [[TMP40]], <4 x i16> [[TMP42]], <4 x i16> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_s16(int16_t *a, int16x4x3_t b) {
  vst3_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <2 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i32>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i32> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i32> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i32>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i32> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x i32>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x i32>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <2 x i32>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <2 x i32>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i32> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i32> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i32> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v2i32.p0(<2 x i32> [[TMP40]], <2 x i32> [[TMP42]], <2 x i32> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_s32(int32_t *a, int32x2x3_t b) {
  vst3_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x i64>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <1 x i64> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <1 x i64> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <1 x i64> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v1i64.p0(<1 x i64> [[TMP40]], <1 x i64> [[TMP42]], <1 x i64> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_s64(int64_t *a, int64x1x3_t b) {
  vst3_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <4 x half>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <4 x half>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x half>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x half> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <4 x half>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x half> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <4 x half>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x half>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x half> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x half>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x half>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x half>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <4 x i16> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <4 x i16> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <4 x i16> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v4f16.p0(<4 x half> [[TMP40]], <4 x half> [[TMP42]], <4 x half> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_f16(float16_t *a, float16x4x3_t b) {
  vst3_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <2 x float>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x float>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x float>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x float> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x float>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x float> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x float>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x float>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i32>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x float> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x float>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x float>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <2 x i32>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <2 x float>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i32> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i32> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i32> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v2f32.p0(<2 x float> [[TMP40]], <2 x float> [[TMP42]], <2 x float> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_f32(float32_t *a, float32x2x3_t b) {
  vst3_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <1 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <1 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x double>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x double> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <1 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x double> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <1 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x double> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x double>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x double>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x double>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <1 x i64> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <1 x i64> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <1 x i64> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v1f64.p0(<1 x double> [[TMP40]], <1 x double> [[TMP42]], <1 x double> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_f64(float64_t *a, float64x1x3_t b) {
  vst3_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <8 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i8>, ptr [[TMP32]], align 8
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[_MSLD5]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP33]], 0
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[_MSLD6]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP34]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[_MSLD7]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP35]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP36:%.*]], label [[TMP37:%.*]], !prof [[PROF2]]
// CHECK:       36:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       37:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], <8 x i8> [[TMP29]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_p8(poly8_t *a, poly8x8x3_t b) {
  vst3_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst3_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <4 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x i16>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <4 x i16> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <4 x i16> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <4 x i16> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st3.v4i16.p0(<4 x i16> [[TMP40]], <4 x i16> [[TMP42]], <4 x i16> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst3_p16(poly16_t *a, poly16x4x3_t b) {
  vst3_p16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X16X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <16 x i8>, ptr [[TMP32]], align 16
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP33:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <16 x i8>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <16 x i8> [[_MSLD7]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <16 x i8> [[_MSLD8]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[_MSLD9]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP39]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[_MSLD10]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP40]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP41:%.*]], label [[TMP42:%.*]], !prof [[PROF2]]
// CHECK:       41:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       42:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], <16 x i8> [[TMP29]], <16 x i8> [[TMP33]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_u8(uint8_t *a, uint8x16x4_t b) {
  vst4q_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X8X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <8 x i16>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i16> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i16> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x i16>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <8 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <8 x i16>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <8 x i16> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <8 x i16> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <8 x i16> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <8 x i16> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v8i16.p0(<8 x i16> [[TMP46]], <8 x i16> [[TMP48]], <8 x i16> [[TMP50]], <8 x i16> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_u16(uint16_t *a, uint16x8x4_t b) {
  vst4q_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X4X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <4 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i32>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i32> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i32> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <4 x i32>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i32> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <4 x i32>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <4 x i32>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <4 x i32> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <4 x i32> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x i32>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x i32>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <4 x i32>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <4 x i32>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <4 x i32>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <4 x i32>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <4 x i32> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <4 x i32> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <4 x i32> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <4 x i32> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v4i32.p0(<4 x i32> [[TMP46]], <4 x i32> [[TMP48]], <4 x i32> [[TMP50]], <4 x i32> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_u32(uint32_t *a, uint32x4x4_t b) {
  vst4q_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X2X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i64>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i64> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x i64> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x i64>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <2 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <2 x i64>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i64> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i64> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i64> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i64> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v2i64.p0(<2 x i64> [[TMP46]], <2 x i64> [[TMP48]], <2 x i64> [[TMP50]], <2 x i64> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_u64(uint64_t *a, uint64x2x4_t b) {
  vst4q_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X16X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <16 x i8>, ptr [[TMP32]], align 16
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP33:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <16 x i8>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <16 x i8> [[_MSLD7]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <16 x i8> [[_MSLD8]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[_MSLD9]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP39]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[_MSLD10]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP40]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP41:%.*]], label [[TMP42:%.*]], !prof [[PROF2]]
// CHECK:       41:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       42:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], <16 x i8> [[TMP29]], <16 x i8> [[TMP33]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_s8(int8_t *a, int8x16x4_t b) {
  vst4q_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X8X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <8 x i16>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i16> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i16> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x i16>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <8 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <8 x i16>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <8 x i16> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <8 x i16> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <8 x i16> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <8 x i16> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v8i16.p0(<8 x i16> [[TMP46]], <8 x i16> [[TMP48]], <8 x i16> [[TMP50]], <8 x i16> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_s16(int16_t *a, int16x8x4_t b) {
  vst4q_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i32>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X4X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <4 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i32>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i32> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i32>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i32> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i32>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <4 x i32>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i32> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <4 x i32>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <4 x i32>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <4 x i32>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <4 x i32> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <4 x i32> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x i32>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x i32>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <4 x i32>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <4 x i32>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <4 x i32>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <4 x i32>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <4 x i32> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <4 x i32> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <4 x i32> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <4 x i32> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v4i32.p0(<4 x i32> [[TMP46]], <4 x i32> [[TMP48]], <4 x i32> [[TMP50]], <4 x i32> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_s32(int32_t *a, int32x4x4_t b) {
  vst4q_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X2X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i64>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i64> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x i64> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x i64>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <2 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <2 x i64>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i64> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i64> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i64> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i64> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v2i64.p0(<2 x i64> [[TMP46]], <2 x i64> [[TMP48]], <2 x i64> [[TMP50]], <2 x i64> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_s64(int64_t *a, int64x2x4_t b) {
  vst4q_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x half>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X8X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <8 x half>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <8 x half>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x half>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x half> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <8 x half>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x half>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x half> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <8 x half>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x half>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x half> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <8 x half>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <8 x half>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <8 x i16>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i16> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x half> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x half>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x half>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x half>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <8 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <8 x half>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <8 x i16> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <8 x i16> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <8 x i16> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <8 x i16> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v8f16.p0(<8 x half> [[TMP46]], <8 x half> [[TMP48]], <8 x half> [[TMP50]], <8 x half> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_f16(float16_t *a, float16x8x4_t b) {
  vst4q_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x float>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <4 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X4X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <4 x i32>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <4 x float>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <4 x float>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x float>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i32>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i32> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x float> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <4 x float>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x float>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <4 x i32>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i32> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x float> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <4 x float>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x float>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <4 x i32>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i32> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x float> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <4 x float>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <4 x float>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <4 x i32>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <4 x i32> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <4 x float> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <4 x i32>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <4 x float>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <4 x i32>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <4 x float>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <4 x i32>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <4 x float>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <4 x i32>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <4 x float>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <4 x i32> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <4 x i32> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <4 x i32> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <4 x i32> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v4f32.p0(<4 x float> [[TMP46]], <4 x float> [[TMP48]], <4 x float> [[TMP50]], <4 x float> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_f32(float32_t *a, float32x4x4_t b) {
  vst4q_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <2 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x double>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x double> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x double> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x double> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x double>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i64>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i64> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x double> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x double>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x double>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x double>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <2 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <2 x double>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i64> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i64> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i64> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i64> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v2f64.p0(<2 x double> [[TMP46]], <2 x double> [[TMP48]], <2 x double> [[TMP50]], <2 x double> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_f64(float64_t *a, float64x2x4_t b) {
  vst4q_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <16 x i8>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <16 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X16X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X16X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <16 x i8>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <16 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <16 x i8>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <16 x i8>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x i8>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <16 x i8>, ptr [[TMP28]], align 16
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <16 x i8>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <16 x i8>, ptr [[TMP32]], align 16
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_POLY8X16X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <16 x i8>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP33:%.*]] = load <16 x i8>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <16 x i8>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <16 x i8> [[_MSLD7]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <16 x i8> [[_MSLD8]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[_MSLD9]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP39]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[_MSLD10]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP40]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP41:%.*]], label [[TMP42:%.*]], !prof [[PROF2]]
// CHECK:       41:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       42:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v16i8.p0(<16 x i8> [[TMP21]], <16 x i8> [[TMP25]], <16 x i8> [[TMP29]], <16 x i8> [[TMP33]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_p8(poly8_t *a, poly8x16x4_t b) {
  vst4q_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4q_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i16>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <8 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X8X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X8X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <8 x i16>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <8 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i16>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i16>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i16> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <8 x i16>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <8 x i16>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <8 x i16> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <8 x i16> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i16>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <8 x i16>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i16> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i16> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_POLY16X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <8 x i16>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <8 x i16>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <8 x i16>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i16> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i16> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <8 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <8 x i16>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <8 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <8 x i16>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <8 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <8 x i16>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <8 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <8 x i16>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <8 x i16> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <8 x i16> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <8 x i16> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <8 x i16> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v8i16.p0(<8 x i16> [[TMP46]], <8 x i16> [[TMP48]], <8 x i16> [[TMP50]], <8 x i16> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4q_p16(poly16_t *a, poly16x8x4_t b) {
  vst4q_p16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_u8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT8X8X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <8 x i8>, ptr [[TMP32]], align 8
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <8 x i8>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i8> [[_MSLD7]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i8> [[_MSLD8]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[_MSLD9]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP39]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[_MSLD10]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP40]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP41:%.*]], label [[TMP42:%.*]], !prof [[PROF2]]
// CHECK:       41:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       42:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], <8 x i8> [[TMP29]], <8 x i8> [[TMP33]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_u8(uint8_t *a, uint8x8x4_t b) {
  vst4_u8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_u16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT16X4X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <4 x i16>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <4 x i16> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <4 x i16> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x i16>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <4 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <4 x i16>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <4 x i16> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <4 x i16> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <4 x i16> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <4 x i16> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v4i16.p0(<4 x i16> [[TMP46]], <4 x i16> [[TMP48]], <4 x i16> [[TMP50]], <4 x i16> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_u16(uint16_t *a, uint16x4x4_t b) {
  vst4_u16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_u32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT32X2X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <2 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i32>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i32> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i32> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i32>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i32> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x i32>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i32>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i32> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x i32> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x i32>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x i32>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <2 x i32>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <2 x i32>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <2 x i32>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <2 x i32>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i32> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i32> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i32> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i32> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v2i32.p0(<2 x i32> [[TMP46]], <2 x i32> [[TMP48]], <2 x i32> [[TMP50]], <2 x i32> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_u32(uint32_t *a, uint32x2x4_t b) {
  vst4_u32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_u64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_UINT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_UINT64X1X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_UINT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <1 x i64>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <1 x i64> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <1 x i64> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x i64>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <1 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <1 x i64>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <1 x i64> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <1 x i64> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <1 x i64> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <1 x i64> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v1i64.p0(<1 x i64> [[TMP46]], <1 x i64> [[TMP48]], <1 x i64> [[TMP50]], <1 x i64> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_u64(uint64_t *a, uint64x1x4_t b) {
  vst4_u64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_s8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT8X8X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <8 x i8>, ptr [[TMP32]], align 8
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <8 x i8>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i8> [[_MSLD7]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i8> [[_MSLD8]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[_MSLD9]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP39]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[_MSLD10]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP40]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP41:%.*]], label [[TMP42:%.*]], !prof [[PROF2]]
// CHECK:       41:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       42:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], <8 x i8> [[TMP29]], <8 x i8> [[TMP33]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_s8(int8_t *a, int8x8x4_t b) {
  vst4_s8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_s16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT16X4X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <4 x i16>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <4 x i16> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <4 x i16> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x i16>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <4 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <4 x i16>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <4 x i16> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <4 x i16> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <4 x i16> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <4 x i16> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v4i16.p0(<4 x i16> [[TMP46]], <4 x i16> [[TMP48]], <4 x i16> [[TMP50]], <4 x i16> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_s16(int16_t *a, int16x4x4_t b) {
  vst4_s16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_s32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i32>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT32X2X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <2 x i32>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i32>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i32> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i32>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i32> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i32>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i32>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i32> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x i32>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x i32>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i32>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i32> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x i32> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x i32>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x i32>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <2 x i32>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <2 x i32>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <2 x i32>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <2 x i32>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i32> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i32> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i32> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i32> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v2i32.p0(<2 x i32> [[TMP46]], <2 x i32> [[TMP48]], <2 x i32> [[TMP50]], <2 x i32> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_s32(int32_t *a, int32x2x4_t b) {
  vst4_s32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_s64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_INT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_INT64X1X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_INT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <1 x i64>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <1 x i64> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <1 x i64> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x i64>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <1 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <1 x i64>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <1 x i64> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <1 x i64> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <1 x i64> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <1 x i64> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v1i64.p0(<1 x i64> [[TMP46]], <1 x i64> [[TMP48]], <1 x i64> [[TMP50]], <1 x i64> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_s64(int64_t *a, int64x1x4_t b) {
  vst4_s64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_f16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x half>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT16X4X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <4 x half>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <4 x half>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x half>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x half> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <4 x half>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x half>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x half> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <4 x half>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x half>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x half> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <4 x half>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <4 x half>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <4 x i16>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <4 x i16> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <4 x half> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x half>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x half>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x half>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <4 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <4 x half>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <4 x i16> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <4 x i16> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <4 x i16> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <4 x i16> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v4f16.p0(<4 x half> [[TMP46]], <4 x half> [[TMP48]], <4 x half> [[TMP50]], <4 x half> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_f16(float16_t *a, float16x4x4_t b) {
  vst4_f16(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_f32(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x float>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i32>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT32X2X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i32>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <2 x float>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x float>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x float>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i32>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i32> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x float> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x float>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x float>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i32>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i32> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x float> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x float>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x float>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i32>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i32> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x float> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT32X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x float>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x float>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i32>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i32> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x float> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <2 x i32>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <2 x float>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <2 x i32>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <2 x float>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <2 x i32>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <2 x float>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <2 x i32>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <2 x float>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i32> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i32> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i32> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i32> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v2f32.p0(<2 x float> [[TMP46]], <2 x float> [[TMP48]], <2 x float> [[TMP50]], <2 x float> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_f32(float32_t *a, float32x2x4_t b) {
  vst4_f32(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_f64(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <1 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x double>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x double> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x double> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x double> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <1 x double>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <1 x i64>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <1 x i64> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <1 x double> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x double>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x double>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x double>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <1 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <1 x double>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <1 x i64> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <1 x i64> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <1 x i64> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <1 x i64> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v1f64.p0(<1 x double> [[TMP46]], <1 x double> [[TMP48]], <1 x double> [[TMP50]], <1 x double> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_f64(float64_t *a, float64x1x4_t b) {
  vst4_f64(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_p8(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <8 x i8>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <8 x i8>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY8X8X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY8X8X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <8 x i8>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <8 x i8>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i8>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <8 x i8>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP25:%.*]] = load <8 x i8>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP26:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP27:%.*]] = xor i64 [[TMP26]], 193514046488576
// CHECK-NEXT:    [[TMP28:%.*]] = inttoptr i64 [[TMP27]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <8 x i8>, ptr [[TMP28]], align 8
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP29:%.*]] = load <8 x i8>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP30:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP31:%.*]] = xor i64 [[TMP30]], 193514046488576
// CHECK-NEXT:    [[TMP32:%.*]] = inttoptr i64 [[TMP31]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <8 x i8>, ptr [[TMP32]], align 8
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_POLY8X8X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <8 x i8>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP33:%.*]] = load <8 x i8>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <8 x i8>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <8 x i8> [[_MSLD7]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <8 x i8> [[_MSLD8]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[_MSLD9]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP39]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[_MSLD10]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP40]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP41:%.*]], label [[TMP42:%.*]], !prof [[PROF2]]
// CHECK:       41:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       42:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v8i8.p0(<8 x i8> [[TMP21]], <8 x i8> [[TMP25]], <8 x i8> [[TMP29]], <8 x i8> [[TMP33]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_p8(poly8_t *a, poly8x8x4_t b) {
  vst4_p8(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst4_p16(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <4 x i16>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <4 x i16>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY16X4X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY16X4X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <4 x i16>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <4 x i16>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <4 x i16>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <4 x i16>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <4 x i16> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <4 x i16> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <4 x i16>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <4 x i16>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <4 x i16> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <4 x i16> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <4 x i16>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <4 x i16>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <4 x i16> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <4 x i16> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_POLY16X4X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <4 x i16>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <4 x i16>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <4 x i16>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <4 x i16> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <4 x i16> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <4 x i16>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <4 x i16>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <4 x i16>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <4 x i16>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <4 x i16>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <4 x i16>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <4 x i16>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <4 x i16>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <4 x i16> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <4 x i16> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <4 x i16> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <4 x i16> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st4.v4i16.p0(<4 x i16> [[TMP46]], <4 x i16> [[TMP48]], <4 x i16> [[TMP50]], <4 x i16> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst4_p16(poly16_t *a, poly16x4x4_t b) {
  vst4_p16(a, b);
}

// CHECK-LABEL: define dso_local %struct.float64x2x2_t @test_vld1q_f64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <2 x double>, <2 x double> } @llvm.aarch64.neon.ld1x2.v2f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x double>, <2 x double> } [[VLD1XN]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X2X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X2_T]] [[TMP22]]
//
float64x2x2_t test_vld1q_f64_x2(float64_t const *a) {
  return vld1q_f64_x2(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x2_t @test_vld1q_p64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X2_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld1x2.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64> } [[VLD1XN]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY64X2X2_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [2 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X2_T]] [[TMP22]]
//
poly64x2x2_t test_vld1q_p64_x2(poly64_t const *a) {
  return vld1q_p64_x2(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x2_t @test_vld1_f64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <1 x double>, <1 x double> } @llvm.aarch64.neon.ld1x2.v1f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x double>, <1 x double> } [[VLD1XN]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X1X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X2_T]] [[TMP22]]
//
float64x1x2_t test_vld1_f64_x2(float64_t const *a) {
  return vld1_f64_x2(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x2_t @test_vld1_p64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X2_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld1x2.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64> } [[VLD1XN]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 16)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY64X1X2_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [2 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [2 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X2_T]] [[TMP22]]
//
poly64x1x2_t test_vld1_p64_x2(poly64_t const *a) {
  return vld1_p64_x2(a);
}

// CHECK-LABEL: define dso_local %struct.float64x2x3_t @test_vld1q_f64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld1x3.v2f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x double>, <2 x double>, <2 x double> } [[VLD1XN]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X2X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X3_T]] [[TMP22]]
//
float64x2x3_t test_vld1q_f64_x3(float64_t const *a) {
  return vld1q_f64_x3(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x3_t @test_vld1q_p64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X3_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld1x3.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64> } [[VLD1XN]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 48)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY64X2X3_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [3 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X3_T]] [[TMP22]]
//
poly64x2x3_t test_vld1q_p64_x3(poly64_t const *a) {
  return vld1q_p64_x3(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x3_t @test_vld1_f64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld1x3.v1f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x double>, <1 x double>, <1 x double> } [[VLD1XN]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X1X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X3_T]] [[TMP22]]
//
float64x1x3_t test_vld1_f64_x3(float64_t const *a) {
  return vld1_f64_x3(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x3_t @test_vld1_p64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X3_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld1x3.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64> } [[VLD1XN]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 24)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY64X1X3_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [3 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [3 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X3_T]] [[TMP22]]
//
poly64x1x3_t test_vld1_p64_x3(poly64_t const *a) {
  return vld1_p64_x3(a);
}

// CHECK-LABEL: define dso_local %struct.float64x2x4_t @test_vld1q_f64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <2 x double>, <2 x double>, <2 x double>, <2 x double> } @llvm.aarch64.neon.ld1x4.v2f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x double>, <2 x double>, <2 x double>, <2 x double> } [[VLD1XN]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X2X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X2X4_T]] [[TMP22]]
//
float64x2x4_t test_vld1q_f64_x4(float64_t const *a) {
  return vld1q_f64_x4(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x2x4_t @test_vld1q_p64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP2]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X2X4_T]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP11]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } @llvm.aarch64.neon.ld1x4.v2i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } zeroinitializer, ptr [[TMP20]], align 16
// CHECK-NEXT:    store { <2 x i64>, <2 x i64>, <2 x i64>, <2 x i64> } [[VLD1XN]], ptr [[__RET]], align 16
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 64)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY64X2X4_T]], ptr [[RETVAL]], align 16
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <2 x i64>] }, ptr [[TMP25]], align 16
// CHECK-NEXT:    store { [4 x <2 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY64X2X4_T]] [[TMP22]]
//
poly64x2x4_t test_vld1q_p64_x4(poly64_t const *a) {
  return vld1q_p64_x4(a);
}

// CHECK-LABEL: define dso_local %struct.float64x1x4_t @test_vld1_f64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <1 x double>, <1 x double>, <1 x double>, <1 x double> } @llvm.aarch64.neon.ld1x4.v1f64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x double>, <1 x double>, <1 x double>, <1 x double> } [[VLD1XN]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_FLOAT64X1X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_FLOAT64X1X4_T]] [[TMP22]]
//
float64x1x4_t test_vld1_f64_x4(float64_t const *a) {
  return vld1_f64_x4(a);
}

// CHECK-LABEL: define dso_local %struct.poly64x1x4_t @test_vld1_p64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[RETVAL:%.*]] = alloca [[STRUCT_POLY64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP1:%.*]] = xor i64 [[TMP0]], 193514046488576
// CHECK-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[TMP1]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP2]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP4:%.*]] = xor i64 [[TMP3]], 193514046488576
// CHECK-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[TMP4]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP5]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__RET:%.*]] = alloca [[STRUCT_POLY64X1X4_T]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP7:%.*]] = xor i64 [[TMP6]], 193514046488576
// CHECK-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP8]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP9]], 193514046488576
// CHECK-NEXT:    [[TMP11:%.*]] = inttoptr i64 [[TMP10]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP11]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP12:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP15]], align 8
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    br i1 [[_MSCMP]], label [[TMP16:%.*]], label [[TMP17:%.*]], !prof [[PROF2]]
// CHECK:       16:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       17:
// CHECK-NEXT:    [[VLD1XN:%.*]] = call { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } @llvm.aarch64.neon.ld1x4.v1i64.p0(ptr [[TMP12]])
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[__RET]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } zeroinitializer, ptr [[TMP20]], align 8
// CHECK-NEXT:    store { <1 x i64>, <1 x i64>, <1 x i64>, <1 x i64> } [[VLD1XN]], ptr [[__RET]], align 8
// CHECK-NEXT:    [[TMP21:%.*]] = call ptr @__msan_memcpy(ptr [[RETVAL]], ptr [[__RET]], i64 32)
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__RET]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP22:%.*]] = load [[STRUCT_POLY64X1X4_T]], ptr [[RETVAL]], align 8
// CHECK-NEXT:    [[TMP23:%.*]] = ptrtoint ptr [[RETVAL]] to i64
// CHECK-NEXT:    [[TMP24:%.*]] = xor i64 [[TMP23]], 193514046488576
// CHECK-NEXT:    [[TMP25:%.*]] = inttoptr i64 [[TMP24]] to ptr
// CHECK-NEXT:    [[_MSLD1:%.*]] = load { [4 x <1 x i64>] }, ptr [[TMP25]], align 8
// CHECK-NEXT:    store { [4 x <1 x i64>] } [[_MSLD1]], ptr @__msan_retval_tls, align 8
// CHECK-NEXT:    ret [[STRUCT_POLY64X1X4_T]] [[TMP22]]
//
poly64x1x4_t test_vld1_p64_x4(poly64_t const *a) {
  return vld1_p64_x4(a);
}

// CHECK-LABEL: define dso_local void @test_vst1q_f64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <2 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x double>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x double> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x double> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x double>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x double>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x2.v2f64.p0(<2 x double> [[TMP34]], <2 x double> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_f64_x2(float64_t *a, float64x2x2_t b) {
  vst1q_f64_x2(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_p64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X2_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X2_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [2 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD3]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD4]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[TMP33]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP35]] to i128
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i128 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x2.v2i64.p0(<2 x i64> [[TMP34]], <2 x i64> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_p64_x2(poly64_t *a, poly64x2x2_t b) {
  vst1q_p64_x2(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_f64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <1 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <1 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x double>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x double> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <1 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x double> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x double>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x double>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x2.v1f64.p0(<1 x double> [[TMP34]], <1 x double> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_f64_x2(float64_t *a, float64x1x2_t b) {
  vst1_f64_x2(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_p64_x2(
// CHECK-SAME: ptr noundef [[A:%.*]], [2 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [2 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X2_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X2_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X2_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [2 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [2 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 16, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 16)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [2 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD3:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD3]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X2_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [2 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD4:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD4]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[TMP33:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP35:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP36:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[TMP33]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP37]], 0
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP35]] to i64
// CHECK-NEXT:    [[_MSCMP5:%.*]] = icmp ne i64 [[TMP38]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP5]]
// CHECK-NEXT:    [[_MSCMP6:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR7:%.*]] = or i1 [[_MSOR]], [[_MSCMP6]]
// CHECK-NEXT:    br i1 [[_MSOR7]], label [[TMP39:%.*]], label [[TMP40:%.*]], !prof [[PROF2]]
// CHECK:       39:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       40:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x2.v1i64.p0(<1 x i64> [[TMP34]], <1 x i64> [[TMP36]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_p64_x2(poly64_t *a, poly64x1x2_t b) {
  vst1_p64_x2(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_f64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <2 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x double>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x double> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x double> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x double> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x double>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x double>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x double>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i64> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i64> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i64> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x3.v2f64.p0(<2 x double> [[TMP40]], <2 x double> [[TMP42]], <2 x double> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_f64_x3(float64_t *a, float64x2x3_t b) {
  vst1q_f64_x3(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_p64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X3_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X3_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [3 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 48, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 48)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD5]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD6]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <2 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x i64>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <2 x i64> [[TMP39]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <2 x i64> [[TMP41]] to i128
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i128 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <2 x i64> [[TMP43]] to i128
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i128 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x3.v2i64.p0(<2 x i64> [[TMP40]], <2 x i64> [[TMP42]], <2 x i64> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 48, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_p64_x3(poly64_t *a, poly64x2x3_t b) {
  vst1q_p64_x3(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_f64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <1 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <1 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x double>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x double> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <1 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x double> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <1 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x double> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x double>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x double>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x double>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <1 x i64> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <1 x i64> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <1 x i64> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x3.v1f64.p0(<1 x double> [[TMP40]], <1 x double> [[TMP42]], <1 x double> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_f64_x3(float64_t *a, float64x1x3_t b) {
  vst1_f64_x3(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_p64_x3(
// CHECK-SAME: ptr noundef [[A:%.*]], [3 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [3 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X3_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X3_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X3_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [3 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [3 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 24, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 24)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD5:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD5]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD6:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD6]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X3_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [3 x <1 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[TMP39:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP40:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP41:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP42:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x i64>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <1 x i64> [[TMP39]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP45]], 0
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <1 x i64> [[TMP41]] to i64
// CHECK-NEXT:    [[_MSCMP8:%.*]] = icmp ne i64 [[TMP46]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP8]]
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <1 x i64> [[TMP43]] to i64
// CHECK-NEXT:    [[_MSCMP9:%.*]] = icmp ne i64 [[TMP47]], 0
// CHECK-NEXT:    [[_MSOR10:%.*]] = or i1 [[_MSOR]], [[_MSCMP9]]
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR12:%.*]] = or i1 [[_MSOR10]], [[_MSCMP11]]
// CHECK-NEXT:    br i1 [[_MSOR12]], label [[TMP48:%.*]], label [[TMP49:%.*]], !prof [[PROF2]]
// CHECK:       48:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       49:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x3.v1i64.p0(<1 x i64> [[TMP40]], <1 x i64> [[TMP42]], <1 x i64> [[TMP44]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 24, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_p64_x3(poly64_t *a, poly64x1x3_t b) {
  vst1_p64_x3(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_f64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x double>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X2X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <2 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x double>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x double> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x double>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x double> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x double>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x double> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x double>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x double>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i64>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i64> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x double> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x double>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x double>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x double>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <2 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <2 x double>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i64> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i64> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i64> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i64> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x4.v2f64.p0(<2 x double> [[TMP46]], <2 x double> [[TMP48]], <2 x double> [[TMP50]], <2 x double> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_f64_x4(float64_t *a, float64x2x4_t b) {
  vst1q_f64_x4(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1q_p64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <2 x i64>] alignstack(16) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <2 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X2X4_T:%.*]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP3]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X2X4_T]], align 16
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <2 x i64>] [[TMP0]], ptr [[TMP9]], align 16
// CHECK-NEXT:    store [4 x <2 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 16 [[TMP15]], i8 -1, i64 64, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 64)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <2 x i64>, ptr [[ARRAYIDX]], align 16
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <2 x i64>, ptr [[TMP24]], align 16
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <2 x i64> [[_MSLD7]] to <16 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <2 x i64> [[TMP21]] to <16 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <2 x i64>, ptr [[ARRAYIDX2]], align 16
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <2 x i64>, ptr [[TMP30]], align 16
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <2 x i64> [[_MSLD8]] to <16 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <2 x i64> [[TMP27]] to <16 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <2 x i64>, ptr [[ARRAYIDX4]], align 16
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <2 x i64>, ptr [[TMP36]], align 16
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <2 x i64> [[_MSLD9]] to <16 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <2 x i64> [[TMP33]] to <16 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_POLY64X2X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <2 x i64>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <2 x i64>, ptr [[ARRAYIDX6]], align 16
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <2 x i64>, ptr [[TMP42]], align 16
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <2 x i64> [[_MSLD10]] to <16 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <2 x i64> [[TMP39]] to <16 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <16 x i8> [[TMP25]] to <2 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <16 x i8> [[TMP26]] to <2 x i64>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <16 x i8> [[TMP31]] to <2 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <16 x i8> [[TMP32]] to <2 x i64>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <16 x i8> [[TMP37]] to <2 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <16 x i8> [[TMP38]] to <2 x i64>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <16 x i8> [[TMP43]] to <2 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <16 x i8> [[TMP44]] to <2 x i64>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <2 x i64> [[TMP45]] to i128
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i128 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <2 x i64> [[TMP47]] to i128
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i128 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <2 x i64> [[TMP49]] to i128
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i128 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <2 x i64> [[TMP51]] to i128
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i128 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x4.v2i64.p0(<2 x i64> [[TMP46]], <2 x i64> [[TMP48]], <2 x i64> [[TMP50]], <2 x i64> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 64, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1q_p64_x4(poly64_t *a, poly64x2x4_t b) {
  vst1q_p64_x4(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_f64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x double>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_FLOAT64X1X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <1 x double>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x double>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x double> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x double>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x double> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x double>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x double> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_FLOAT64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <1 x double>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <1 x double>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <1 x i64>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <1 x i64> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <1 x double> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x double>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x double>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x double>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <1 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <1 x double>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <1 x i64> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <1 x i64> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <1 x i64> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <1 x i64> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x4.v1f64.p0(<1 x double> [[TMP46]], <1 x double> [[TMP48]], <1 x double> [[TMP50]], <1 x double> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_f64_x4(float64_t *a, float64x1x4_t b) {
  vst1_f64_x4(a, b);
}

// CHECK-LABEL: define dso_local void @test_vst1_p64_x4(
// CHECK-SAME: ptr noundef [[A:%.*]], [4 x <1 x i64>] alignstack(8) [[B_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load [4 x <1 x i64>], ptr inttoptr (i64 add (i64 ptrtoint (ptr @__msan_param_tls to i64), i64 8) to ptr), align 8
// CHECK-NEXT:    call void @llvm.donothing()
// CHECK-NEXT:    [[B:%.*]] = alloca [[STRUCT_POLY64X1X4_T:%.*]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[B]] to i64
// CHECK-NEXT:    [[TMP2:%.*]] = xor i64 [[TMP1]], 193514046488576
// CHECK-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP3]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP5:%.*]] = xor i64 [[TMP4]], 193514046488576
// CHECK-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[TMP5]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP6]], i8 -1, i64 8, i1 false)
// CHECK-NEXT:    [[__S1:%.*]] = alloca [[STRUCT_POLY64X1X4_T]], align 8
// CHECK-NEXT:    [[COERCE_DIVE:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X4_T]], ptr [[B]], i32 0, i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = ptrtoint ptr [[COERCE_DIVE]] to i64
// CHECK-NEXT:    [[TMP8:%.*]] = xor i64 [[TMP7]], 193514046488576
// CHECK-NEXT:    [[TMP9:%.*]] = inttoptr i64 [[TMP8]] to ptr
// CHECK-NEXT:    store [4 x <1 x i64>] [[TMP0]], ptr [[TMP9]], align 8
// CHECK-NEXT:    store [4 x <1 x i64>] [[B_COERCE]], ptr [[COERCE_DIVE]], align 8
// CHECK-NEXT:    [[TMP10:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP11:%.*]] = xor i64 [[TMP10]], 193514046488576
// CHECK-NEXT:    [[TMP12:%.*]] = inttoptr i64 [[TMP11]] to ptr
// CHECK-NEXT:    store i64 0, ptr [[TMP12]], align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    [[TMP13:%.*]] = ptrtoint ptr [[__S1]] to i64
// CHECK-NEXT:    [[TMP14:%.*]] = xor i64 [[TMP13]], 193514046488576
// CHECK-NEXT:    [[TMP15:%.*]] = inttoptr i64 [[TMP14]] to ptr
// CHECK-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP15]], i8 -1, i64 32, i1 false)
// CHECK-NEXT:    [[TMP16:%.*]] = call ptr @__msan_memcpy(ptr [[__S1]], ptr [[B]], i64 32)
// CHECK-NEXT:    [[TMP17:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP18:%.*]] = ptrtoint ptr [[A_ADDR]] to i64
// CHECK-NEXT:    [[TMP19:%.*]] = xor i64 [[TMP18]], 193514046488576
// CHECK-NEXT:    [[TMP20:%.*]] = inttoptr i64 [[TMP19]] to ptr
// CHECK-NEXT:    [[_MSLD:%.*]] = load i64, ptr [[TMP20]], align 8
// CHECK-NEXT:    [[VAL:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL]], i64 0, i64 0
// CHECK-NEXT:    [[TMP21:%.*]] = load <1 x i64>, ptr [[ARRAYIDX]], align 8
// CHECK-NEXT:    [[TMP22:%.*]] = ptrtoint ptr [[ARRAYIDX]] to i64
// CHECK-NEXT:    [[TMP23:%.*]] = xor i64 [[TMP22]], 193514046488576
// CHECK-NEXT:    [[TMP24:%.*]] = inttoptr i64 [[TMP23]] to ptr
// CHECK-NEXT:    [[_MSLD7:%.*]] = load <1 x i64>, ptr [[TMP24]], align 8
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <1 x i64> [[_MSLD7]] to <8 x i8>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <1 x i64> [[TMP21]] to <8 x i8>
// CHECK-NEXT:    [[VAL1:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL1]], i64 0, i64 1
// CHECK-NEXT:    [[TMP27:%.*]] = load <1 x i64>, ptr [[ARRAYIDX2]], align 8
// CHECK-NEXT:    [[TMP28:%.*]] = ptrtoint ptr [[ARRAYIDX2]] to i64
// CHECK-NEXT:    [[TMP29:%.*]] = xor i64 [[TMP28]], 193514046488576
// CHECK-NEXT:    [[TMP30:%.*]] = inttoptr i64 [[TMP29]] to ptr
// CHECK-NEXT:    [[_MSLD8:%.*]] = load <1 x i64>, ptr [[TMP30]], align 8
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <1 x i64> [[_MSLD8]] to <8 x i8>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <1 x i64> [[TMP27]] to <8 x i8>
// CHECK-NEXT:    [[VAL3:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX4:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL3]], i64 0, i64 2
// CHECK-NEXT:    [[TMP33:%.*]] = load <1 x i64>, ptr [[ARRAYIDX4]], align 8
// CHECK-NEXT:    [[TMP34:%.*]] = ptrtoint ptr [[ARRAYIDX4]] to i64
// CHECK-NEXT:    [[TMP35:%.*]] = xor i64 [[TMP34]], 193514046488576
// CHECK-NEXT:    [[TMP36:%.*]] = inttoptr i64 [[TMP35]] to ptr
// CHECK-NEXT:    [[_MSLD9:%.*]] = load <1 x i64>, ptr [[TMP36]], align 8
// CHECK-NEXT:    [[TMP37:%.*]] = bitcast <1 x i64> [[_MSLD9]] to <8 x i8>
// CHECK-NEXT:    [[TMP38:%.*]] = bitcast <1 x i64> [[TMP33]] to <8 x i8>
// CHECK-NEXT:    [[VAL5:%.*]] = getelementptr inbounds [[STRUCT_POLY64X1X4_T]], ptr [[__S1]], i32 0, i32 0
// CHECK-NEXT:    [[ARRAYIDX6:%.*]] = getelementptr inbounds [4 x <1 x i64>], ptr [[VAL5]], i64 0, i64 3
// CHECK-NEXT:    [[TMP39:%.*]] = load <1 x i64>, ptr [[ARRAYIDX6]], align 8
// CHECK-NEXT:    [[TMP40:%.*]] = ptrtoint ptr [[ARRAYIDX6]] to i64
// CHECK-NEXT:    [[TMP41:%.*]] = xor i64 [[TMP40]], 193514046488576
// CHECK-NEXT:    [[TMP42:%.*]] = inttoptr i64 [[TMP41]] to ptr
// CHECK-NEXT:    [[_MSLD10:%.*]] = load <1 x i64>, ptr [[TMP42]], align 8
// CHECK-NEXT:    [[TMP43:%.*]] = bitcast <1 x i64> [[_MSLD10]] to <8 x i8>
// CHECK-NEXT:    [[TMP44:%.*]] = bitcast <1 x i64> [[TMP39]] to <8 x i8>
// CHECK-NEXT:    [[TMP45:%.*]] = bitcast <8 x i8> [[TMP25]] to <1 x i64>
// CHECK-NEXT:    [[TMP46:%.*]] = bitcast <8 x i8> [[TMP26]] to <1 x i64>
// CHECK-NEXT:    [[TMP47:%.*]] = bitcast <8 x i8> [[TMP31]] to <1 x i64>
// CHECK-NEXT:    [[TMP48:%.*]] = bitcast <8 x i8> [[TMP32]] to <1 x i64>
// CHECK-NEXT:    [[TMP49:%.*]] = bitcast <8 x i8> [[TMP37]] to <1 x i64>
// CHECK-NEXT:    [[TMP50:%.*]] = bitcast <8 x i8> [[TMP38]] to <1 x i64>
// CHECK-NEXT:    [[TMP51:%.*]] = bitcast <8 x i8> [[TMP43]] to <1 x i64>
// CHECK-NEXT:    [[TMP52:%.*]] = bitcast <8 x i8> [[TMP44]] to <1 x i64>
// CHECK-NEXT:    [[TMP53:%.*]] = bitcast <1 x i64> [[TMP45]] to i64
// CHECK-NEXT:    [[_MSCMP:%.*]] = icmp ne i64 [[TMP53]], 0
// CHECK-NEXT:    [[TMP54:%.*]] = bitcast <1 x i64> [[TMP47]] to i64
// CHECK-NEXT:    [[_MSCMP11:%.*]] = icmp ne i64 [[TMP54]], 0
// CHECK-NEXT:    [[_MSOR:%.*]] = or i1 [[_MSCMP]], [[_MSCMP11]]
// CHECK-NEXT:    [[TMP55:%.*]] = bitcast <1 x i64> [[TMP49]] to i64
// CHECK-NEXT:    [[_MSCMP12:%.*]] = icmp ne i64 [[TMP55]], 0
// CHECK-NEXT:    [[_MSOR13:%.*]] = or i1 [[_MSOR]], [[_MSCMP12]]
// CHECK-NEXT:    [[TMP56:%.*]] = bitcast <1 x i64> [[TMP51]] to i64
// CHECK-NEXT:    [[_MSCMP14:%.*]] = icmp ne i64 [[TMP56]], 0
// CHECK-NEXT:    [[_MSOR15:%.*]] = or i1 [[_MSOR13]], [[_MSCMP14]]
// CHECK-NEXT:    [[_MSCMP16:%.*]] = icmp ne i64 [[_MSLD]], 0
// CHECK-NEXT:    [[_MSOR17:%.*]] = or i1 [[_MSOR15]], [[_MSCMP16]]
// CHECK-NEXT:    br i1 [[_MSOR17]], label [[TMP57:%.*]], label [[TMP58:%.*]], !prof [[PROF2]]
// CHECK:       57:
// CHECK-NEXT:    call void @__msan_warning_noreturn() #[[ATTR7]]
// CHECK-NEXT:    unreachable
// CHECK:       58:
// CHECK-NEXT:    call void @llvm.aarch64.neon.st1x4.v1i64.p0(<1 x i64> [[TMP46]], <1 x i64> [[TMP48]], <1 x i64> [[TMP50]], <1 x i64> [[TMP52]], ptr [[TMP17]])
// CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[__S1]]) #[[ATTR4]]
// CHECK-NEXT:    ret void
//
void test_vst1_p64_x4(poly64_t *a, poly64x1x4_t b) {
  vst1_p64_x4(a, b);
}
//.
// CHECK: [[PROF2]] = !{!"branch_weights", i32 1, i32 1048575}
//.
