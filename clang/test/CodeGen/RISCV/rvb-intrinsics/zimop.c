// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv32 -target-feature +experimental-zimop -emit-llvm %s -o - \
// RUN:     -disable-O0-optnone | opt -S -passes=mem2reg \
// RUN:     | FileCheck %s  -check-prefix=RV32ZIMOP
// RUN: %clang_cc1 -triple riscv64 -target-feature +experimental-zimop -emit-llvm %s -o - \
// RUN:     -disable-O0-optnone | opt -S -passes=mem2reg \
// RUN:     | FileCheck %s  -check-prefix=RV64ZIMOP

#include <stdint.h>

#if __riscv_xlen == 64
// RV64ZIMOP-LABEL: @mopr_0_64(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mopr.i64(i64 [[A:%.*]], i64 0)
// RV64ZIMOP-NEXT:    ret i64 [[TMP0]]
//
uint64_t mopr_0_64(uint64_t a) {
  return __builtin_riscv_mopr_64(a, 0);
}

// RV64ZIMOP-LABEL: @mopr_31_64(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mopr.i64(i64 [[A:%.*]], i64 31)
// RV64ZIMOP-NEXT:    ret i64 [[TMP0]]
//
uint64_t mopr_31_64(uint64_t a) {
  return __builtin_riscv_mopr_64(a, 31);
}

// RV64ZIMOP-LABEL: @moprr_0_64(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.moprr.i64(i64 [[A:%.*]], i64 [[B:%.*]], i64 0)
// RV64ZIMOP-NEXT:    ret i64 [[TMP0]]
//
uint64_t moprr_0_64(uint64_t a, uint64_t b) {
  return __builtin_riscv_moprr_64(a, b, 0);
}

// RV64ZIMOP-LABEL: @moprr_7_64(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.moprr.i64(i64 [[A:%.*]], i64 [[B:%.*]], i64 7)
// RV64ZIMOP-NEXT:    ret i64 [[TMP0]]
//
uint64_t moprr_7_64(uint64_t a, uint64_t b) {
  return __builtin_riscv_moprr_64(a, b, 7);
}

#endif

// RV32ZIMOP-LABEL: @mopr_0_32(
// RV32ZIMOP-NEXT:  entry:
// RV32ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.mopr.i32(i32 [[A:%.*]], i32 0)
// RV32ZIMOP-NEXT:    ret i32 [[TMP0]]
//
// RV64ZIMOP-LABEL: @mopr_0_32(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.mopr.i32(i32 [[A:%.*]], i32 0)
// RV64ZIMOP-NEXT:    ret i32 [[TMP0]]
//
uint32_t mopr_0_32(uint32_t a) {
  return __builtin_riscv_mopr_32(a, 0);
}

// RV32ZIMOP-LABEL: @mopr_31_32(
// RV32ZIMOP-NEXT:  entry:
// RV32ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.mopr.i32(i32 [[A:%.*]], i32 31)
// RV32ZIMOP-NEXT:    ret i32 [[TMP0]]
//
// RV64ZIMOP-LABEL: @mopr_31_32(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.mopr.i32(i32 [[A:%.*]], i32 31)
// RV64ZIMOP-NEXT:    ret i32 [[TMP0]]
//
uint32_t mopr_31_32(uint32_t a) {
  return __builtin_riscv_mopr_32(a, 31);
}

// RV32ZIMOP-LABEL: @moprr_0_32(
// RV32ZIMOP-NEXT:  entry:
// RV32ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.moprr.i32(i32 [[A:%.*]], i32 [[B:%.*]], i32 0)
// RV32ZIMOP-NEXT:    ret i32 [[TMP0]]
//
// RV64ZIMOP-LABEL: @moprr_0_32(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.moprr.i32(i32 [[A:%.*]], i32 [[B:%.*]], i32 0)
// RV64ZIMOP-NEXT:    ret i32 [[TMP0]]
//
uint32_t moprr_0_32(uint32_t a, uint32_t b) {
  return __builtin_riscv_moprr_32(a, b, 0);
}

// RV32ZIMOP-LABEL: @moprr_7_32(
// RV32ZIMOP-NEXT:  entry:
// RV32ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.moprr.i32(i32 [[A:%.*]], i32 [[B:%.*]], i32 7)
// RV32ZIMOP-NEXT:    ret i32 [[TMP0]]
//
// RV64ZIMOP-LABEL: @moprr_7_32(
// RV64ZIMOP-NEXT:  entry:
// RV64ZIMOP-NEXT:    [[TMP0:%.*]] = call i32 @llvm.riscv.moprr.i32(i32 [[A:%.*]], i32 [[B:%.*]], i32 7)
// RV64ZIMOP-NEXT:    ret i32 [[TMP0]]
//
uint32_t moprr_7_32(uint32_t a, uint32_t b) {
  return __builtin_riscv_moprr_32(a, b, 7);
}

