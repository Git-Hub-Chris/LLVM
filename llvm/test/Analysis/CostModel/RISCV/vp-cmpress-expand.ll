; NOTE: Assertions have been autogenerated by utils/update_analyze_test_checks.py
; RUN: opt < %s -passes="print<cost-model>" 2>&1 -disable-output -S -mtriple=riscv64 -mattr=+v,+f,+d,+zfh,+zvfh | FileCheck %s

define void @vp_compress() {
; CHECK-LABEL: 'vp_compress'
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %0 = call <vscale x 1 x i64> @llvm.experimental.vp.compress.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %1 = call <vscale x 2 x i32> @llvm.experimental.vp.compress.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %2 = call <vscale x 4 x i16> @llvm.experimental.vp.compress.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %3 = call <vscale x 8 x i8> @llvm.experimental.vp.compress.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %4 = call <vscale x 2 x i64> @llvm.experimental.vp.compress.nxv2i64(<vscale x 2 x i64> undef, <vscale x 2 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %5 = call <vscale x 4 x i32> @llvm.experimental.vp.compress.nxv4i32(<vscale x 4 x i32> undef, <vscale x 4 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %6 = call <vscale x 8 x i16> @llvm.experimental.vp.compress.nxv8i16(<vscale x 8 x i16> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %7 = call <vscale x 16 x i8> @llvm.experimental.vp.compress.nxv16i8(<vscale x 16 x i8> undef, <vscale x 16 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %8 = call <vscale x 4 x i64> @llvm.experimental.vp.compress.nxv4i64(<vscale x 4 x i64> undef, <vscale x 4 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %9 = call <vscale x 8 x i32> @llvm.experimental.vp.compress.nxv8i32(<vscale x 8 x i32> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %10 = call <vscale x 16 x i16> @llvm.experimental.vp.compress.nxv16i16(<vscale x 16 x i16> undef, <vscale x 16 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %11 = call <vscale x 32 x i8> @llvm.experimental.vp.compress.nxv32i8(<vscale x 32 x i8> undef, <vscale x 32 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %12 = call <vscale x 8 x i64> @llvm.experimental.vp.compress.nxv8i64(<vscale x 8 x i64> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %13 = call <vscale x 16 x i32> @llvm.experimental.vp.compress.nxv16i32(<vscale x 16 x i32> undef, <vscale x 16 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %14 = call <vscale x 32 x i16> @llvm.experimental.vp.compress.nxv32i16(<vscale x 32 x i16> undef, <vscale x 32 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 1 for instruction: %15 = call <vscale x 64 x i8> @llvm.experimental.vp.compress.nxv64i8(<vscale x 64 x i8> undef, <vscale x 64 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Invalid cost for instruction: %16 = call <vscale x 128 x i8> @llvm.experimental.vp.compress.nxv128i8(<vscale x 128 x i8> undef, <vscale x 128 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
;
entry:
  %0 = call <vscale x 1 x i64> @llvm.experimental.vp.compress.nxv1i64(<vscale x 1 x i64> undef,<vscale x 1 x i1> undef,i32 undef)
  %1 = call <vscale x 2 x i32> @llvm.experimental.vp.compress.nxv2i32(<vscale x 2 x i32> undef,<vscale x 2 x i1> undef,i32 undef)
  %2 = call <vscale x 4 x i16> @llvm.experimental.vp.compress.nxv4i16(<vscale x 4 x i16> undef,<vscale x 4 x i1> undef,i32 undef)
  %3 = call <vscale x 8 x i8> @llvm.experimental.vp.compress.nxv8i8(<vscale x 8 x i8> undef,<vscale x 8 x i1> undef,i32 undef)
  %4 = call <vscale x 2 x i64> @llvm.experimental.vp.compress.nxv2i64(<vscale x 2 x i64> undef,<vscale x 2 x i1> undef,i32 undef)
  %5 = call <vscale x 4 x i32> @llvm.experimental.vp.compress.nxv4i32(<vscale x 4 x i32> undef,<vscale x 4 x i1> undef,i32 undef)
  %6 = call <vscale x 8 x i16> @llvm.experimental.vp.compress.nxv8i16(<vscale x 8 x i16> undef,<vscale x 8 x i1> undef,i32 undef)
  %7 = call <vscale x 16 x i8> @llvm.experimental.vp.compress.nxv16i8(<vscale x 16 x i8> undef,<vscale x 16 x i1> undef,i32 undef)
  %8 = call <vscale x 4 x i64> @llvm.experimental.vp.compress.nxv4i64(<vscale x 4 x i64> undef,<vscale x 4 x i1> undef,i32 undef)
  %9 = call <vscale x 8 x i32> @llvm.experimental.vp.compress.nxv8i32(<vscale x 8 x i32> undef,<vscale x 8 x i1> undef,i32 undef)
  %10 = call <vscale x 16 x i16> @llvm.experimental.vp.compress.nxv16i16(<vscale x 16 x i16> undef,<vscale x 16 x i1> undef,i32 undef)
  %11 = call <vscale x 32 x i8> @llvm.experimental.vp.compress.nxv32i8(<vscale x 32 x i8> undef,<vscale x 32 x i1> undef,i32 undef)
  %12 = call <vscale x 8 x i64> @llvm.experimental.vp.compress.nxv8i64(<vscale x 8 x i64> undef,<vscale x 8 x i1> undef,i32 undef)
  %13 = call <vscale x 16 x i32> @llvm.experimental.vp.compress.nxv16i32(<vscale x 16 x i32> undef,<vscale x 16 x i1> undef,i32 undef)
  %14 = call <vscale x 32 x i16> @llvm.experimental.vp.compress.nxv32i16(<vscale x 32 x i16> undef,<vscale x 32 x i1> undef,i32 undef)
  %15 = call <vscale x 64 x i8> @llvm.experimental.vp.compress.nxv64i8(<vscale x 64 x i8> undef,<vscale x 64 x i1> undef,i32 undef)
  %16 = call <vscale x 128 x i8> @llvm.experimental.vp.compress.nxv128i8(<vscale x 128 x i8> undef,<vscale x 128 x i1> undef,i32 undef)
  ret void
}

define void @vp_expand() {
; CHECK-LABEL: 'vp_expand'
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %0 = call <vscale x 1 x i64> @llvm.experimental.vp.expand.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %1 = call <vscale x 2 x i32> @llvm.experimental.vp.expand.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %2 = call <vscale x 4 x i16> @llvm.experimental.vp.expand.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %3 = call <vscale x 8 x i8> @llvm.experimental.vp.expand.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %4 = call <vscale x 2 x i64> @llvm.experimental.vp.expand.nxv2i64(<vscale x 2 x i64> undef, <vscale x 2 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %5 = call <vscale x 4 x i32> @llvm.experimental.vp.expand.nxv4i32(<vscale x 4 x i32> undef, <vscale x 4 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %6 = call <vscale x 8 x i16> @llvm.experimental.vp.expand.nxv8i16(<vscale x 8 x i16> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %7 = call <vscale x 16 x i8> @llvm.experimental.vp.expand.nxv16i8(<vscale x 16 x i8> undef, <vscale x 16 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %8 = call <vscale x 4 x i64> @llvm.experimental.vp.expand.nxv4i64(<vscale x 4 x i64> undef, <vscale x 4 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %9 = call <vscale x 8 x i32> @llvm.experimental.vp.expand.nxv8i32(<vscale x 8 x i32> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %10 = call <vscale x 16 x i16> @llvm.experimental.vp.expand.nxv16i16(<vscale x 16 x i16> undef, <vscale x 16 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %11 = call <vscale x 32 x i8> @llvm.experimental.vp.expand.nxv32i8(<vscale x 32 x i8> undef, <vscale x 32 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %12 = call <vscale x 8 x i64> @llvm.experimental.vp.expand.nxv8i64(<vscale x 8 x i64> undef, <vscale x 8 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %13 = call <vscale x 16 x i32> @llvm.experimental.vp.expand.nxv16i32(<vscale x 16 x i32> undef, <vscale x 16 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 4 for instruction: %14 = call <vscale x 32 x i16> @llvm.experimental.vp.expand.nxv32i16(<vscale x 32 x i16> undef, <vscale x 32 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Invalid cost for instruction: %15 = call <vscale x 64 x i8> @llvm.experimental.vp.expand.nxv64i8(<vscale x 64 x i8> undef, <vscale x 64 x i1> undef, i32 undef)
; CHECK-NEXT:  Cost Model: Found an estimated cost of 0 for instruction: ret void
;
entry:
  %0 = call <vscale x 1 x i64> @llvm.experimental.vp.expand.nxv1i64(<vscale x 1 x i64> undef,<vscale x 1 x i1> undef,i32 undef)
  %1 = call <vscale x 2 x i32> @llvm.experimental.vp.expand.nxv2i32(<vscale x 2 x i32> undef,<vscale x 2 x i1> undef,i32 undef)
  %2 = call <vscale x 4 x i16> @llvm.experimental.vp.expand.nxv4i16(<vscale x 4 x i16> undef,<vscale x 4 x i1> undef,i32 undef)
  %3 = call <vscale x 8 x i8> @llvm.experimental.vp.expand.nxv8i8(<vscale x 8 x i8> undef,<vscale x 8 x i1> undef,i32 undef)
  %4 = call <vscale x 2 x i64> @llvm.experimental.vp.expand.nxv2i64(<vscale x 2 x i64> undef,<vscale x 2 x i1> undef,i32 undef)
  %5 = call <vscale x 4 x i32> @llvm.experimental.vp.expand.nxv4i32(<vscale x 4 x i32> undef,<vscale x 4 x i1> undef,i32 undef)
  %6 = call <vscale x 8 x i16> @llvm.experimental.vp.expand.nxv8i16(<vscale x 8 x i16> undef,<vscale x 8 x i1> undef,i32 undef)
  %7 = call <vscale x 16 x i8> @llvm.experimental.vp.expand.nxv16i8(<vscale x 16 x i8> undef,<vscale x 16 x i1> undef,i32 undef)
  %8 = call <vscale x 4 x i64> @llvm.experimental.vp.expand.nxv4i64(<vscale x 4 x i64> undef,<vscale x 4 x i1> undef,i32 undef)
  %9 = call <vscale x 8 x i32> @llvm.experimental.vp.expand.nxv8i32(<vscale x 8 x i32> undef,<vscale x 8 x i1> undef,i32 undef)
  %10 = call <vscale x 16 x i16> @llvm.experimental.vp.expand.nxv16i16(<vscale x 16 x i16> undef,<vscale x 16 x i1> undef,i32 undef)
  %11 = call <vscale x 32 x i8> @llvm.experimental.vp.expand.nxv32i8(<vscale x 32 x i8> undef,<vscale x 32 x i1> undef,i32 undef)
  %12 = call <vscale x 8 x i64> @llvm.experimental.vp.expand.nxv8i64(<vscale x 8 x i64> undef,<vscale x 8 x i1> undef,i32 undef)
  %13 = call <vscale x 16 x i32> @llvm.experimental.vp.expand.nxv16i32(<vscale x 16 x i32> undef,<vscale x 16 x i1> undef,i32 undef)
  %14 = call <vscale x 32 x i16> @llvm.experimental.vp.expand.nxv32i16(<vscale x 32 x i16> undef,<vscale x 32 x i1> undef,i32 undef)
  %15 = call <vscale x 64 x i8> @llvm.experimental.vp.expand.nxv64i8(<vscale x 64 x i8> undef,<vscale x 64 x i1> undef,i32 undef)
  ret void
}

declare <vscale x 1 x i64> @llvm.experimental.vp.compress.nxv1i64(<vscale x 1 x i64>,<vscale x 1 x i1>,i32)
declare <vscale x 2 x i32> @llvm.experimental.vp.compress.nxv2i32(<vscale x 2 x i32>,<vscale x 2 x i1>,i32)
declare <vscale x 4 x i16> @llvm.experimental.vp.compress.nxv4i16(<vscale x 4 x i16>,<vscale x 4 x i1>,i32)
declare <vscale x 8 x i8> @llvm.experimental.vp.compress.nxv8i8(<vscale x 8 x i8>,<vscale x 8 x i1>,i32)
declare <vscale x 2 x i64> @llvm.experimental.vp.compress.nxv2i64(<vscale x 2 x i64>,<vscale x 2 x i1>,i32)
declare <vscale x 4 x i32> @llvm.experimental.vp.compress.nxv4i32(<vscale x 4 x i32>,<vscale x 4 x i1>,i32)
declare <vscale x 8 x i16> @llvm.experimental.vp.compress.nxv8i16(<vscale x 8 x i16>,<vscale x 8 x i1>,i32)
declare <vscale x 16 x i8> @llvm.experimental.vp.compress.nxv16i8(<vscale x 16 x i8>,<vscale x 16 x i1>,i32)
declare <vscale x 4 x i64> @llvm.experimental.vp.compress.nxv4i64(<vscale x 4 x i64>,<vscale x 4 x i1>,i32)
declare <vscale x 8 x i32> @llvm.experimental.vp.compress.nxv8i32(<vscale x 8 x i32>,<vscale x 8 x i1>,i32)
declare <vscale x 16 x i16> @llvm.experimental.vp.compress.nxv16i16(<vscale x 16 x i16>,<vscale x 16 x i1>,i32)
declare <vscale x 32 x i8> @llvm.experimental.vp.compress.nxv32i8(<vscale x 32 x i8>,<vscale x 32 x i1>,i32)
declare <vscale x 8 x i64> @llvm.experimental.vp.compress.nxv8i64(<vscale x 8 x i64>,<vscale x 8 x i1>,i32)
declare <vscale x 16 x i32> @llvm.experimental.vp.compress.nxv16i32(<vscale x 16 x i32>,<vscale x 16 x i1>,i32)
declare <vscale x 32 x i16> @llvm.experimental.vp.compress.nxv32i16(<vscale x 32 x i16>,<vscale x 32 x i1>,i32)
declare <vscale x 64 x i8> @llvm.experimental.vp.compress.nxv64i8(<vscale x 64 x i8>,<vscale x 64 x i1>,i32)
declare <vscale x 128 x i8> @llvm.experimental.vp.compress.nxv128i8(<vscale x 128 x i8>,<vscale x 128 x i1>,i32)

declare <vscale x 1 x i64> @llvm.experimental.vp.expand.nxv1i64(<vscale x 1 x i64>,<vscale x 1 x i1>,i32)
declare <vscale x 2 x i32> @llvm.experimental.vp.expand.nxv2i32(<vscale x 2 x i32>,<vscale x 2 x i1>,i32)
declare <vscale x 4 x i16> @llvm.experimental.vp.expand.nxv4i16(<vscale x 4 x i16>,<vscale x 4 x i1>,i32)
declare <vscale x 8 x i8> @llvm.experimental.vp.expand.nxv8i8(<vscale x 8 x i8>,<vscale x 8 x i1>,i32)
declare <vscale x 2 x i64> @llvm.experimental.vp.expand.nxv2i64(<vscale x 2 x i64>,<vscale x 2 x i1>,i32)
declare <vscale x 4 x i32> @llvm.experimental.vp.expand.nxv4i32(<vscale x 4 x i32>,<vscale x 4 x i1>,i32)
declare <vscale x 8 x i16> @llvm.experimental.vp.expand.nxv8i16(<vscale x 8 x i16>,<vscale x 8 x i1>,i32)
declare <vscale x 16 x i8> @llvm.experimental.vp.expand.nxv16i8(<vscale x 16 x i8>,<vscale x 16 x i1>,i32)
declare <vscale x 4 x i64> @llvm.experimental.vp.expand.nxv4i64(<vscale x 4 x i64>,<vscale x 4 x i1>,i32)
declare <vscale x 8 x i32> @llvm.experimental.vp.expand.nxv8i32(<vscale x 8 x i32>,<vscale x 8 x i1>,i32)
declare <vscale x 16 x i16> @llvm.experimental.vp.expand.nxv16i16(<vscale x 16 x i16>,<vscale x 16 x i1>,i32)
declare <vscale x 32 x i8> @llvm.experimental.vp.expand.nxv32i8(<vscale x 32 x i8>,<vscale x 32 x i1>,i32)
declare <vscale x 8 x i64> @llvm.experimental.vp.expand.nxv8i64(<vscale x 8 x i64>,<vscale x 8 x i1>,i32)
declare <vscale x 16 x i32> @llvm.experimental.vp.expand.nxv16i32(<vscale x 16 x i32>,<vscale x 16 x i1>,i32)
declare <vscale x 32 x i16> @llvm.experimental.vp.expand.nxv32i16(<vscale x 32 x i16>,<vscale x 32 x i1>,i32)
declare <vscale x 64 x i8> @llvm.experimental.vp.expand.nxv64i8(<vscale x 64 x i8>,<vscale x 64 x i1>,i32)
