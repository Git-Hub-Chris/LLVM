; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -passes=vector-widen -mtriple aarch64-linux-gnu -mattr=+sme2 -S 2>&1 | FileCheck %s

define void @fptrunc(ptr %ptr, ptr %ptr1) {
; CHECK-LABEL: @fptrunc(
; CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP2:%.*]] = shl nuw nsw i64 [[TMP1]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP4:%.*]] = shl nuw nsw i64 [[TMP3]], 2
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 4 x float>, ptr [[PTR:%.*]], align 4
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds float, ptr [[PTR]], i64 [[TMP2]]
; CHECK-NEXT:    [[WIDE_LOAD9:%.*]] = load <vscale x 4 x float>, ptr [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = fptrunc <vscale x 4 x float> [[WIDE_LOAD]] to <vscale x 4 x half>
; CHECK-NEXT:    [[EXTR:%.*]] = call <vscale x 1 x half> @llvm.vector.extract.nxv1f16.nxv4f16(<vscale x 4 x half> [[TMP6]], i64 0)
; CHECK-NEXT:    [[EXTEND:%.*]] = fpext <vscale x 1 x half> [[EXTR]] to <vscale x 1 x float>
; CHECK-NEXT:    [[INS:%.*]] = call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv1f32(<vscale x 4 x float> [[WIDE_LOAD9]], <vscale x 1 x float> [[EXTEND]], i64 0)
; CHECK-NEXT:    [[TMP7:%.*]] = fptrunc <vscale x 4 x float> [[INS]] to <vscale x 4 x half>
; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds half, ptr [[PTR1:%.*]], i64 0
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP6]], ptr [[TMP8]], align 2
; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds half, ptr [[TMP8]], i64 [[TMP4]]
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP7]], ptr [[TMP9]], align 2
; CHECK-NEXT:    ret void
;
  %1 = tail call i64 @llvm.vscale.i64()
  %2 = shl nuw nsw i64 %1, 2
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 2
  %wide.load = load <vscale x 4 x float>, ptr %ptr, align 4
  %5 = getelementptr inbounds float, ptr %ptr, i64 %2
  %wide.load9 = load <vscale x 4 x float>, ptr %5, align 4
  %6 = fptrunc <vscale x 4 x float> %wide.load to <vscale x 4 x half>
  %extr = call <vscale x 1 x half> @llvm.vector.extract.nxv1f16.nxv4f16(<vscale x 4 x half> %6, i64 0)
  %extend = fpext <vscale x 1 x half> %extr to <vscale x 1 x float>
  %ins = call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv1f32(<vscale x 4 x float> %wide.load9, <vscale x 1 x float> %extend, i64 0)
  %7 = fptrunc <vscale x 4 x float> %ins to <vscale x 4 x half>
  %8 = getelementptr inbounds half, ptr %ptr1, i64 0
  store <vscale x 4 x half> %6, ptr %8, align 2
  %9 = getelementptr inbounds half, ptr %8, i64 %4
  store <vscale x 4 x half> %7, ptr %9, align 2
  ret void
}

declare i64 @llvm.vscale.i64()
declare <vscale x 1 x half> @llvm.vector.extract.nxv1f16.nxv4f16(<vscale x 4 x half>, i64 immarg)
declare <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv1f32(<vscale x 4 x float>, <vscale x 1 x float>, i64 immarg)
