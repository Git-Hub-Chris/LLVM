; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 3
; RUN: opt -passes=vector-widen -mtriple aarch64-linux-gnu -mattr=+sme2 -S %s 2>&1 | FileCheck %s

define void @add(ptr %ptr, ptr %ptr1) {
; CHECK-LABEL: define void @add(
; CHECK-SAME: ptr [[PTR:%.*]], ptr [[PTR1:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[TMP0:%.*]] = load <vscale x 4 x i32>, ptr [[PTR]], align 16
; CHECK-NEXT:    [[INCDEC_PTR:%.*]] = getelementptr inbounds <vscale x 4 x i32>, ptr [[PTR]], i64 1
; CHECK-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i32>, ptr [[INCDEC_PTR]], align 16
; CHECK-NEXT:    [[INCDEC_PTR1:%.*]] = getelementptr inbounds <vscale x 4 x i32>, ptr [[PTR]], i64 2
; CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 4 x i32>, ptr [[INCDEC_PTR1]], align 16
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> undef, <vscale x 4 x i32> [[TMP0]], i64 0)
; CHECK-NEXT:    [[TMP4:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> [[TMP3]], <vscale x 4 x i32> [[TMP1]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> undef, <vscale x 4 x i32> [[TMP2]], i64 0)
; CHECK-NEXT:    [[TMP6:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> [[TMP5]], <vscale x 4 x i32> [[TMP2]], i64 4)
; CHECK-NEXT:    [[TMP7:%.*]] = add <vscale x 8 x i32> [[TMP4]], [[TMP6]]
; CHECK-NEXT:    [[TMP8:%.*]] = call <vscale x 4 x i32> @llvm.vector.extract.nxv4i32.nxv8i32(<vscale x 8 x i32> [[TMP7]], i64 4)
; CHECK-NEXT:    [[TMP9:%.*]] = call <vscale x 4 x i32> @llvm.vector.extract.nxv4i32.nxv8i32(<vscale x 8 x i32> [[TMP7]], i64 0)
; CHECK-NEXT:    store <vscale x 4 x i32> [[TMP9]], ptr [[PTR1]], align 16
; CHECK-NEXT:    [[INCDEC_PTR3:%.*]] = getelementptr inbounds <vscale x 4 x i32>, ptr [[PTR1]], i64 1
; CHECK-NEXT:    store <vscale x 4 x i32> [[TMP8]], ptr [[INCDEC_PTR3]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %0 = load <vscale x 4 x i32>, ptr %ptr, align 16
  %incdec.ptr = getelementptr inbounds <vscale x 4 x i32>, ptr %ptr, i64 1
  %1 = load <vscale x 4 x i32>, ptr %incdec.ptr, align 16
  %incdec.ptr1 = getelementptr inbounds <vscale x 4 x i32>, ptr %ptr, i64 2
  %2 = load <vscale x 4 x i32>, ptr %incdec.ptr1, align 16
  %add = add <vscale x 4 x i32> %0, %2
  %add4 = add <vscale x 4 x i32> %1, %2
  store <vscale x 4 x i32> %add, ptr %ptr1, align 16
  %incdec.ptr3 = getelementptr inbounds <vscale x 4 x i32>, ptr %ptr1, i64 1
  store <vscale x 4 x i32> %add4, ptr %incdec.ptr3, align 16
  ret void
}

define void @add_ir_flags(ptr %ptr, ptr %ptr1) {
; CHECK-LABEL: define void @add_ir_flags(
; CHECK-SAME: ptr [[PTR:%.*]], ptr [[PTR1:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[TMP0:%.*]] = load <vscale x 4 x i32>, ptr [[PTR]], align 16
; CHECK-NEXT:    [[INCDEC_PTR:%.*]] = getelementptr inbounds <vscale x 4 x i32>, ptr [[PTR]], i64 1
; CHECK-NEXT:    [[TMP1:%.*]] = load <vscale x 4 x i32>, ptr [[INCDEC_PTR]], align 16
; CHECK-NEXT:    [[INCDEC_PTR1:%.*]] = getelementptr inbounds <vscale x 4 x i32>, ptr [[PTR]], i64 2
; CHECK-NEXT:    [[TMP2:%.*]] = load <vscale x 4 x i32>, ptr [[INCDEC_PTR1]], align 16
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> undef, <vscale x 4 x i32> [[TMP0]], i64 0)
; CHECK-NEXT:    [[TMP4:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> [[TMP3]], <vscale x 4 x i32> [[TMP1]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> undef, <vscale x 4 x i32> [[TMP2]], i64 0)
; CHECK-NEXT:    [[TMP6:%.*]] = call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv4i32(<vscale x 8 x i32> [[TMP5]], <vscale x 4 x i32> [[TMP2]], i64 4)
; CHECK-NEXT:    [[TMP7:%.*]] = add nuw <vscale x 8 x i32> [[TMP4]], [[TMP6]]
; CHECK-NEXT:    [[TMP8:%.*]] = call <vscale x 4 x i32> @llvm.vector.extract.nxv4i32.nxv8i32(<vscale x 8 x i32> [[TMP7]], i64 4)
; CHECK-NEXT:    [[TMP9:%.*]] = call <vscale x 4 x i32> @llvm.vector.extract.nxv4i32.nxv8i32(<vscale x 8 x i32> [[TMP7]], i64 0)
; CHECK-NEXT:    store <vscale x 4 x i32> [[TMP9]], ptr [[PTR1]], align 16
; CHECK-NEXT:    [[INCDEC_PTR3:%.*]] = getelementptr inbounds <vscale x 4 x i32>, ptr [[PTR1]], i64 1
; CHECK-NEXT:    store <vscale x 4 x i32> [[TMP8]], ptr [[INCDEC_PTR3]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %0 = load <vscale x 4 x i32>, ptr %ptr, align 16
  %incdec.ptr = getelementptr inbounds <vscale x 4 x i32>, ptr %ptr, i64 1
  %1 = load <vscale x 4 x i32>, ptr %incdec.ptr, align 16
  %incdec.ptr1 = getelementptr inbounds <vscale x 4 x i32>, ptr %ptr, i64 2
  %2 = load <vscale x 4 x i32>, ptr %incdec.ptr1, align 16
  %add = add nuw nsw <vscale x 4 x i32> %0, %2
  %add4 = add nuw <vscale x 4 x i32> %1, %2
  store <vscale x 4 x i32> %add, ptr %ptr1, align 16
  %incdec.ptr3 = getelementptr inbounds <vscale x 4 x i32>, ptr %ptr1, i64 1
  store <vscale x 4 x i32> %add4, ptr %incdec.ptr3, align 16
  ret void
}
