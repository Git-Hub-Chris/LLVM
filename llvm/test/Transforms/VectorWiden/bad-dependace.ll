; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -passes=vector-widen -mtriple aarch64-linux-gnu -mattr=+sme2 -S 2>&1 | FileCheck %s

define void @fptrunc(<vscale x 4 x float> %a, <vscale x 4 x float> %b, ptr %ptr) {
; CHECK-LABEL: @fptrunc(
; CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP2:%.*]] = shl nuw nsw i64 [[TMP1]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = fptrunc <vscale x 4 x float> [[A:%.*]] to <vscale x 4 x half>
; CHECK-NEXT:    [[EXTR:%.*]] = call <vscale x 1 x half> @llvm.vector.extract.nxv1f16.nxv4f16(<vscale x 4 x half> [[TMP3]], i64 0)
; CHECK-NEXT:    [[EXTEND:%.*]] = fpext <vscale x 1 x half> [[EXTR]] to <vscale x 1 x float>
; CHECK-NEXT:    [[INS:%.*]] = call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv1f32(<vscale x 4 x float> [[B:%.*]], <vscale x 1 x float> [[EXTEND]], i64 0)
; CHECK-NEXT:    [[TMP4:%.*]] = fptrunc <vscale x 4 x float> [[INS]] to <vscale x 4 x half>
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds half, ptr [[PTR:%.*]], i64 0
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP3]], ptr [[TMP5]], align 2
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds half, ptr [[TMP5]], i64 [[TMP2]]
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP4]], ptr [[TMP6]], align 2
; CHECK-NEXT:    ret void
;
  %1 = tail call i64 @llvm.vscale.i64()
  %2 = shl nuw nsw i64 %1, 2
  %3 = fptrunc <vscale x 4 x float> %a to <vscale x 4 x half>
  %extr = call <vscale x 1 x half> @llvm.vector.extract.nxv1f16.nxv4f16(<vscale x 4 x half> %3, i64 0)
  %extend = fpext <vscale x 1 x half> %extr to <vscale x 1 x float>
  %ins = call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv1f32(<vscale x 4 x float> %b, <vscale x 1 x float> %extend, i64 0)
  %4 = fptrunc <vscale x 4 x float> %ins to <vscale x 4 x half>
  %5 = getelementptr inbounds half, ptr %ptr, i64 0
  store <vscale x 4 x half> %3, ptr %5, align 2
  %6 = getelementptr inbounds half, ptr %5, i64 %2
  store <vscale x 4 x half> %4, ptr %6, align 2
  ret void
}

define void @add(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b, ptr %ptr) {
; CHECK-LABEL: @add(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ADD:%.*]] = add <vscale x 4 x i32> [[A:%.*]], [[B:%.*]]
; CHECK-NEXT:    [[ADD4:%.*]] = add <vscale x 4 x i32> [[ADD]], [[B]]
; CHECK-NEXT:    store <vscale x 4 x i32> [[ADD]], ptr [[PTR:%.*]], align 16
; CHECK-NEXT:    [[INCDEC_PTR3:%.*]] = getelementptr inbounds <vscale x 4 x i32>, ptr [[PTR]], i64 1
; CHECK-NEXT:    store <vscale x 4 x i32> [[ADD4]], ptr [[INCDEC_PTR3]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %add = add <vscale x 4 x i32> %a, %b
  %add4 = add <vscale x 4 x i32> %add, %b
  store <vscale x 4 x i32> %add, ptr %ptr, align 16
  %incdec.ptr3 = getelementptr inbounds <vscale x 4 x i32>, ptr %ptr, i64 1
  store <vscale x 4 x i32> %add4, ptr %incdec.ptr3, align 16
  ret void
}

declare i64 @llvm.vscale.i64()
declare <vscale x 1 x half> @llvm.vector.extract.nxv1f16.nxv4f16(<vscale x 4 x half>, i64 immarg)
declare <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv1f32(<vscale x 4 x float>, <vscale x 1 x float>, i64 immarg)
