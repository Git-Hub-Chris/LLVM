; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -passes=vector-widen -mtriple aarch64-linux-gnu -mattr=+sme2 -S 2>&1 | FileCheck %s

define void @fptrunc(<vscale x 4 x float> %a, <vscale x 4 x float> %b, ptr %ptr) {
; CHECK-LABEL: @fptrunc(
; CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP2:%.*]] = shl nuw nsw i64 [[TMP1]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> undef, <vscale x 4 x float> [[B:%.*]], i64 0)
; CHECK-NEXT:    [[TMP4:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> [[TMP3]], <vscale x 4 x float> [[A:%.*]], i64 4)
; CHECK-NEXT:    [[TMP5:%.*]] = fptrunc <vscale x 8 x float> [[TMP4]] to <vscale x 8 x half>
; CHECK-NEXT:    [[TMP6:%.*]] = call <vscale x 4 x half> @llvm.vector.extract.nxv4f16.nxv8f16(<vscale x 8 x half> [[TMP5]], i64 0)
; CHECK-NEXT:    [[TMP7:%.*]] = call <vscale x 4 x half> @llvm.vector.extract.nxv4f16.nxv8f16(<vscale x 8 x half> [[TMP5]], i64 4)
; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds half, ptr [[PTR:%.*]], i64 0
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP7]], ptr [[TMP8]], align 2
; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr inbounds half, ptr [[TMP8]], i64 [[TMP2]]
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP6]], ptr [[TMP9]], align 2
; CHECK-NEXT:    ret void
;
  %1 = tail call i64 @llvm.vscale.i64()
  %2 = shl nuw nsw i64 %1, 2
  %3 = fptrunc <vscale x 4 x float> %a to <vscale x 4 x half>
  %4 = fptrunc <vscale x 4 x float> %b to <vscale x 4 x half>
  %5 = getelementptr inbounds half, ptr %ptr, i64 0
  store <vscale x 4 x half> %3, ptr %5, align 2
  %6 = getelementptr inbounds half, ptr %5, i64 %2
  store <vscale x 4 x half> %4, ptr %6, align 2
  ret void
}

declare i64 @llvm.vscale.i64()
