; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -passes=vector-widen -mtriple aarch64-linux-gnu -mattr=+sme2 -S 2>&1 | FileCheck %s

define void @fptrunc(ptr %ptr, ptr %ptr1) {
; CHECK-LABEL: @fptrunc(
; CHECK-NEXT:    [[TMP1:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP2:%.*]] = shl nuw nsw i64 [[TMP1]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP4:%.*]] = shl nuw nsw i64 [[TMP3]], 2
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 4 x float>, ptr [[PTR:%.*]], align 4
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds float, ptr [[PTR]], i64 [[TMP2]]
; CHECK-NEXT:    [[WIDE_LOAD9:%.*]] = load <vscale x 4 x float>, ptr [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> undef, <vscale x 4 x float> [[WIDE_LOAD9]], i64 0)
; CHECK-NEXT:    [[TMP7:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> [[TMP6]], <vscale x 4 x float> [[WIDE_LOAD]], i64 4)
; CHECK-NEXT:    [[TMP8:%.*]] = fptrunc <vscale x 8 x float> [[TMP7]] to <vscale x 8 x half>
; CHECK-NEXT:    [[TMP9:%.*]] = call <vscale x 4 x half> @llvm.vector.extract.nxv4f16.nxv8f16(<vscale x 8 x half> [[TMP8]], i64 0)
; CHECK-NEXT:    [[TMP10:%.*]] = call <vscale x 4 x half> @llvm.vector.extract.nxv4f16.nxv8f16(<vscale x 8 x half> [[TMP8]], i64 4)
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds half, ptr [[PTR1:%.*]], i64 0
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP10]], ptr [[TMP11]], align 2
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds half, ptr [[TMP11]], i64 [[TMP4]]
; CHECK-NEXT:    store <vscale x 4 x half> [[TMP9]], ptr [[TMP12]], align 2
; CHECK-NEXT:    ret void
;
  %1 = tail call i64 @llvm.vscale.i64()
  %2 = shl nuw nsw i64 %1, 2
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 2
  %wide.load = load <vscale x 4 x float>, ptr %ptr, align 4
  %5 = getelementptr inbounds float, ptr %ptr, i64 %2
  %wide.load9 = load <vscale x 4 x float>, ptr %5, align 4
  %6 = fptrunc <vscale x 4 x float> %wide.load to <vscale x 4 x half>
  %7 = fptrunc <vscale x 4 x float> %wide.load9 to <vscale x 4 x half>
  %8 = getelementptr inbounds half, ptr %ptr1, i64 0
  store <vscale x 4 x half> %6, ptr %8, align 2
  %9 = getelementptr inbounds half, ptr %8, i64 %4
  store <vscale x 4 x half> %7, ptr %9, align 2
  ret void
}

declare i64 @llvm.vscale.i64()
