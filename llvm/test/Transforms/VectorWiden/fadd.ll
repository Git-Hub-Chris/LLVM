; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 3
; RUN: opt -passes=vector-widen -vw-override-target-consider-to-widen=1 -S %s 2>&1 | FileCheck %s

define void @add(<vscale x 4 x float> %a, <vscale x 4 x float> %b, <vscale x 4 x float> %c, ptr %ptr) {
; CHECK-LABEL: define void @add(
; CHECK-SAME: <vscale x 4 x float> [[A:%.*]], <vscale x 4 x float> [[B:%.*]], <vscale x 4 x float> [[C:%.*]], ptr [[PTR:%.*]]) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> undef, <vscale x 4 x float> [[A]], i64 0)
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> [[TMP0]], <vscale x 4 x float> [[B]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> undef, <vscale x 4 x float> [[C]], i64 0)
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> [[TMP2]], <vscale x 4 x float> [[C]], i64 4)
; CHECK-NEXT:    [[TMP4:%.*]] = fadd <vscale x 8 x float> [[TMP1]], [[TMP3]]
; CHECK-NEXT:    [[TMP5:%.*]] = call <vscale x 4 x float> @llvm.vector.extract.nxv4f32.nxv8f32(<vscale x 8 x float> [[TMP4]], i64 4)
; CHECK-NEXT:    [[TMP6:%.*]] = call <vscale x 4 x float> @llvm.vector.extract.nxv4f32.nxv8f32(<vscale x 8 x float> [[TMP4]], i64 0)
; CHECK-NEXT:    store <vscale x 4 x float> [[TMP6]], ptr [[PTR]], align 16
; CHECK-NEXT:    [[INCDEC_PTR3:%.*]] = getelementptr inbounds <vscale x 4 x float>, ptr [[PTR]], i64 1
; CHECK-NEXT:    store <vscale x 4 x float> [[TMP5]], ptr [[INCDEC_PTR3]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %add = fadd <vscale x 4 x float> %a, %c
  %add4 = fadd <vscale x 4 x float> %b, %c
  store <vscale x 4 x float> %add, ptr %ptr, align 16
  %incdec.ptr3 = getelementptr inbounds <vscale x 4 x float>, ptr %ptr, i64 1
  store <vscale x 4 x float> %add4, ptr %incdec.ptr3, align 16
  ret void
}

define void @add_ir_flags(<vscale x 4 x float> %a, <vscale x 4 x float> %b, <vscale x 4 x float> %c, ptr %ptr) {
; CHECK-LABEL: define void @add_ir_flags(
; CHECK-SAME: <vscale x 4 x float> [[A:%.*]], <vscale x 4 x float> [[B:%.*]], <vscale x 4 x float> [[C:%.*]], ptr [[PTR:%.*]]) {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> undef, <vscale x 4 x float> [[A]], i64 0)
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> [[TMP0]], <vscale x 4 x float> [[B]], i64 4)
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> undef, <vscale x 4 x float> [[C]], i64 0)
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv4f32(<vscale x 8 x float> [[TMP2]], <vscale x 4 x float> [[C]], i64 4)
; CHECK-NEXT:    [[TMP4:%.*]] = fadd nnan ninf <vscale x 8 x float> [[TMP1]], [[TMP3]]
; CHECK-NEXT:    [[TMP5:%.*]] = call <vscale x 4 x float> @llvm.vector.extract.nxv4f32.nxv8f32(<vscale x 8 x float> [[TMP4]], i64 4)
; CHECK-NEXT:    [[TMP6:%.*]] = call <vscale x 4 x float> @llvm.vector.extract.nxv4f32.nxv8f32(<vscale x 8 x float> [[TMP4]], i64 0)
; CHECK-NEXT:    store <vscale x 4 x float> [[TMP6]], ptr [[PTR]], align 16
; CHECK-NEXT:    [[INCDEC_PTR3:%.*]] = getelementptr inbounds <vscale x 4 x float>, ptr [[PTR]], i64 1
; CHECK-NEXT:    store <vscale x 4 x float> [[TMP5]], ptr [[INCDEC_PTR3]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %add = fadd fast nnan <vscale x 4 x float> %a, %c
  %add4 = fadd nnan ninf <vscale x 4 x float> %b, %c
  store <vscale x 4 x float> %add, ptr %ptr, align 16
  %incdec.ptr3 = getelementptr inbounds <vscale x 4 x float>, ptr %ptr, i64 1
  store <vscale x 4 x float> %add4, ptr %incdec.ptr3, align 16
  ret void
}
