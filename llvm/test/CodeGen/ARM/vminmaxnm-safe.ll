; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple armv8 -mattr=+neon,+fp-armv8 | FileCheck %s

; vectors

define <4 x float> @vmaxnmq(ptr %A, ptr %B) nounwind {
; CHECK-LABEL: vmaxnmq:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vld1.64 {d16, d17}, [r1]
; CHECK-NEXT:    vld1.64 {d18, d19}, [r0]
; CHECK-NEXT:    vmaxnm.f32 q8, q9, q8
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    vmov r2, r3, d17
; CHECK-NEXT:    bx lr
  %tmp1 = load <4 x float>, ptr %A
  %tmp2 = load <4 x float>, ptr %B
  %tmp3 = call <4 x float> @llvm.arm.neon.vmaxnm.v4f32(<4 x float> %tmp1, <4 x float> %tmp2)
  ret <4 x float> %tmp3
}

define <2 x float> @vmaxnmd(ptr %A, ptr %B) nounwind {
; CHECK-LABEL: vmaxnmd:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr d16, [r1]
; CHECK-NEXT:    vldr d17, [r0]
; CHECK-NEXT:    vmaxnm.f32 d16, d17, d16
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
  %tmp1 = load <2 x float>, ptr %A
  %tmp2 = load <2 x float>, ptr %B
  %tmp3 = call <2 x float> @llvm.arm.neon.vmaxnm.v2f32(<2 x float> %tmp1, <2 x float> %tmp2)
  ret <2 x float> %tmp3
}

define <4 x float> @vminnmq(ptr %A, ptr %B) nounwind {
; CHECK-LABEL: vminnmq:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vld1.64 {d16, d17}, [r1]
; CHECK-NEXT:    vld1.64 {d18, d19}, [r0]
; CHECK-NEXT:    vminnm.f32 q8, q9, q8
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    vmov r2, r3, d17
; CHECK-NEXT:    bx lr
  %tmp1 = load <4 x float>, ptr %A
  %tmp2 = load <4 x float>, ptr %B
  %tmp3 = call <4 x float> @llvm.arm.neon.vminnm.v4f32(<4 x float> %tmp1, <4 x float> %tmp2)
  ret <4 x float> %tmp3
}

define <2 x float> @vminnmd(ptr %A, ptr %B) nounwind {
; CHECK-LABEL: vminnmd:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr d16, [r1]
; CHECK-NEXT:    vldr d17, [r0]
; CHECK-NEXT:    vminnm.f32 d16, d17, d16
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
  %tmp1 = load <2 x float>, ptr %A
  %tmp2 = load <2 x float>, ptr %B
  %tmp3 = call <2 x float> @llvm.arm.neon.vminnm.v2f32(<2 x float> %tmp1, <2 x float> %tmp2)
  ret <2 x float> %tmp3
}

; scalars

define float @fp_armv8_vminnm_o(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vminnm_o:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r1
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp olt float %a, %b
  %cond = select i1 %cmp, float %a, float %b
  ret float %cond
}

define double @fp_armv8_vminnm_ole(double %a, double %b) {
; CHECK-LABEL: fp_armv8_vminnm_ole:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov d16, r0, r1
; CHECK-NEXT:    vmov d17, r2, r3
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f64 d16, d16, d17
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
  %cmp = fcmp ole double %a, %b
  %cond = select i1 %cmp, double %a, double %b
  ret double %cond
}

define float @fp_armv8_vminnm_o_rev(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vminnm_o_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vmov s2, r1
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ogt float %a, %b
  %cond = select i1 %cmp, float %b, float %a
  ret float %cond
}

define double @fp_armv8_vminnm_oge_rev(double %a, double %b) {
; CHECK-LABEL: fp_armv8_vminnm_oge_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov d16, r2, r3
; CHECK-NEXT:    vmov d17, r0, r1
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f64 d16, d16, d17
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
  %cmp = fcmp oge double %a, %b
  %cond = select i1 %cmp, double %b, double %a
  ret double %cond
}

define float @fp_armv8_vminnm_u(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vminnm_u:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vmov s2, r1
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ult float %a, %b
  %cond = select i1 %cmp, float %a, float %b
  ret float %cond
}

define float @fp_armv8_vminnm_ule(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vminnm_ule:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vmov s2, r1
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ule float %a, %b
  %cond = select i1 %cmp, float %a, float %b
  ret float %cond
}

define float @fp_armv8_vminnm_u_rev(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vminnm_u_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r1
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ugt float %a, %b
  %cond = select i1 %cmp, float %b, float %a
  ret float %cond
}

define double @fp_armv8_vminnm_uge_rev(double %a, double %b) {
; CHECK-LABEL: fp_armv8_vminnm_uge_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov d16, r0, r1
; CHECK-NEXT:    vmov d17, r2, r3
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f64 d16, d16, d17
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
  %cmp = fcmp uge double %a, %b
  %cond = select i1 %cmp, double %b, double %a
  ret double %cond
}

define float @fp_armv8_vmaxnm_o(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_o:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r1
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ogt float %a, %b
  %cond = select i1 %cmp, float %a, float %b
  ret float %cond
}

define float @fp_armv8_vmaxnm_oge(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_oge:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r1
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp oge float %a, %b
  %cond = select i1 %cmp, float %a, float %b
  ret float %cond
}

define float @fp_armv8_vmaxnm_o_rev(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_o_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vmov s2, r1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp olt float %a, %b
  %cond = select i1 %cmp, float %b, float %a
  ret float %cond
}

define float @fp_armv8_vmaxnm_ole_rev(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_ole_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vmov s2, r1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ole float %a, %b
  %cond = select i1 %cmp, float %b, float %a
  ret float %cond
}

define float @fp_armv8_vmaxnm_u(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_u:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vmov s2, r1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ugt float %a, %b
  %cond = select i1 %cmp, float %a, float %b
  ret float %cond
}

define float @fp_armv8_vmaxnm_uge(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_uge:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vmov s2, r1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp uge float %a, %b
  %cond = select i1 %cmp, float %a, float %b
  ret float %cond
}

define float @fp_armv8_vmaxnm_u_rev(float %a, float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_u_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r1
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
  %cmp = fcmp ult float %a, %b
  %cond = select i1 %cmp, float %b, float %a
  ret float %cond
}

define double @fp_armv8_vmaxnm_ule_rev(double %a, double %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_ule_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov d16, r2, r3
; CHECK-NEXT:    vmov d17, r0, r1
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f64 d16, d17, d16
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
  %cmp = fcmp ule double %a, %b
  %cond = select i1 %cmp, double %b, double %a
  ret double %cond
}

; known non-NaNs

define float @fp_armv8_vminnm_NNNo(float %a) {
; CHECK-LABEL: fp_armv8_vminnm_NNNo:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov.f32 s0, #1.200000e+01
; CHECK-NEXT:    vldr s4, .LCPI20_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s0, s4
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI20_0:
; CHECK-NEXT:    .long 0x42080000 @ float 34
  %cmp1 = fcmp olt float %a, 12.
  %cond1 = select i1 %cmp1, float %a, float 12.
  %cmp2 = fcmp olt float 34., %cond1
  %cond2 = select i1 %cmp2, float 34., float %cond1
  ret float %cond2
}

define double @fp_armv8_vminnm_NNNole(double %a) {
; CHECK-LABEL: fp_armv8_vminnm_NNNole:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov d16, r0, r1
; CHECK-NEXT:    vldr d17, .LCPI21_0
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f64 d16, d16, d17
; CHECK-NEXT:    vldr d17, .LCPI21_1
; CHECK-NEXT:    vcmp.f64 d16, d17
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f64 d16, d17, d16
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 3
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI21_0:
; CHECK-NEXT:    .long 0 @ double 34
; CHECK-NEXT:    .long 1078001664
; CHECK-NEXT:  .LCPI21_1:
; CHECK-NEXT:    .long 0 @ double 56
; CHECK-NEXT:    .long 1078722560
  %cmp1 = fcmp ole double %a, 34.
  %cond1 = select i1 %cmp1, double %a, double 34.
  %cmp2 = fcmp ole double 56., %cond1
  %cond2 = select i1 %cmp2, double 56., double %cond1
  ret double %cond2
}

define float @fp_armv8_vminnm_NNNo_rev(float %a) {
; CHECK-LABEL: fp_armv8_vminnm_NNNo_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI22_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI22_1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s0, s2
; CHECK-NEXT:    vcmp.f32 s4, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s0, s4
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI22_0:
; CHECK-NEXT:    .long 0x42600000 @ float 56
; CHECK-NEXT:  .LCPI22_1:
; CHECK-NEXT:    .long 0x429c0000 @ float 78
  %cmp1 = fcmp ogt float %a, 56.
  %cond1 = select i1 %cmp1, float 56., float %a
  %cmp2 = fcmp ogt float 78., %cond1
  %cond2 = select i1 %cmp2, float %cond1, float 78.
  ret float %cond2
}

define double @fp_armv8_vminnm_NNNoge_rev(double %a) {
; CHECK-LABEL: fp_armv8_vminnm_NNNoge_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr d16, .LCPI23_0
; CHECK-NEXT:    vmov d17, r0, r1
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f64 d16, d16, d17
; CHECK-NEXT:    vldr d17, .LCPI23_1
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f64 d16, d16, d17
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 3
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI23_0:
; CHECK-NEXT:    .long 0 @ double 78
; CHECK-NEXT:    .long 1079214080
; CHECK-NEXT:  .LCPI23_1:
; CHECK-NEXT:    .long 0 @ double 90
; CHECK-NEXT:    .long 1079410688
  %cmp1 = fcmp oge double %a, 78.
  %cond1 = select i1 %cmp1, double 78., double %a
  %cmp2 = fcmp oge double 90., %cond1
  %cond2 = select i1 %cmp2, double %cond1, double 90.
  ret double %cond2
}

define float @fp_armv8_vminnm_NNNu(float %b) {
; CHECK-LABEL: fp_armv8_vminnm_NNNu:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov.f32 s0, #1.200000e+01
; CHECK-NEXT:    vldr s4, .LCPI24_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s0, s4
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI24_0:
; CHECK-NEXT:    .long 0x42080000 @ float 34
  %cmp1 = fcmp ult float 12., %b
  %cond1 = select i1 %cmp1, float 12., float %b
  %cmp2 = fcmp ult float %cond1, 34.
  %cond2 = select i1 %cmp2, float %cond1, float 34.
  ret float %cond2
}

define float @fp_armv8_vminnm_NNNule(float %b) {
; CHECK-LABEL: fp_armv8_vminnm_NNNule:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI25_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI25_1
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s0, s4
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI25_0:
; CHECK-NEXT:    .long 0x42080000 @ float 34
; CHECK-NEXT:  .LCPI25_1:
; CHECK-NEXT:    .long 0x42600000 @ float 56
  %cmp1 = fcmp ule float 34., %b
  %cond1 = select i1 %cmp1, float 34., float %b
  %cmp2 = fcmp ule float %cond1, 56.
  %cond2 = select i1 %cmp2, float %cond1, float 56.
  ret float %cond2
}

define float @fp_armv8_vminnm_NNNu_rev(float %b) {
; CHECK-LABEL: fp_armv8_vminnm_NNNu_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI26_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI26_1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s2
; CHECK-NEXT:    vcmp.f32 s4, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s4
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI26_0:
; CHECK-NEXT:    .long 0x42600000 @ float 56
; CHECK-NEXT:  .LCPI26_1:
; CHECK-NEXT:    .long 0x429c0000 @ float 78
  %cmp1 = fcmp ugt float 56., %b
  %cond1 = select i1 %cmp1, float %b, float 56.
  %cmp2 = fcmp ugt float %cond1, 78.
  %cond2 = select i1 %cmp2, float 78., float %cond1
  ret float %cond2
}

define double @fp_armv8_vminnm_NNNuge_rev(double %b) {
; CHECK-LABEL: fp_armv8_vminnm_NNNuge_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr d16, .LCPI27_0
; CHECK-NEXT:    vmov d17, r0, r1
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f64 d16, d16, d17
; CHECK-NEXT:    vldr d17, .LCPI27_1
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f64 d16, d16, d17
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 3
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI27_0:
; CHECK-NEXT:    .long 0 @ double 78
; CHECK-NEXT:    .long 1079214080
; CHECK-NEXT:  .LCPI27_1:
; CHECK-NEXT:    .long 0 @ double 90
; CHECK-NEXT:    .long 1079410688
  %cmp1 = fcmp uge double 78., %b
  %cond1 = select i1 %cmp1, double %b, double 78.
  %cmp2 = fcmp uge double %cond1, 90.
  %cond2 = select i1 %cmp2, double 90., double %cond1
  ret double %cond2
}

define float @fp_armv8_vmaxnm_NNNo(float %a) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNo:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov.f32 s0, #1.200000e+01
; CHECK-NEXT:    vldr s4, .LCPI28_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s4, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI28_0:
; CHECK-NEXT:    .long 0x42080000 @ float 34
  %cmp1 = fcmp ogt float %a, 12.
  %cond1 = select i1 %cmp1, float %a, float 12.
  %cmp2 = fcmp ogt float 34., %cond1
  %cond2 = select i1 %cmp2, float 34., float %cond1
  ret float %cond2
}

define float @fp_armv8_vmaxnm_NNNoge(float %a) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNoge:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI29_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI29_1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s4, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI29_0:
; CHECK-NEXT:    .long 0x42080000 @ float 34
; CHECK-NEXT:  .LCPI29_1:
; CHECK-NEXT:    .long 0x42600000 @ float 56
  %cmp1 = fcmp oge float %a, 34.
  %cond1 = select i1 %cmp1, float %a, float 34.
  %cmp2 = fcmp oge float 56., %cond1
  %cond2 = select i1 %cmp2, float 56., float %cond1
  ret float %cond2
}

define float @fp_armv8_vmaxnm_NNNo_rev(float %a) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNo_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI30_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI30_1
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s0, s2
; CHECK-NEXT:    vcmp.f32 s0, s4
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s0, s4
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI30_0:
; CHECK-NEXT:    .long 0x42600000 @ float 56
; CHECK-NEXT:  .LCPI30_1:
; CHECK-NEXT:    .long 0x429c0000 @ float 78
  %cmp1 = fcmp olt float %a, 56.
  %cond1 = select i1 %cmp1, float 56., float %a
  %cmp2 = fcmp olt float 78., %cond1
  %cond2 = select i1 %cmp2, float %cond1, float 78.
  ret float %cond2
}

define float @fp_armv8_vmaxnm_NNNole_rev(float %a) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNole_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI31_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI31_1
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s2
; CHECK-NEXT:    vcmp.f32 s0, s4
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s4
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI31_0:
; CHECK-NEXT:    .long 0x429c0000 @ float 78
; CHECK-NEXT:  .LCPI31_1:
; CHECK-NEXT:    .long 0x42b40000 @ float 90
  %cmp1 = fcmp ole float %a, 78.
  %cond1 = select i1 %cmp1, float 78., float %a
  %cmp2 = fcmp ole float 90., %cond1
  %cond2 = select i1 %cmp2, float %cond1, float 90.
  ret float %cond2
}

define float @fp_armv8_vmaxnm_NNNu(float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNu:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov.f32 s0, #1.200000e+01
; CHECK-NEXT:    vldr s4, .LCPI32_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s4, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI32_0:
; CHECK-NEXT:    .long 0x42080000 @ float 34
  %cmp1 = fcmp ugt float 12., %b
  %cond1 = select i1 %cmp1, float 12., float %b
  %cmp2 = fcmp ugt float %cond1, 34.
  %cond2 = select i1 %cmp2, float %cond1, float 34.
  ret float %cond2
}

define float @fp_armv8_vmaxnm_NNNuge(float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNuge:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI33_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI33_1
; CHECK-NEXT:    vcmp.f32 s2, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s4, s0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI33_0:
; CHECK-NEXT:    .long 0x42080000 @ float 34
; CHECK-NEXT:  .LCPI33_1:
; CHECK-NEXT:    .long 0x42600000 @ float 56
  %cmp1 = fcmp uge float 34., %b
  %cond1 = select i1 %cmp1, float 34., float %b
  %cmp2 = fcmp uge float %cond1, 56.
  %cond2 = select i1 %cmp2, float %cond1, float 56.
  ret float %cond2
}

define float @fp_armv8_vmaxnm_NNNu_rev(float %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNu_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI34_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s4, .LCPI34_1
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s2
; CHECK-NEXT:    vcmp.f32 s0, s4
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s4
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI34_0:
; CHECK-NEXT:    .long 0x42600000 @ float 56
; CHECK-NEXT:  .LCPI34_1:
; CHECK-NEXT:    .long 0x429c0000 @ float 78
  %cmp1 = fcmp ult float 56., %b
  %cond1 = select i1 %cmp1, float %b, float 56.
  %cmp2 = fcmp ult float %cond1, 78.
  %cond2 = select i1 %cmp2, float 78., float %cond1
  ret float %cond2
}

define double @fp_armv8_vmaxnm_NNNule_rev( double %b) {
; CHECK-LABEL: fp_armv8_vmaxnm_NNNule_rev:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov d16, r0, r1
; CHECK-NEXT:    vldr d17, .LCPI35_0
; CHECK-NEXT:    vcmp.f64 d17, d16
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f64 d16, d17, d16
; CHECK-NEXT:    vldr d17, .LCPI35_1
; CHECK-NEXT:    vcmp.f64 d16, d17
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f64 d16, d16, d17
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 3
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI35_0:
; CHECK-NEXT:    .long 0 @ double 78
; CHECK-NEXT:    .long 1079214080
; CHECK-NEXT:  .LCPI35_1:
; CHECK-NEXT:    .long 0 @ double 90
; CHECK-NEXT:    .long 1079410688
  %cmp1 = fcmp ule double 78., %b
  %cond1 = select i1 %cmp1, double %b, double 78.
  %cmp2 = fcmp ule double %cond1, 90.
  %cond2 = select i1 %cmp2, double 90., double %cond1
  ret double %cond2
}

define float @fp_armv8_vminmaxnm_0(float %a) {
; CHECK-LABEL: fp_armv8_vminmaxnm_0:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vldr s0, .LCPI36_0
; CHECK-NEXT:    vcmp.f32 s2, #0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vmov.f32 s4, s0
; CHECK-NEXT:    vmovlt.f32 s4, s2
; CHECK-NEXT:    vcmp.f32 s4, #0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s0, s4, s0
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI36_0:
; CHECK-NEXT:    .long 0x00000000 @ float 0
  %cmp1 = fcmp ult float %a, 0.
  %cond1 = select i1 %cmp1, float %a, float 0.
  %cmp2 = fcmp ogt float %cond1, 0.
  %cond2 = select i1 %cmp2, float %cond1, float 0.
  ret float %cond2
}

define float @fp_armv8_vminmaxnm_neg0(float %a) {
; CHECK-LABEL: fp_armv8_vminmaxnm_neg0:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI37_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s2, s2, s0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s2
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI37_0:
; CHECK-NEXT:    .long 0x80000000 @ float -0
  %cmp1 = fcmp olt float %a, -0.
  %cond1 = select i1 %cmp1, float %a, float -0.
  %cmp2 = fcmp ugt float %cond1, -0.
  %cond2 = select i1 %cmp2, float %cond1, float -0.
  ret float %cond2
}

define float @fp_armv8_vminmaxnm_e_0(float %a) {
; CHECK-LABEL: fp_armv8_vminmaxnm_e_0:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vmov s0, r0
; CHECK-NEXT:    vldr s2, .LCPI38_0
; CHECK-NEXT:    vcmp.f32 s0, #0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s2, s0
; CHECK-NEXT:    vcmp.f32 s0, #0
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vmovle.f32 s0, s2
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI38_0:
; CHECK-NEXT:    .long 0x00000000 @ float 0
  %cmp1 = fcmp nsz ole float 0., %a
  %cond1 = select i1 %cmp1, float 0., float %a
  %cmp2 = fcmp nsz uge float 0., %cond1
  %cond2 = select i1 %cmp2, float 0., float %cond1
  ret float %cond2
}

define float @fp_armv8_vminmaxnm_e_neg0(float %a) {
; CHECK-LABEL: fp_armv8_vminmaxnm_e_neg0:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vldr s0, .LCPI39_0
; CHECK-NEXT:    vmov s2, r0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselgt.f32 s2, s2, s0
; CHECK-NEXT:    vcmp.f32 s0, s2
; CHECK-NEXT:    vmrs APSR_nzcv, fpscr
; CHECK-NEXT:    vselge.f32 s0, s0, s2
; CHECK-NEXT:    vmov r0, s0
; CHECK-NEXT:    bx lr
; CHECK-NEXT:    .p2align 2
; CHECK-NEXT:  @ %bb.1:
; CHECK-NEXT:  .LCPI39_0:
; CHECK-NEXT:    .long 0x80000000 @ float -0
  %cmp1 = fcmp nsz ule float -0., %a
  %cond1 = select i1 %cmp1, float -0., float %a
  %cmp2 = fcmp nsz oge float -0., %cond1
  %cond2 = select i1 %cmp2, float -0., float %cond1
  ret float %cond2
}

declare <4 x float> @llvm.arm.neon.vminnm.v4f32(<4 x float>, <4 x float>) nounwind readnone
declare <2 x float> @llvm.arm.neon.vminnm.v2f32(<2 x float>, <2 x float>) nounwind readnone
declare <4 x float> @llvm.arm.neon.vmaxnm.v4f32(<4 x float>, <4 x float>) nounwind readnone
declare <2 x float> @llvm.arm.neon.vmaxnm.v2f32(<2 x float>, <2 x float>) nounwind readnone
