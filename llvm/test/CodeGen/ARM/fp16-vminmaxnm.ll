; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=arm-eabi -mattr=+fullfp16 -enable-unsafe-fp-math -enable-no-nans-fp-math | FileCheck %s --check-prefix=ARMEABI
; RUN: llc < %s -mtriple thumbv7a -mattr=+fullfp16 -enable-unsafe-fp-math -enable-no-nans-fp-math | FileCheck %s --check-prefix=THUMBV7A

; TODO: we can't pass half-precision arguments as "half" types yet. We do
; that for the time being by passing "float %f.coerce" and the necessary
; bitconverts/truncates. In these tests we pass i16 and use 1 bitconvert, which
; is the shortest way to get a half type. But when we can pass half types, we
; want to use that here.

define half @fp16_vminnm_o(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_o:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r1
; ARMEABI-NEXT:    vmov.f16 s2, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vminnm_o:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r1
; THUMBV7A-NEXT:    vmov.f16 s2, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast olt half %0, %1
  %cond = select i1 %cmp, half %0, half %1
  ret half %cond
}

define half @fp16_vminnm_o_rev(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_o_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, r1
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vminnm_o_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, r1
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ogt half %0, %1
  %cond = select i1 %cmp, half %1, half %0
  ret half %cond
}

define half @fp16_vminnm_u(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_u:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r1
; ARMEABI-NEXT:    vmov.f16 s2, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vminnm_u:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r1
; THUMBV7A-NEXT:    vmov.f16 s2, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ult half %0, %1
  %cond = select i1 %cmp, half %0, half %1
  ret half %cond
}

define half @fp16_vminnm_ule(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_ule:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r1
; ARMEABI-NEXT:    vmov.f16 s2, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vminnm_ule:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r1
; THUMBV7A-NEXT:    vmov.f16 s2, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ule half %0, %1
  %cond = select i1 %cmp, half %0, half %1
  ret half %cond
}

define half @fp16_vminnm_u_rev(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_u_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, r1
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vminnm_u_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, r1
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ugt half %0, %1
  %cond = select i1 %cmp, half %1, half %0
  ret half %cond
}

define half @fp16_vmaxnm_o(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_o:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r1
; ARMEABI-NEXT:    vmov.f16 s2, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vmaxnm_o:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r1
; THUMBV7A-NEXT:    vmov.f16 s2, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ogt half %0, %1
  %cond = select i1 %cmp, half %0, half %1
  ret half %cond
}

define half @fp16_vmaxnm_oge(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_oge:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r1
; ARMEABI-NEXT:    vmov.f16 s2, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vmaxnm_oge:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r1
; THUMBV7A-NEXT:    vmov.f16 s2, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast oge half %0, %1
  %cond = select i1 %cmp, half %0, half %1
  ret half %cond
}

define half @fp16_vmaxnm_o_rev(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_o_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, r1
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vmaxnm_o_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, r1
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast olt half %0, %1
  %cond = select i1 %cmp, half %1, half %0
  ret half %cond
}

define half @fp16_vmaxnm_ole_rev(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_ole_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, r1
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vmaxnm_ole_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, r1
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ole half %0, %1
  %cond = select i1 %cmp, half %1, half %0
  ret half %cond
}

define half @fp16_vmaxnm_u(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_u:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r1
; ARMEABI-NEXT:    vmov.f16 s2, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vmaxnm_u:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r1
; THUMBV7A-NEXT:    vmov.f16 s2, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ugt half %0, %1
  %cond = select i1 %cmp, half %0, half %1
  ret half %cond
}

define half @fp16_vmaxnm_uge(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_uge:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r1
; ARMEABI-NEXT:    vmov.f16 s2, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vmaxnm_uge:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r1
; THUMBV7A-NEXT:    vmov.f16 s2, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast uge half %0, %1
  %cond = select i1 %cmp, half %0, half %1
  ret half %cond
}

define half @fp16_vmaxnm_u_rev(i16 signext %a, i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_u_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, r1
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
;
; THUMBV7A-LABEL: fp16_vmaxnm_u_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, r1
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
entry:
  %0 = bitcast i16 %a to half
  %1 = bitcast i16 %b to half
  %cmp = fcmp fast ult half %0, %1
  %cond = select i1 %cmp, half %1, half %0
  ret half %cond
}

; known non-NaNs

define half @fp16_vminnm_NNNo(i16 signext %a) {
; ARMEABI-LABEL: fp16_vminnm_NNNo:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, #1.200000e+01
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vldr.16 s2, .LCPI12_0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI12_0:
; ARMEABI-NEXT:    .short 0x5040 @ half 34
;
; THUMBV7A-LABEL: fp16_vminnm_NNNo:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, #1.200000e+01
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI12_0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI12_0:
; THUMBV7A-NEXT:    .short 0x5040 @ half 34
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast olt half %0, 12.
  %cond1 = select i1 %cmp1, half %0, half 12.
  %cmp2 = fcmp fast olt half 34., %cond1
  %cond2 = select i1 %cmp2, half 34., half %cond1
  ret half %cond2
}

define half @fp16_vminnm_NNNo_rev(i16 signext %a) {
; ARMEABI-LABEL: fp16_vminnm_NNNo_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI13_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI13_1
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI13_0:
; ARMEABI-NEXT:    .short 0x5300 @ half 56
; ARMEABI-NEXT:  .LCPI13_1:
; ARMEABI-NEXT:    .short 0x54e0 @ half 78
;
; THUMBV7A-LABEL: fp16_vminnm_NNNo_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI13_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI13_1
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI13_0:
; THUMBV7A-NEXT:    .short 0x5300 @ half 56
; THUMBV7A-NEXT:  .LCPI13_1:
; THUMBV7A-NEXT:    .short 0x54e0 @ half 78
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast ogt half %0, 56.
  %cond1 = select i1 %cmp1, half 56., half %0
  %cmp2 = fcmp fast ogt half 78., %cond1
  %cond2 = select i1 %cmp2, half %cond1, half 78.
  ret half %cond2
}

define half @fp16_vminnm_NNNu(i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_NNNu:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, #1.200000e+01
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI14_0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI14_0:
; ARMEABI-NEXT:    .short 0x5040 @ half 34
;
; THUMBV7A-LABEL: fp16_vminnm_NNNu:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, #1.200000e+01
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI14_0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI14_0:
; THUMBV7A-NEXT:    .short 0x5040 @ half 34
entry:
  %0 = bitcast i16 %b to half
  %cmp1 = fcmp fast ult half 12., %0
  %cond1 = select i1 %cmp1, half 12., half %0
  %cmp2 = fcmp fast ult half %cond1, 34.
  %cond2 = select i1 %cmp2, half %cond1, half 34.
  ret half %cond2
}

define half @fp16_vminnm_NNNule(i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_NNNule:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI15_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI15_1
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI15_0:
; ARMEABI-NEXT:    .short 0x5040 @ half 34
; ARMEABI-NEXT:  .LCPI15_1:
; ARMEABI-NEXT:    .short 0x5300 @ half 56
;
; THUMBV7A-LABEL: fp16_vminnm_NNNule:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI15_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI15_1
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI15_0:
; THUMBV7A-NEXT:    .short 0x5040 @ half 34
; THUMBV7A-NEXT:  .LCPI15_1:
; THUMBV7A-NEXT:    .short 0x5300 @ half 56
entry:
  %0 = bitcast i16 %b to half
  %cmp1 = fcmp fast ule half 34., %0
  %cond1 = select i1 %cmp1, half 34., half %0
  %cmp2 = fcmp fast ule half %cond1, 56.
  %cond2 = select i1 %cmp2, half %cond1, half 56.
  ret half %cond2
}

define half @fp16_vminnm_NNNu_rev(i16 signext %b) {
; ARMEABI-LABEL: fp16_vminnm_NNNu_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI16_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vldr.16 s2, .LCPI16_1
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI16_0:
; ARMEABI-NEXT:    .short 0x5300 @ half 56
; ARMEABI-NEXT:  .LCPI16_1:
; ARMEABI-NEXT:    .short 0x54e0 @ half 78
;
; THUMBV7A-LABEL: fp16_vminnm_NNNu_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI16_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI16_1
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI16_0:
; THUMBV7A-NEXT:    .short 0x5300 @ half 56
; THUMBV7A-NEXT:  .LCPI16_1:
; THUMBV7A-NEXT:    .short 0x54e0 @ half 78
entry:
  %0 = bitcast i16 %b to half
  %cmp1 = fcmp fast ugt half 56., %0
  %cond1 = select i1 %cmp1, half %0, half 56.
  %cmp2 = fcmp fast ugt half %cond1, 78.
  %cond2 = select i1 %cmp2, half 78., half %cond1
  ret half %cond2
}

define half @fp16_vmaxnm_NNNo(i16 signext %a) {
; ARMEABI-LABEL: fp16_vmaxnm_NNNo:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, #1.200000e+01
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vldr.16 s2, .LCPI17_0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI17_0:
; ARMEABI-NEXT:    .short 0x5040 @ half 34
;
; THUMBV7A-LABEL: fp16_vmaxnm_NNNo:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, #1.200000e+01
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI17_0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI17_0:
; THUMBV7A-NEXT:    .short 0x5040 @ half 34
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast ogt half %0, 12.
  %cond1 = select i1 %cmp1, half %0, half 12.
  %cmp2 = fcmp fast ogt half 34., %cond1
  %cond2 = select i1 %cmp2, half 34., half %cond1
  ret half %cond2
}

define half @fp16_vmaxnm_NNNoge(i16 signext %a) {
; ARMEABI-LABEL: fp16_vmaxnm_NNNoge:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI18_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s0, s2
; ARMEABI-NEXT:    vldr.16 s2, .LCPI18_1
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI18_0:
; ARMEABI-NEXT:    .short 0x5040 @ half 34
; ARMEABI-NEXT:  .LCPI18_1:
; ARMEABI-NEXT:    .short 0x5300 @ half 56
;
; THUMBV7A-LABEL: fp16_vmaxnm_NNNoge:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI18_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s0, s2
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI18_1
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI18_0:
; THUMBV7A-NEXT:    .short 0x5040 @ half 34
; THUMBV7A-NEXT:  .LCPI18_1:
; THUMBV7A-NEXT:    .short 0x5300 @ half 56
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast oge half %0, 34.
  %cond1 = select i1 %cmp1, half %0, half 34.
  %cmp2 = fcmp fast oge half 56., %cond1
  %cond2 = select i1 %cmp2, half 56., half %cond1
  ret half %cond2
}

define half @fp16_vmaxnm_NNNo_rev(i16 signext %a) {
; ARMEABI-LABEL: fp16_vmaxnm_NNNo_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI19_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI19_1
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI19_0:
; ARMEABI-NEXT:    .short 0x5300 @ half 56
; ARMEABI-NEXT:  .LCPI19_1:
; ARMEABI-NEXT:    .short 0x54e0 @ half 78
;
; THUMBV7A-LABEL: fp16_vmaxnm_NNNo_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI19_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI19_1
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI19_0:
; THUMBV7A-NEXT:    .short 0x5300 @ half 56
; THUMBV7A-NEXT:  .LCPI19_1:
; THUMBV7A-NEXT:    .short 0x54e0 @ half 78
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast olt half %0, 56.
  %cond1 = select i1 %cmp1, half 56., half %0
  %cmp2 = fcmp fast olt half 78., %cond1
  %cond2 = select i1 %cmp2, half %cond1, half 78.
  ret half %cond2
}

define half @fp16_vmaxnm_NNNole_rev(i16 signext %a) {
; ARMEABI-LABEL: fp16_vmaxnm_NNNole_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI20_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI20_1
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI20_0:
; ARMEABI-NEXT:    .short 0x54e0 @ half 78
; ARMEABI-NEXT:  .LCPI20_1:
; ARMEABI-NEXT:    .short 0x55a0 @ half 90
;
; THUMBV7A-LABEL: fp16_vmaxnm_NNNole_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI20_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI20_1
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI20_0:
; THUMBV7A-NEXT:    .short 0x54e0 @ half 78
; THUMBV7A-NEXT:  .LCPI20_1:
; THUMBV7A-NEXT:    .short 0x55a0 @ half 90
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast ole half %0, 78.
  %cond1 = select i1 %cmp1, half 78., half %0
  %cmp2 = fcmp fast ole half 90., %cond1
  %cond2 = select i1 %cmp2, half %cond1, half 90.
  ret half %cond2
}

define half @fp16_vmaxnm_NNNu(i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_NNNu:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vmov.f16 s2, #1.200000e+01
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI21_0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI21_0:
; ARMEABI-NEXT:    .short 0x5040 @ half 34
;
; THUMBV7A-LABEL: fp16_vmaxnm_NNNu:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vmov.f16 s2, #1.200000e+01
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI21_0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI21_0:
; THUMBV7A-NEXT:    .short 0x5040 @ half 34
entry:
  %0 = bitcast i16 %b to half
  %cmp1 = fcmp fast ugt half 12., %0
  %cond1 = select i1 %cmp1, half 12., half %0
  %cmp2 = fcmp fast ugt half %cond1, 34.
  %cond2 = select i1 %cmp2, half %cond1, half 34.
  ret half %cond2
}

define half @fp16_vmaxnm_NNNuge(i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_NNNuge:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI22_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI22_1
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI22_0:
; ARMEABI-NEXT:    .short 0x5040 @ half 34
; ARMEABI-NEXT:  .LCPI22_1:
; ARMEABI-NEXT:    .short 0x5300 @ half 56
;
; THUMBV7A-LABEL: fp16_vmaxnm_NNNuge:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI22_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI22_1
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI22_0:
; THUMBV7A-NEXT:    .short 0x5040 @ half 34
; THUMBV7A-NEXT:  .LCPI22_1:
; THUMBV7A-NEXT:    .short 0x5300 @ half 56
entry:
  %0 = bitcast i16 %b to half
  %cmp1 = fcmp fast uge half 34., %0
  %cond1 = select i1 %cmp1, half 34., half %0
  %cmp2 = fcmp fast uge half %cond1, 56.
  %cond2 = select i1 %cmp2, half %cond1, half 56.
  ret half %cond2
}

define half @fp16_vmaxnm_NNNu_rev(i16 signext %b) {
; ARMEABI-LABEL: fp16_vmaxnm_NNNu_rev:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI23_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vldr.16 s2, .LCPI23_1
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI23_0:
; ARMEABI-NEXT:    .short 0x5300 @ half 56
; ARMEABI-NEXT:  .LCPI23_1:
; ARMEABI-NEXT:    .short 0x54e0 @ half 78
;
; THUMBV7A-LABEL: fp16_vmaxnm_NNNu_rev:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI23_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI23_1
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI23_0:
; THUMBV7A-NEXT:    .short 0x5300 @ half 56
; THUMBV7A-NEXT:  .LCPI23_1:
; THUMBV7A-NEXT:    .short 0x54e0 @ half 78
entry:
  %0 = bitcast i16 %b to half
  %cmp1 = fcmp fast ult half 56., %0
  %cond1 = select i1 %cmp1, half %0, half 56.
  %cmp2 = fcmp fast ult half %cond1, 78.
  %cond2 = select i1 %cmp2, half 78., half %cond1
  ret half %cond2
}

define half @fp16_vminmaxnm_0(i16 signext %a) {
; ARMEABI-LABEL: fp16_vminmaxnm_0:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI24_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vcmp.f16 s0, #0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI24_0:
; ARMEABI-NEXT:    .short 0x0000 @ half 0
;
; THUMBV7A-LABEL: fp16_vminmaxnm_0:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI24_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vcmp.f16 s0, #0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI24_0:
; THUMBV7A-NEXT:    .short 0x0000 @ half 0
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast olt half %0, 0.
  %cond1 = select i1 %cmp1, half %0, half 0.
  %cmp2 = fcmp fast ogt half %cond1, 0.
  %cond2 = select i1 %cmp2, half %cond1, half 0.
  ret half %cond2
}

define half @fp16_vminmaxnm_neg0(i16 signext %a) {
; ARMEABI-LABEL: fp16_vminmaxnm_neg0:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI25_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselgt.f16 s0, s0, s2
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI25_0:
; ARMEABI-NEXT:    .short 0x8000 @ half -0
;
; THUMBV7A-LABEL: fp16_vminmaxnm_neg0:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI25_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselgt.f16 s0, s0, s2
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI25_0:
; THUMBV7A-NEXT:    .short 0x8000 @ half -0
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast olt half %0, -0.
  %cond1 = select i1 %cmp1, half %0, half -0.
  %cmp2 = fcmp fast ugt half %cond1, -0.
  %cond2 = select i1 %cmp2, half %cond1, half -0.
  ret half %cond2
}

define half @fp16_vminmaxnm_e_0(i16 signext %a) {
; ARMEABI-LABEL: fp16_vminmaxnm_e_0:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vldr.16 s2, .LCPI26_0
; ARMEABI-NEXT:    vcmp.f16 s0, #0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI26_0:
; ARMEABI-NEXT:    .short 0x0000 @ half 0
;
; THUMBV7A-LABEL: fp16_vminmaxnm_e_0:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI26_0
; THUMBV7A-NEXT:    vcmp.f16 s0, #0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI26_0:
; THUMBV7A-NEXT:    .short 0x0000 @ half 0
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast ule half 0., %0
  %cond1 = select i1 %cmp1, half 0., half %0
  %cmp2 = fcmp fast uge half 0., %cond1
  %cond2 = select i1 %cmp2, half 0., half %cond1
  ret half %cond2
}

define half @fp16_vminmaxnm_e_neg0(i16 signext %a) {
; ARMEABI-LABEL: fp16_vminmaxnm_e_neg0:
; ARMEABI:       @ %bb.0: @ %entry
; ARMEABI-NEXT:    vldr.16 s2, .LCPI27_0
; ARMEABI-NEXT:    vmov.f16 s0, r0
; ARMEABI-NEXT:    vcmp.f16 s0, s2
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vcmp.f16 s2, s0
; ARMEABI-NEXT:    vmrs APSR_nzcv, fpscr
; ARMEABI-NEXT:    vselge.f16 s0, s2, s0
; ARMEABI-NEXT:    vmov r0, s0
; ARMEABI-NEXT:    mov pc, lr
; ARMEABI-NEXT:    .p2align 1
; ARMEABI-NEXT:  @ %bb.1:
; ARMEABI-NEXT:  .LCPI27_0:
; ARMEABI-NEXT:    .short 0x8000 @ half -0
;
; THUMBV7A-LABEL: fp16_vminmaxnm_e_neg0:
; THUMBV7A:       @ %bb.0: @ %entry
; THUMBV7A-NEXT:    vldr.16 s2, .LCPI27_0
; THUMBV7A-NEXT:    vmov.f16 s0, r0
; THUMBV7A-NEXT:    vcmp.f16 s0, s2
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vcmp.f16 s2, s0
; THUMBV7A-NEXT:    vmrs APSR_nzcv, fpscr
; THUMBV7A-NEXT:    vselge.f16 s0, s2, s0
; THUMBV7A-NEXT:    vmov r0, s0
; THUMBV7A-NEXT:    bx lr
; THUMBV7A-NEXT:    .p2align 1
; THUMBV7A-NEXT:  @ %bb.1:
; THUMBV7A-NEXT:  .LCPI27_0:
; THUMBV7A-NEXT:    .short 0x8000 @ half -0
entry:
  %0 = bitcast i16 %a to half
  %cmp1 = fcmp fast ule half -0., %0
  %cond1 = select i1 %cmp1, half -0., half %0
  %cmp2 = fcmp fast oge half -0., %cond1
  %cond2 = select i1 %cmp2, half -0., half %cond1
  ret half %cond2
}
