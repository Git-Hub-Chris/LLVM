; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: sed 's/iXLen/i32/g' %s | llc -mtriple=riscv32 -mattr=+v,+zvbb -riscv-enable-vl-optimizer=false -verify-machineinstrs | FileCheck %s --check-prefixes=NOVLOPT
; RUN: sed 's/iXLen/i64/g' %s | llc -mtriple=riscv64 -mattr=+v,+zvbb -riscv-enable-vl-optimizer=false -verify-machineinstrs | FileCheck %s --check-prefixes=NOVLOPT
; RUN: sed 's/iXLen/i32/g' %s | llc -mtriple=riscv32 -mattr=+v,+zvbb -riscv-enable-vl-optimizer -verify-machineinstrs | FileCheck %s --check-prefixes=VLOPT
; RUN: sed 's/iXLen/i64/g' %s | llc -mtriple=riscv64 -mattr=+v,+zvbb -riscv-enable-vl-optimizer -verify-machineinstrs | FileCheck %s --check-prefixes=VLOPT

; The purpose of this test is to check the behavior of specific instructions as it relates to the VL optimizer

define <vscale x 4 x i32> @vadd_vi(<vscale x 4 x i32> %a, iXLen %vl) {
; NOVLOPT-LABEL: vadd_vi:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vi v10, v8, 5
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v10, v8
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vadd_vi:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vadd.vi v10, v8, 5
; VLOPT-NEXT:    vadd.vv v8, v10, v8
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %a, i32 5, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %a, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vadd_vv(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vadd_vv:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v8, v10
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v8, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vadd_vv:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vadd.vv v8, v8, v10
; VLOPT-NEXT:    vadd.vv v8, v8, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %b, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vadd_vx(<vscale x 4 x i32> %a, i32 %b, iXLen %vl) {
; NOVLOPT-LABEL: vadd_vx:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a2, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vx v10, v8, a0
; NOVLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v10, v8
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vadd_vx:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; VLOPT-NEXT:    vadd.vx v10, v8, a0
; VLOPT-NEXT:    vadd.vv v8, v10, v8
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %a, i32 %b, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %a, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vmul_vv(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vmul_vv:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vmul.vv v8, v8, v10
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vmul.vv v8, v8, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vmul_vv:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vmul.vv v8, v8, v10
; VLOPT-NEXT:    vmul.vv v8, v8, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vmul.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vmul.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %b, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vmul_vx(<vscale x 4 x i32> %a, i32 %b, iXLen %vl) {
; NOVLOPT-LABEL: vmul_vx:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a2, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vmul.vx v10, v8, a0
; NOVLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; NOVLOPT-NEXT:    vmul.vv v8, v10, v8
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vmul_vx:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; VLOPT-NEXT:    vmul.vx v10, v8, a0
; VLOPT-NEXT:    vmul.vv v8, v10, v8
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vmul.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %a, i32 %b, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vmul.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %a, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vsll_vi(<vscale x 4 x i32> %a, iXLen %vl) {
; NOVLOPT-LABEL: vsll_vi:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vsll.vi v10, v8, 5
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vsll.vv v8, v10, v8
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vsll_vi:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vsll.vi v10, v8, 5
; VLOPT-NEXT:    vsll.vv v8, v10, v8
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vsll.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %a, iXLen 5, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vsll.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %a, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vsext.vf2(<vscale x 4 x i16> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vsext.vf2:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vsext.vf2 v12, v8
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v12, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vsext.vf2:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vsext.vf2 v12, v8
; VLOPT-NEXT:    vadd.vv v8, v12, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vsext.nxv4i32.nxv4i16(<vscale x 4 x i32> poison, <vscale x 4 x i16> %a, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %b, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vsext.vf4(<vscale x 4 x i8> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vsext.vf4:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vsext.vf4 v12, v8
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v12, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vsext.vf4:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vsext.vf4 v12, v8
; VLOPT-NEXT:    vadd.vv v8, v12, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vsext.nxv4i32.nxv4i8(<vscale x 4 x i32> poison, <vscale x 4 x i8> %a, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %b, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i64> @vsext.vf8(<vscale x 4 x i8> %a, <vscale x 4 x i64> %b, iXLen %vl) {
; NOVLOPT-LABEL: vsext.vf8:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; NOVLOPT-NEXT:    vsext.vf8 v16, v8
; NOVLOPT-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v16, v12
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vsext.vf8:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; VLOPT-NEXT:    vsext.vf8 v16, v8
; VLOPT-NEXT:    vadd.vv v8, v16, v12
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i64> @llvm.riscv.vsext.nxv4i32.nxv4i8(<vscale x 4 x i64> poison, <vscale x 4 x i8> %a, iXLen -1)
  %2 = call <vscale x 4 x i64> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, <vscale x 4 x i64> %b, iXLen %vl)
  ret <vscale x 4 x i64> %2
}

define <vscale x 4 x i32> @vzext.vf2(<vscale x 4 x i16> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vzext.vf2:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vzext.vf2 v12, v8
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v12, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vzext.vf2:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vzext.vf2 v12, v8
; VLOPT-NEXT:    vadd.vv v8, v12, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vzext.nxv4i32.nxv4i16(<vscale x 4 x i32> poison, <vscale x 4 x i16> %a, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %b, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vzext.vf4(<vscale x 4 x i8> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vzext.vf4:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vzext.vf4 v12, v8
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v12, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vzext.vf4:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vzext.vf4 v12, v8
; VLOPT-NEXT:    vadd.vv v8, v12, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vzext.nxv4i32.nxv4i8(<vscale x 4 x i32> poison, <vscale x 4 x i8> %a, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %b, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i64> @vzext.vf8(<vscale x 4 x i8> %a, <vscale x 4 x i64> %b, iXLen %vl) {
; NOVLOPT-LABEL: vzext.vf8:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; NOVLOPT-NEXT:    vzext.vf8 v16, v8
; NOVLOPT-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v16, v12
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vzext.vf8:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; VLOPT-NEXT:    vzext.vf8 v16, v8
; VLOPT-NEXT:    vadd.vv v8, v16, v12
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i64> @llvm.riscv.vzext.nxv4i32.nxv4i8(<vscale x 4 x i64> poison, <vscale x 4 x i8> %a, iXLen -1)
  %2 = call <vscale x 4 x i64> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, <vscale x 4 x i64> %b, iXLen %vl)
  ret <vscale x 4 x i64> %2
}

; FIXME: Add a test for vmv.v.i
; FIXME: Add a test for vmv.v.v

define <vscale x 4 x i32> @vmv.v.x(<vscale x 4 x i32> %a, i32 %x, iXLen %vl) {
; NOVLOPT-LABEL: vmv.v.x:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a2, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vmv.v.x v10, a0
; NOVLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v10, v8
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vmv.v.x:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; VLOPT-NEXT:    vmv.v.x v10, a0
; VLOPT-NEXT:    vadd.vv v8, v10, v8
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vmv.v.x.nxv4i32(<vscale x 4 x i32> poison, i32 %x, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %a, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i16> @vnsrl_wi(<vscale x 4 x i32> %a, <vscale x 4 x i16> %b, iXLen %vl) {
; NOVLOPT-LABEL: vnsrl_wi:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e16, m1, ta, ma
; NOVLOPT-NEXT:    vnsrl.wi v11, v8, 5
; NOVLOPT-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v11, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vnsrl_wi:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; VLOPT-NEXT:    vnsrl.wi v11, v8, 5
; VLOPT-NEXT:    vadd.vv v8, v11, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i16> @llvm.riscv.vnsrl.nxv4i16.nxv4i32(<vscale x 4 x i16> poison, <vscale x 4 x i32> %a, iXLen 5, iXLen -1)
  %2 = call <vscale x 4 x i16> @llvm.riscv.vadd.nxv4i16.nxv4i16(<vscale x 4 x i16> poison, <vscale x 4 x i16> %1, <vscale x 4 x i16> %b, iXLen %vl)
  ret <vscale x 4 x i16> %2
}

define <vscale x 4 x i64> @vwadd.vv(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vwadd.vv:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vwadd.vv v12, v8, v10
; NOVLOPT-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v12, v12
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vwadd.vv:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vwadd.vv v12, v8, v10
; VLOPT-NEXT:    vsetvli zero, zero, e64, m4, ta, ma
; VLOPT-NEXT:    vadd.vv v8, v12, v12
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i64> @llvm.riscv.vwadd.nxv4i64.nxv4i32.nxv4i32(<vscale x 4 x i64> poison, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen -1)
  %2 = call <vscale x 4 x i64> @llvm.riscv.vadd.nxv4i64.nxv4i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, <vscale x 4 x i64> %1, iXLen %vl)
  ret <vscale x 4 x i64> %2
}

define <vscale x 4 x i64> @vwaddu.vv(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vwaddu.vv:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; NOVLOPT-NEXT:    vwaddu.vv v12, v8, v10
; NOVLOPT-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v12, v12
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vwaddu.vv:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; VLOPT-NEXT:    vwaddu.vv v12, v8, v10
; VLOPT-NEXT:    vsetvli zero, zero, e64, m4, ta, ma
; VLOPT-NEXT:    vadd.vv v8, v12, v12
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i64> @llvm.riscv.vwaddu.nxv4i64.nxv4i32.nxv4i32(<vscale x 4 x i64> poison, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b, iXLen -1)
  %2 = call <vscale x 4 x i64> @llvm.riscv.vadd.nxv4i64.nxv4i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> %1, <vscale x 4 x i64> %1, iXLen %vl)
  ret <vscale x 4 x i64> %2
}

define <vscale x 4 x i32> @vwmacc_vx(<vscale x 4 x i16> %a, i16 %b, iXLen %vl) {
; NOVLOPT-LABEL: vwmacc_vx:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a2, zero, e16, m1, ta, ma
; NOVLOPT-NEXT:    vwmacc.vx v10, a0, v8
; NOVLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v10, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vwmacc_vx:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a1, e16, m1, ta, ma
; VLOPT-NEXT:    vwmacc.vx v10, a0, v8
; VLOPT-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; VLOPT-NEXT:    vadd.vv v8, v10, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vwmacc.nxv4i32.i16(<vscale x 4 x i32> poison, i16 %b, <vscale x 4 x i16> %a, iXLen -1, iXLen 0)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %1, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vwmaccu_vx(<vscale x 4 x i16> %a, i16 %b, iXLen %vl) {
; NOVLOPT-LABEL: vwmaccu_vx:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a2, zero, e16, m1, ta, ma
; NOVLOPT-NEXT:    vwmaccu.vx v10, a0, v8
; NOVLOPT-NEXT:    vsetvli zero, a1, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v10, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vwmaccu_vx:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a1, e16, m1, ta, ma
; VLOPT-NEXT:    vwmaccu.vx v10, a0, v8
; VLOPT-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; VLOPT-NEXT:    vadd.vv v8, v10, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vwmaccu.nxv4i32.i16(<vscale x 4 x i32> poison, i16 %b, <vscale x 4 x i16> %a, iXLen -1, iXLen 0)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %1, iXLen %vl)
  ret <vscale x 4 x i32> %2
}

define <vscale x 4 x i32> @vwsll.vi(<vscale x 4 x i16> %a, <vscale x 4 x i32> %b, iXLen %vl) {
; NOVLOPT-LABEL: vwsll.vi:
; NOVLOPT:       # %bb.0:
; NOVLOPT-NEXT:    vsetvli a1, zero, e16, m1, ta, ma
; NOVLOPT-NEXT:    vwsll.vi v12, v8, 1
; NOVLOPT-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; NOVLOPT-NEXT:    vadd.vv v8, v12, v10
; NOVLOPT-NEXT:    ret
;
; VLOPT-LABEL: vwsll.vi:
; VLOPT:       # %bb.0:
; VLOPT-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; VLOPT-NEXT:    vwsll.vi v12, v8, 1
; VLOPT-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; VLOPT-NEXT:    vadd.vv v8, v12, v10
; VLOPT-NEXT:    ret
  %1 = call <vscale x 4 x i32> @llvm.riscv.vwsll.nxv4i32.nxv4i16(<vscale x 4 x i32> poison, <vscale x 4 x i16> %a,iXLen 1, iXLen -1)
  %2 = call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32(<vscale x 4 x i32> poison, <vscale x 4 x i32> %1, <vscale x 4 x i32> %b, iXLen %vl)
  ret <vscale x 4 x i32> %2
}
