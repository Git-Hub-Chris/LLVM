; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -target-abi=ilp32d -mattr=+v,+zvfh,+zvfbfmin -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFH,RV32
; RUN: llc -mtriple=riscv64 -target-abi=lp64d -mattr=+v,+zvfh,+zvfbfmin -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFH,RV64
; RUN: llc -mtriple=riscv32 -target-abi=ilp32d -mattr=+v,+zvfhmin,+zvfbfmin -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFHMIN,RV32
; RUN: llc -mtriple=riscv64 -target-abi=lp64d -mattr=+v,+zvfhmin,+zvfbfmin -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFHMIN,RV64


define void @fadd_v8bf16(ptr %x, ptr %y) {
; RV32-LABEL: fadd_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fadd_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = fadd <8 x bfloat> %a, %b
  store <8 x bfloat> %c, ptr %x
  ret void
}

define void @fadd_v6bf16(ptr %x, ptr %y) {
; RV32-LABEL: fadd_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fadd_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = fadd <6 x bfloat> %a, %b
  store <6 x bfloat> %c, ptr %x
  ret void
}

define void @fadd_v8f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fadd_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfadd.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fadd_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = fadd <8 x half> %a, %b
  store <8 x half> %c, ptr %x
  ret void
}

define void @fadd_v6f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fadd_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfadd.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fadd_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = fadd <6 x half> %a, %b
  store <6 x half> %c, ptr %x
  ret void
}

define void @fadd_v4f32(ptr %x, ptr %y) {
; CHECK-LABEL: fadd_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfadd.vv v8, v8, v9
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fadd <4 x float> %a, %b
  store <4 x float> %c, ptr %x
  ret void
}

define void @fadd_v2f64(ptr %x, ptr %y) {
; CHECK-LABEL: fadd_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfadd.vv v8, v8, v9
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = fadd <2 x double> %a, %b
  store <2 x double> %c, ptr %x
  ret void
}

define void @fsub_v8bf16(ptr %x, ptr %y) {
; RV32-LABEL: fsub_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fsub_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = fsub <8 x bfloat> %a, %b
  store <8 x bfloat> %c, ptr %x
  ret void
}

define void @fsub_v6bf16(ptr %x, ptr %y) {
; RV32-LABEL: fsub_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fsub_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = fsub <6 x bfloat> %a, %b
  store <6 x bfloat> %c, ptr %x
  ret void
}

define void @fsub_v8f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fsub_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfsub.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fsub_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = fsub <8 x half> %a, %b
  store <8 x half> %c, ptr %x
  ret void
}

define void @fsub_v6f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fsub_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfsub.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fsub_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = fsub <6 x half> %a, %b
  store <6 x half> %c, ptr %x
  ret void
}

define void @fsub_v4f32(ptr %x, ptr %y) {
; CHECK-LABEL: fsub_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfsub.vv v8, v8, v9
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fsub <4 x float> %a, %b
  store <4 x float> %c, ptr %x
  ret void
}

define void @fsub_v2f64(ptr %x, ptr %y) {
; CHECK-LABEL: fsub_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfsub.vv v8, v8, v9
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = fsub <2 x double> %a, %b
  store <2 x double> %c, ptr %x
  ret void
}

define void @fmul_v8bf16(ptr %x, ptr %y) {
; RV32-LABEL: fmul_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmul_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = fmul <8 x bfloat> %a, %b
  store <8 x bfloat> %c, ptr %x
  ret void
}

define void @fmul_v6bf16(ptr %x, ptr %y) {
; RV32-LABEL: fmul_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmul_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = fmul <6 x bfloat> %a, %b
  store <6 x bfloat> %c, ptr %x
  ret void
}

define void @fmul_v8f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fmul_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmul.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmul_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = fmul <8 x half> %a, %b
  store <8 x half> %c, ptr %x
  ret void
}

define void @fmul_v6f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fmul_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmul.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmul_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = fmul <6 x half> %a, %b
  store <6 x half> %c, ptr %x
  ret void
}

define void @fmul_v4f32(ptr %x, ptr %y) {
; CHECK-LABEL: fmul_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfmul.vv v8, v8, v9
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fmul <4 x float> %a, %b
  store <4 x float> %c, ptr %x
  ret void
}

define void @fmul_v2f64(ptr %x, ptr %y) {
; CHECK-LABEL: fmul_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfmul.vv v8, v8, v9
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = fmul <2 x double> %a, %b
  store <2 x double> %c, ptr %x
  ret void
}

define void @fdiv_v8bf16(ptr %x, ptr %y) {
; RV32-LABEL: fdiv_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fdiv_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = fdiv <8 x bfloat> %a, %b
  store <8 x bfloat> %c, ptr %x
  ret void
}

define void @fdiv_v6bf16(ptr %x, ptr %y) {
; RV32-LABEL: fdiv_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fdiv_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = fdiv <6 x bfloat> %a, %b
  store <6 x bfloat> %c, ptr %x
  ret void
}

define void @fdiv_v8f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fdiv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfdiv.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fdiv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfdiv.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = fdiv <8 x half> %a, %b
  store <8 x half> %c, ptr %x
  ret void
}

define void @fdiv_v6f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fdiv_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfdiv.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fdiv_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfdiv.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = fdiv <6 x half> %a, %b
  store <6 x half> %c, ptr %x
  ret void
}

define void @fdiv_v4f32(ptr %x, ptr %y) {
; CHECK-LABEL: fdiv_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfdiv.vv v8, v8, v9
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fdiv <4 x float> %a, %b
  store <4 x float> %c, ptr %x
  ret void
}

define void @fdiv_v2f64(ptr %x, ptr %y) {
; CHECK-LABEL: fdiv_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfdiv.vv v8, v8, v9
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = fdiv <2 x double> %a, %b
  store <2 x double> %c, ptr %x
  ret void
}

define void @fneg_v8bf16(ptr %x) {
; RV32-LABEL: fneg_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -32
; RV32-NEXT:    .cfi_def_cfa_offset 32
; RV32-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 16(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x20, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 32 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s1, 524288
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 32
; RV32-NEXT:    ret
;
; RV64-LABEL: fneg_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = fneg <8 x bfloat> %a
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @fneg_v6bf16(ptr %x) {
; RV32-LABEL: fneg_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -32
; RV32-NEXT:    .cfi_def_cfa_offset 32
; RV32-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 16(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x20, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 32 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s1, 524288
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 32
; RV32-NEXT:    ret
;
; RV64-LABEL: fneg_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = fneg <6 x bfloat> %a
  store <6 x bfloat> %b, ptr %x
  ret void
}

define void @fneg_v8f16(ptr %x) {
; ZVFH-LABEL: fneg_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfneg.v v8, v8
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fneg_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = fneg <8 x half> %a
  store <8 x half> %b, ptr %x
  ret void
}

define void @fneg_v6f16(ptr %x) {
; ZVFH-LABEL: fneg_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfneg.v v8, v8
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fneg_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = fneg <6 x half> %a
  store <6 x half> %b, ptr %x
  ret void
}

define void @fneg_v4f32(ptr %x) {
; CHECK-LABEL: fneg_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfneg.v v8, v8
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = fneg <4 x float> %a
  store <4 x float> %b, ptr %x
  ret void
}

define void @fneg_v2f64(ptr %x) {
; CHECK-LABEL: fneg_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfneg.v v8, v8
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = fneg <2 x double> %a
  store <2 x double> %b, ptr %x
  ret void
}

define void @fabs_v8bf16(ptr %x) {
; RV32-LABEL: fabs_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fabs_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.fabs.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @fabs_v6bf16(ptr %x) {
; RV32-LABEL: fabs_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 17
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fabs_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = call <6 x bfloat> @llvm.fabs.v6bf16(<6 x bfloat> %a)
  store <6 x bfloat> %b, ptr %x
  ret void
}

define void @fabs_v8f16(ptr %x) {
; ZVFH-LABEL: fabs_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfabs.v v8, v8
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fabs_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a1, a1, -1
; ZVFHMIN-NEXT:    vand.vx v8, v8, a1
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.fabs.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @fabs_v6f16(ptr %x) {
; ZVFH-LABEL: fabs_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfabs.v v8, v8
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fabs_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a1, a1, -1
; ZVFHMIN-NEXT:    vand.vx v8, v8, a1
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = call <6 x half> @llvm.fabs.v6f16(<6 x half> %a)
  store <6 x half> %b, ptr %x
  ret void
}

define void @fabs_v4f32(ptr %x) {
; CHECK-LABEL: fabs_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfabs.v v8, v8
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.fabs.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @fabs_v2f64(ptr %x) {
; CHECK-LABEL: fabs_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfabs.v v8, v8
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.fabs.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @copysign_v8bf16(ptr %x, ptr %y) {
; RV32-LABEL: copysign_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    vslidedown.vi v10, v8, 1
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v10, v9, 1
; RV32-NEXT:    vmv.x.s a3, v10
; RV32-NEXT:    lui a4, 8
; RV32-NEXT:    addi a5, a4, -1
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vmv.x.s a3, v8
; RV32-NEXT:    and a3, a3, a4
; RV32-NEXT:    vmv.x.s a6, v9
; RV32-NEXT:    and a6, a6, a5
; RV32-NEXT:    or a3, a6, a3
; RV32-NEXT:    vmv.v.x v10, a3
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 2
; RV32-NEXT:    vmv.x.s a2, v11
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v11, v9, 2
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 3
; RV32-NEXT:    vmv.x.s a2, v11
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v11, v9, 3
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 5
; RV32-NEXT:    vmv.x.s a2, v11
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v11, v9, 5
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 4
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a4
; RV32-NEXT:    vslidedown.vi v11, v9, 4
; RV32-NEXT:    vmv.x.s a4, v11
; RV32-NEXT:    and a4, a4, a5
; RV32-NEXT:    or a3, a4, a3
; RV32-NEXT:    vmv.v.x v11, a3
; RV32-NEXT:    vslide1down.vx v11, v11, a2
; RV32-NEXT:    vslidedown.vi v12, v8, 6
; RV32-NEXT:    vmv.x.s a2, v12
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v12, v9, 6
; RV32-NEXT:    vmv.x.s a3, v12
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslide1down.vx v11, v11, a2
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a2, v8
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a2, v8
; RV32-NEXT:    and a2, a2, a5
; RV32-NEXT:    or a1, a2, a1
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    vslide1down.vx v8, v11, a1
; RV32-NEXT:    vslidedown.vi v8, v10, 4, v0.t
; RV32-NEXT:    vse16.v v8, (a0)
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    vslidedown.vi v10, v8, 1
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v10, v9, 1
; RV64-NEXT:    vmv.x.s a3, v10
; RV64-NEXT:    lui a4, 8
; RV64-NEXT:    addiw a5, a4, -1
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vmv.x.s a3, v8
; RV64-NEXT:    and a3, a3, a4
; RV64-NEXT:    vmv.x.s a6, v9
; RV64-NEXT:    and a6, a6, a5
; RV64-NEXT:    or a3, a6, a3
; RV64-NEXT:    vmv.v.x v10, a3
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 2
; RV64-NEXT:    vmv.x.s a2, v11
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v11, v9, 2
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 3
; RV64-NEXT:    vmv.x.s a2, v11
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v11, v9, 3
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 5
; RV64-NEXT:    vmv.x.s a2, v11
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v11, v9, 5
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 4
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a4
; RV64-NEXT:    vslidedown.vi v11, v9, 4
; RV64-NEXT:    vmv.x.s a4, v11
; RV64-NEXT:    and a4, a4, a5
; RV64-NEXT:    or a3, a4, a3
; RV64-NEXT:    vmv.v.x v11, a3
; RV64-NEXT:    vslide1down.vx v11, v11, a2
; RV64-NEXT:    vslidedown.vi v12, v8, 6
; RV64-NEXT:    vmv.x.s a2, v12
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v12, v9, 6
; RV64-NEXT:    vmv.x.s a3, v12
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslide1down.vx v11, v11, a2
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a2, v8
; RV64-NEXT:    and a1, a2, a1
; RV64-NEXT:    vslidedown.vi v8, v9, 7
; RV64-NEXT:    vmv.x.s a2, v8
; RV64-NEXT:    and a2, a2, a5
; RV64-NEXT:    or a1, a2, a1
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    vslide1down.vx v8, v11, a1
; RV64-NEXT:    vslidedown.vi v8, v10, 4, v0.t
; RV64-NEXT:    vse16.v v8, (a0)
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = call <8 x bfloat> @llvm.copysign.v8bf16(<8 x bfloat> %a, <8 x bfloat> %b)
  store <8 x bfloat> %c, ptr %x
  ret void
}

define void @copysign_v6bf16(ptr %x, ptr %y) {
; RV32-LABEL: copysign_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    vslidedown.vi v10, v8, 1
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v10, v9, 1
; RV32-NEXT:    vmv.x.s a3, v10
; RV32-NEXT:    lui a4, 8
; RV32-NEXT:    addi a5, a4, -1
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vmv.x.s a3, v8
; RV32-NEXT:    and a3, a3, a4
; RV32-NEXT:    vmv.x.s a6, v9
; RV32-NEXT:    and a6, a6, a5
; RV32-NEXT:    or a3, a6, a3
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v10, a3
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 2
; RV32-NEXT:    vmv.x.s a2, v11
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v11, v9, 2
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 3
; RV32-NEXT:    vmv.x.s a2, v11
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v11, v9, 3
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 5
; RV32-NEXT:    vmv.x.s a2, v11
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v11, v9, 5
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 4
; RV32-NEXT:    vmv.x.s a3, v11
; RV32-NEXT:    and a3, a3, a4
; RV32-NEXT:    vslidedown.vi v11, v9, 4
; RV32-NEXT:    vmv.x.s a4, v11
; RV32-NEXT:    and a4, a4, a5
; RV32-NEXT:    or a3, a4, a3
; RV32-NEXT:    vmv.v.x v11, a3
; RV32-NEXT:    vslide1down.vx v11, v11, a2
; RV32-NEXT:    vslidedown.vi v12, v8, 6
; RV32-NEXT:    vmv.x.s a2, v12
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    vslidedown.vi v12, v9, 6
; RV32-NEXT:    vmv.x.s a3, v12
; RV32-NEXT:    and a3, a3, a5
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    vslide1down.vx v11, v11, a2
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a2, v8
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a2, v8
; RV32-NEXT:    and a2, a2, a5
; RV32-NEXT:    or a1, a2, a1
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    vslide1down.vx v8, v11, a1
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v10, 4, v0.t
; RV32-NEXT:    vse16.v v8, (a0)
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    vslidedown.vi v10, v8, 1
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v10, v9, 1
; RV64-NEXT:    vmv.x.s a3, v10
; RV64-NEXT:    lui a4, 8
; RV64-NEXT:    addiw a5, a4, -1
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vmv.x.s a3, v8
; RV64-NEXT:    and a3, a3, a4
; RV64-NEXT:    vmv.x.s a6, v9
; RV64-NEXT:    and a6, a6, a5
; RV64-NEXT:    or a3, a6, a3
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v10, a3
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 2
; RV64-NEXT:    vmv.x.s a2, v11
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v11, v9, 2
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 3
; RV64-NEXT:    vmv.x.s a2, v11
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v11, v9, 3
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 5
; RV64-NEXT:    vmv.x.s a2, v11
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v11, v9, 5
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 4
; RV64-NEXT:    vmv.x.s a3, v11
; RV64-NEXT:    and a3, a3, a4
; RV64-NEXT:    vslidedown.vi v11, v9, 4
; RV64-NEXT:    vmv.x.s a4, v11
; RV64-NEXT:    and a4, a4, a5
; RV64-NEXT:    or a3, a4, a3
; RV64-NEXT:    vmv.v.x v11, a3
; RV64-NEXT:    vslide1down.vx v11, v11, a2
; RV64-NEXT:    vslidedown.vi v12, v8, 6
; RV64-NEXT:    vmv.x.s a2, v12
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    vslidedown.vi v12, v9, 6
; RV64-NEXT:    vmv.x.s a3, v12
; RV64-NEXT:    and a3, a3, a5
; RV64-NEXT:    or a2, a3, a2
; RV64-NEXT:    vslide1down.vx v11, v11, a2
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a2, v8
; RV64-NEXT:    and a1, a2, a1
; RV64-NEXT:    vslidedown.vi v8, v9, 7
; RV64-NEXT:    vmv.x.s a2, v8
; RV64-NEXT:    and a2, a2, a5
; RV64-NEXT:    or a1, a2, a1
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    vslide1down.vx v8, v11, a1
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v10, 4, v0.t
; RV64-NEXT:    vse16.v v8, (a0)
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = call <6 x bfloat> @llvm.copysign.v6bf16(<6 x bfloat> %a, <6 x bfloat> %b)
  store <6 x bfloat> %c, ptr %x
  ret void
}

define void @copysign_v8f16(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfsgnj.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vand.vx v8, v8, a1
; ZVFHMIN-NEXT:    addi a1, a1, -1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vor.vv v8, v9, v8
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = call <8 x half> @llvm.copysign.v8f16(<8 x half> %a, <8 x half> %b)
  store <8 x half> %c, ptr %x
  ret void
}

define void @copysign_v6f16(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfsgnj.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vand.vx v8, v8, a1
; ZVFHMIN-NEXT:    addi a1, a1, -1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vor.vv v8, v9, v8
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = call <6 x half> @llvm.copysign.v6f16(<6 x half> %a, <6 x half> %b)
  store <6 x half> %c, ptr %x
  ret void
}

define void @copysign_v4f32(ptr %x, ptr %y) {
; CHECK-LABEL: copysign_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfsgnj.vv v8, v8, v9
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = call <4 x float> @llvm.copysign.v4f32(<4 x float> %a, <4 x float> %b)
  store <4 x float> %c, ptr %x
  ret void
}

define void @copysign_v2f64(ptr %x, ptr %y) {
; CHECK-LABEL: copysign_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfsgnj.vv v8, v8, v9
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = call <2 x double> @llvm.copysign.v2f64(<2 x double> %a, <2 x double> %b)
  store <2 x double> %c, ptr %x
  ret void
}

define void @copysign_vf_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: copysign_vf_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    lui a2, 1048568
; RV32-NEXT:    and a1, a1, a2
; RV32-NEXT:    vslidedown.vi v9, v8, 1
; RV32-NEXT:    vmv.x.s a2, v9
; RV32-NEXT:    lui a3, 8
; RV32-NEXT:    addi a3, a3, -1
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vmv.x.s a4, v8
; RV32-NEXT:    and a4, a4, a3
; RV32-NEXT:    or a4, a4, a1
; RV32-NEXT:    vmv.v.x v9, a4
; RV32-NEXT:    vslide1down.vx v9, v9, a2
; RV32-NEXT:    vslidedown.vi v10, v8, 2
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a2
; RV32-NEXT:    vslidedown.vi v10, v8, 3
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a2
; RV32-NEXT:    vslidedown.vi v10, v8, 5
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vslidedown.vi v10, v8, 4
; RV32-NEXT:    vmv.x.s a4, v10
; RV32-NEXT:    and a4, a4, a3
; RV32-NEXT:    or a4, a4, a1
; RV32-NEXT:    vmv.v.x v10, a4
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 6
; RV32-NEXT:    vmv.x.s a2, v11
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a2, v8
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a1, a2, a1
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    vslide1down.vx v8, v10, a1
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (a0)
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_vf_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    fmv.x.w a1, fa0
; RV64-NEXT:    lui a2, 1048568
; RV64-NEXT:    and a1, a1, a2
; RV64-NEXT:    vslidedown.vi v9, v8, 1
; RV64-NEXT:    vmv.x.s a2, v9
; RV64-NEXT:    lui a3, 8
; RV64-NEXT:    addiw a3, a3, -1
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vmv.x.s a4, v8
; RV64-NEXT:    and a4, a4, a3
; RV64-NEXT:    or a4, a4, a1
; RV64-NEXT:    vmv.v.x v9, a4
; RV64-NEXT:    vslide1down.vx v9, v9, a2
; RV64-NEXT:    vslidedown.vi v10, v8, 2
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a2
; RV64-NEXT:    vslidedown.vi v10, v8, 3
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a2
; RV64-NEXT:    vslidedown.vi v10, v8, 5
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vslidedown.vi v10, v8, 4
; RV64-NEXT:    vmv.x.s a4, v10
; RV64-NEXT:    and a4, a4, a3
; RV64-NEXT:    or a4, a4, a1
; RV64-NEXT:    vmv.v.x v10, a4
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 6
; RV64-NEXT:    vmv.x.s a2, v11
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a2, v8
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a1, a2, a1
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    vslide1down.vx v8, v10, a1
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (a0)
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = call <8 x bfloat> @llvm.copysign.v8bf16(<8 x bfloat> %a, <8 x bfloat> %c)
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @copysign_vf_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: copysign_vf_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    lui a2, 1048568
; RV32-NEXT:    and a1, a1, a2
; RV32-NEXT:    vslidedown.vi v9, v8, 1
; RV32-NEXT:    vmv.x.s a2, v9
; RV32-NEXT:    lui a3, 8
; RV32-NEXT:    addi a3, a3, -1
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vmv.x.s a4, v8
; RV32-NEXT:    and a4, a4, a3
; RV32-NEXT:    or a4, a4, a1
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v9, a4
; RV32-NEXT:    vslide1down.vx v9, v9, a2
; RV32-NEXT:    vslidedown.vi v10, v8, 2
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a2
; RV32-NEXT:    vslidedown.vi v10, v8, 3
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a2
; RV32-NEXT:    vslidedown.vi v10, v8, 5
; RV32-NEXT:    vmv.x.s a2, v10
; RV32-NEXT:    and a2, a2, a3
; RV32-NEXT:    or a2, a2, a1
; RV32-NEXT:    vslidedown.vi v10, v8, 4
; RV32-NEXT:    vmv.x.s a4, v10
; RV32-NEXT:    and a4, a4, a3
; RV32-NEXT:    or a1, a4, a1
; RV32-NEXT:    vmv.v.x v10, a1
; RV32-NEXT:    vslide1down.vx v10, v10, a2
; RV32-NEXT:    vslidedown.vi v11, v8, 6
; RV32-NEXT:    vmv.x.s a1, v11
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    vslide1down.vx v10, v10, a1
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    vslide1down.vx v8, v10, a1
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (a0)
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_vf_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    fmv.x.w a1, fa0
; RV64-NEXT:    lui a2, 1048568
; RV64-NEXT:    and a1, a1, a2
; RV64-NEXT:    vslidedown.vi v9, v8, 1
; RV64-NEXT:    vmv.x.s a2, v9
; RV64-NEXT:    lui a3, 8
; RV64-NEXT:    addiw a3, a3, -1
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vmv.x.s a4, v8
; RV64-NEXT:    and a4, a4, a3
; RV64-NEXT:    or a4, a4, a1
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v9, a4
; RV64-NEXT:    vslide1down.vx v9, v9, a2
; RV64-NEXT:    vslidedown.vi v10, v8, 2
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a2
; RV64-NEXT:    vslidedown.vi v10, v8, 3
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a2
; RV64-NEXT:    vslidedown.vi v10, v8, 5
; RV64-NEXT:    vmv.x.s a2, v10
; RV64-NEXT:    and a2, a2, a3
; RV64-NEXT:    or a2, a2, a1
; RV64-NEXT:    vslidedown.vi v10, v8, 4
; RV64-NEXT:    vmv.x.s a4, v10
; RV64-NEXT:    and a4, a4, a3
; RV64-NEXT:    or a1, a4, a1
; RV64-NEXT:    vmv.v.x v10, a1
; RV64-NEXT:    vslide1down.vx v10, v10, a2
; RV64-NEXT:    vslidedown.vi v11, v8, 6
; RV64-NEXT:    vmv.x.s a1, v11
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    vslide1down.vx v10, v10, a1
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    vslide1down.vx v8, v10, a1
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (a0)
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = call <6 x bfloat> @llvm.copysign.v6bf16(<6 x bfloat> %a, <6 x bfloat> %c)
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @copysign_vf_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: copysign_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfsgnj.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vand.vx v8, v8, a2
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vor.vv v8, v8, v9
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = call <8 x half> @llvm.copysign.v8f16(<8 x half> %a, <8 x half> %c)
  store <8 x half> %d, ptr %x
  ret void
}

define void @copysign_vf_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: copysign_vf_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfsgnj.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_vf_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vand.vx v8, v8, a2
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vor.vv v8, v8, v9
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = call <6 x half> @llvm.copysign.v6f16(<6 x half> %a, <6 x half> %c)
  store <6 x half> %d, ptr %x
  ret void
}

define void @copysign_vf_v4f32(ptr %x, float %y) {
; CHECK-LABEL: copysign_vf_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfsgnj.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = call <4 x float> @llvm.copysign.v4f32(<4 x float> %a, <4 x float> %c)
  store <4 x float> %d, ptr %x
  ret void
}

define void @copysign_vf_v2f64(ptr %x, double %y) {
; CHECK-LABEL: copysign_vf_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfsgnj.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = call <2 x double> @llvm.copysign.v2f64(<2 x double> %a, <2 x double> %c)
  store <2 x double> %d, ptr %x
  ret void
}

define void @copysign_neg_v8bf16(ptr %x, ptr %y) {
; RV32-LABEL: copysign_neg_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 2 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v9, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s4, 524288
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s3, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s6, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s7, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v11, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v11, 1
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    lui a2, 8
; RV32-NEXT:    addi a3, a2, -1
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    lui a4, 1048568
; RV32-NEXT:    and a0, a0, a4
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    vmv.x.s a1, v11
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    and a5, s8, a2
; RV32-NEXT:    or a1, a1, a5
; RV32-NEXT:    vmv.v.x v8, a1
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vslidedown.vi v9, v11, 2
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s7, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vslidedown.vi v9, v11, 3
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s6, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vslidedown.vi v9, v11, 5
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s5, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslidedown.vi v9, v11, 4
; RV32-NEXT:    vmv.x.s a1, v9
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    and a2, s3, a2
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    vmv.v.x v9, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a0
; RV32-NEXT:    vslidedown.vi v10, v11, 6
; RV32-NEXT:    vmv.x.s a0, v10
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s2, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a0
; RV32-NEXT:    vslidedown.vi v10, v11, 7
; RV32-NEXT:    vmv.x.s a0, v10
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a4, s1, a4
; RV32-NEXT:    or a0, a0, a4
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    vslide1down.vx v9, v9, a0
; RV32-NEXT:    vslidedown.vi v9, v8, 4, v0.t
; RV32-NEXT:    vse16.v v9, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_neg_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -160
; RV64-NEXT:    .cfi_def_cfa_offset 160
; RV64-NEXT:    sd ra, 152(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 144(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 136(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 128(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 104(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s6, 96(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s7, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs3, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs4, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs5, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    .cfi_offset s6, -64
; RV64-NEXT:    .cfi_offset s7, -72
; RV64-NEXT:    .cfi_offset fs0, -80
; RV64-NEXT:    .cfi_offset fs1, -88
; RV64-NEXT:    .cfi_offset fs2, -96
; RV64-NEXT:    .cfi_offset fs3, -104
; RV64-NEXT:    .cfi_offset fs4, -112
; RV64-NEXT:    .cfi_offset fs5, -120
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xa0, 0x01, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 160 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs2, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs3, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.x.w s2, fs5
; RV64-NEXT:    fmv.x.w s3, fs4
; RV64-NEXT:    fmv.x.w s4, fs3
; RV64-NEXT:    fmv.x.w s5, fs2
; RV64-NEXT:    fmv.x.w s6, fs1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    fmv.x.w s7, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v11, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v11, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    lui a2, 8
; RV64-NEXT:    addiw a3, a2, -1
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    lui a4, 1048568
; RV64-NEXT:    and a0, a0, a4
; RV64-NEXT:    or a0, a1, a0
; RV64-NEXT:    vmv.x.s a1, v11
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    and a5, s7, a2
; RV64-NEXT:    or a1, a1, a5
; RV64-NEXT:    vmv.v.x v8, a1
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vslidedown.vi v9, v11, 2
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s6, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vslidedown.vi v9, v11, 3
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s5, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vslidedown.vi v9, v11, 5
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s4, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslidedown.vi v9, v11, 4
; RV64-NEXT:    vmv.x.s a1, v9
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    and a2, s3, a2
; RV64-NEXT:    or a1, a1, a2
; RV64-NEXT:    vmv.v.x v9, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a0
; RV64-NEXT:    vslidedown.vi v10, v11, 6
; RV64-NEXT:    vmv.x.s a0, v10
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s2, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a0
; RV64-NEXT:    vslidedown.vi v10, v11, 7
; RV64-NEXT:    vmv.x.s a0, v10
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a4, s1, a4
; RV64-NEXT:    or a0, a0, a4
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    vslide1down.vx v9, v9, a0
; RV64-NEXT:    vslidedown.vi v9, v8, 4, v0.t
; RV64-NEXT:    vse16.v v9, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 152(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 144(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 136(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 128(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 104(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s6, 96(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s7, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs3, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs4, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs5, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 160
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = fneg <8 x bfloat> %b
  %d = call <8 x bfloat> @llvm.copysign.v8bf16(<8 x bfloat> %a, <8 x bfloat> %c)
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @copysign_neg_v6bf16(ptr %x, ptr %y) {
; RV32-LABEL: copysign_neg_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 2 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v9, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s4, 524288
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s3, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s6, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s7, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s4
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v11, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v11, 1
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    lui a2, 8
; RV32-NEXT:    addi a3, a2, -1
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    lui a4, 1048568
; RV32-NEXT:    and a0, a0, a4
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    vmv.x.s a1, v11
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    and a5, s8, a2
; RV32-NEXT:    or a1, a1, a5
; RV32-NEXT:    vmv.v.x v8, a1
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vslidedown.vi v9, v11, 2
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s7, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vslidedown.vi v9, v11, 3
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s6, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vslidedown.vi v9, v11, 5
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s5, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslidedown.vi v9, v11, 4
; RV32-NEXT:    vmv.x.s a1, v9
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    and a2, s3, a2
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    vmv.v.x v9, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a0
; RV32-NEXT:    vslidedown.vi v10, v11, 6
; RV32-NEXT:    vmv.x.s a0, v10
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a1, s2, a4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    vslide1down.vx v9, v9, a0
; RV32-NEXT:    vslidedown.vi v10, v11, 7
; RV32-NEXT:    vmv.x.s a0, v10
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a4, s1, a4
; RV32-NEXT:    or a0, a0, a4
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    vslide1down.vx v9, v9, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v9, v8, 4, v0.t
; RV32-NEXT:    vse16.v v9, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_neg_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -160
; RV64-NEXT:    .cfi_def_cfa_offset 160
; RV64-NEXT:    sd ra, 152(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 144(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 136(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 128(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 104(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s6, 96(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s7, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs3, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs4, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs5, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    .cfi_offset s6, -64
; RV64-NEXT:    .cfi_offset s7, -72
; RV64-NEXT:    .cfi_offset fs0, -80
; RV64-NEXT:    .cfi_offset fs1, -88
; RV64-NEXT:    .cfi_offset fs2, -96
; RV64-NEXT:    .cfi_offset fs3, -104
; RV64-NEXT:    .cfi_offset fs4, -112
; RV64-NEXT:    .cfi_offset fs5, -120
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xa0, 0x01, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 160 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs2, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs3, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.x.w s2, fs5
; RV64-NEXT:    fmv.x.w s3, fs4
; RV64-NEXT:    fmv.x.w s4, fs3
; RV64-NEXT:    fmv.x.w s5, fs2
; RV64-NEXT:    fmv.x.w s6, fs1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    fmv.x.w s7, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v11, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v11, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    lui a2, 8
; RV64-NEXT:    addiw a3, a2, -1
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    lui a4, 1048568
; RV64-NEXT:    and a0, a0, a4
; RV64-NEXT:    or a0, a1, a0
; RV64-NEXT:    vmv.x.s a1, v11
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    and a5, s7, a2
; RV64-NEXT:    or a1, a1, a5
; RV64-NEXT:    vmv.v.x v8, a1
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vslidedown.vi v9, v11, 2
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s6, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vslidedown.vi v9, v11, 3
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s5, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vslidedown.vi v9, v11, 5
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s4, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslidedown.vi v9, v11, 4
; RV64-NEXT:    vmv.x.s a1, v9
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    and a2, s3, a2
; RV64-NEXT:    or a1, a1, a2
; RV64-NEXT:    vmv.v.x v9, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a0
; RV64-NEXT:    vslidedown.vi v10, v11, 6
; RV64-NEXT:    vmv.x.s a0, v10
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a1, s2, a4
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    vslide1down.vx v9, v9, a0
; RV64-NEXT:    vslidedown.vi v10, v11, 7
; RV64-NEXT:    vmv.x.s a0, v10
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    and a4, s1, a4
; RV64-NEXT:    or a0, a0, a4
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    vslide1down.vx v9, v9, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v9, v8, 4, v0.t
; RV64-NEXT:    vse16.v v9, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 152(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 144(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 136(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 128(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 104(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s6, 96(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s7, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs3, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs4, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs5, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 160
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = fneg <6 x bfloat> %b
  %d = call <6 x bfloat> @llvm.copysign.v6bf16(<6 x bfloat> %a, <6 x bfloat> %c)
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @copysign_neg_v8f16(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_neg_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfsgnjn.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_neg_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a2
; ZVFHMIN-NEXT:    vand.vx v8, v8, a1
; ZVFHMIN-NEXT:    vor.vv v8, v9, v8
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = fneg <8 x half> %b
  %d = call <8 x half> @llvm.copysign.v8f16(<8 x half> %a, <8 x half> %c)
  store <8 x half> %d, ptr %x
  ret void
}

define void @copysign_neg_v6f16(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_neg_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfsgnjn.vv v8, v8, v9
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_neg_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a2
; ZVFHMIN-NEXT:    vand.vx v8, v8, a1
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vor.vv v8, v9, v8
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = fneg <6 x half> %b
  %d = call <6 x half> @llvm.copysign.v6f16(<6 x half> %a, <6 x half> %c)
  store <6 x half> %d, ptr %x
  ret void
}

define void @copysign_neg_v4f32(ptr %x, ptr %y) {
; CHECK-LABEL: copysign_neg_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfsgnjn.vv v8, v8, v9
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fneg <4 x float> %b
  %d = call <4 x float> @llvm.copysign.v4f32(<4 x float> %a, <4 x float> %c)
  store <4 x float> %d, ptr %x
  ret void
}

define void @copysign_neg_v2f64(ptr %x, ptr %y) {
; CHECK-LABEL: copysign_neg_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfsgnjn.vv v8, v8, v9
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = fneg <2 x double> %b
  %d = call <2 x double> @llvm.copysign.v2f64(<2 x double> %a, <2 x double> %c)
  store <2 x double> %d, ptr %x
  ret void
}

define void @copysign_neg_trunc_v4bf16_v4f32(ptr %x, ptr %y) {
; RV32-LABEL: copysign_neg_trunc_v4bf16_v4f32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 20(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 16(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a3, a2, 1
; RV32-NEXT:    add a2, a3, a2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle32.v v8, (a1)
; RV32-NEXT:    vslidedown.vi v9, v9, 1
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    lui s4, 8
; RV32-NEXT:    addi s1, s4, -1
; RV32-NEXT:    and s5, a0, s1
; RV32-NEXT:    vfncvtbf16.f.f.w v9, v8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s2, 524288
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    lui s3, 1048568
; RV32-NEXT:    and a0, a0, s3
; RV32-NEXT:    or s5, s5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    and s6, a0, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    and a0, a0, s4
; RV32-NEXT:    or a0, s6, a0
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s5
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    and s4, a0, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    and a0, a0, s3
; RV32-NEXT:    or a0, s4, a0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    and s1, a0, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    and a0, a0, s3
; RV32-NEXT:    or a0, s1, a0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_neg_trunc_v4bf16_v4f32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a3, a2, 1
; RV64-NEXT:    add a2, a3, a2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xe0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 96 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle32.v v8, (a1)
; RV64-NEXT:    vslidedown.vi v9, v9, 1
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    lui s3, 8
; RV64-NEXT:    addiw s1, s3, -1
; RV64-NEXT:    and s4, a0, s1
; RV64-NEXT:    vfncvtbf16.f.f.w v9, v8
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    lui s2, 1048568
; RV64-NEXT:    and a0, a0, s2
; RV64-NEXT:    or s4, s4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    and s5, a0, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    and a0, a0, s3
; RV64-NEXT:    or a0, s5, a0
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    and s3, a0, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    and a0, a0, s2
; RV64-NEXT:    or a0, s3, a0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    and s1, a0, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    and a0, a0, s2
; RV64-NEXT:    or a0, s1, a0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %a = load <4 x bfloat>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fneg <4 x float> %b
  %d = fptrunc <4 x float> %c to <4 x bfloat>
  %e = call <4 x bfloat> @llvm.copysign.v4bf16(<4 x bfloat> %a, <4 x bfloat> %d)
  store <4 x bfloat> %e, ptr %x
  ret void
}

define void @copysign_neg_trunc_v3bf16_v3f32(ptr %x, ptr %y) {
; RV32-LABEL: copysign_neg_trunc_v3bf16_v3f32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 20(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 16(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a3, a2, 1
; RV32-NEXT:    add a2, a3, a2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle32.v v8, (a1)
; RV32-NEXT:    vslidedown.vi v9, v9, 1
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    lui s4, 8
; RV32-NEXT:    addi s1, s4, -1
; RV32-NEXT:    and s5, a0, s1
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vfncvtbf16.f.f.w v9, v8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s2, 524288
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    lui s3, 1048568
; RV32-NEXT:    and a0, a0, s3
; RV32-NEXT:    or s5, s5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    and s6, a0, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    and a0, a0, s4
; RV32-NEXT:    or a0, s6, a0
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s5
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    and s4, a0, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    and a0, a0, s3
; RV32-NEXT:    or a0, s4, a0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    and s1, a0, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s2
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    and a0, a0, s3
; RV32-NEXT:    or a0, s1, a0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: copysign_neg_trunc_v3bf16_v3f32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a3, a2, 1
; RV64-NEXT:    add a2, a3, a2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xe0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 96 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle32.v v8, (a1)
; RV64-NEXT:    vslidedown.vi v9, v9, 1
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    lui s3, 8
; RV64-NEXT:    addiw s1, s3, -1
; RV64-NEXT:    and s4, a0, s1
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vfncvtbf16.f.f.w v9, v8
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    lui s2, 1048568
; RV64-NEXT:    and a0, a0, s2
; RV64-NEXT:    or s4, s4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    and s5, a0, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    and a0, a0, s3
; RV64-NEXT:    or a0, s5, a0
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    and s3, a0, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    and a0, a0, s2
; RV64-NEXT:    or a0, s3, a0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    and s1, a0, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    and a0, a0, s2
; RV64-NEXT:    or a0, s1, a0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %a = load <3 x bfloat>, ptr %x
  %b = load <3 x float>, ptr %y
  %c = fneg <3 x float> %b
  %d = fptrunc <3 x float> %c to <3 x bfloat>
  %e = call <3 x bfloat> @llvm.copysign.v3bf16(<3 x bfloat> %a, <3 x bfloat> %d)
  store <3 x bfloat> %e, ptr %x
  ret void
}

define void @copysign_neg_trunc_v4f16_v4f32(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_neg_trunc_v4f16_v4f32:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFH-NEXT:    vle32.v v8, (a1)
; ZVFH-NEXT:    vle16.v v9, (a0)
; ZVFH-NEXT:    vfncvt.f.f.w v10, v8
; ZVFH-NEXT:    vfsgnjn.vv v8, v9, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_neg_trunc_v4f16_v4f32:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vle32.v v9, (a1)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vand.vx v8, v8, a2
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v9
; ZVFHMIN-NEXT:    vxor.vx v9, v10, a1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vor.vv v8, v8, v9
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <4 x half>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fneg <4 x float> %b
  %d = fptrunc <4 x float> %c to <4 x half>
  %e = call <4 x half> @llvm.copysign.v4f16(<4 x half> %a, <4 x half> %d)
  store <4 x half> %e, ptr %x
  ret void
}

define void @copysign_neg_trunc_v3f16_v3f32(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_neg_trunc_v3f16_v3f32:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 3, e32, m1, ta, ma
; ZVFH-NEXT:    vle32.v v8, (a1)
; ZVFH-NEXT:    vle16.v v9, (a0)
; ZVFH-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFH-NEXT:    vfncvt.f.f.w v10, v8
; ZVFH-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; ZVFH-NEXT:    vfsgnjn.vv v8, v9, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_neg_trunc_v3f16_v3f32:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vle32.v v9, (a1)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vand.vx v8, v8, a2
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v9
; ZVFHMIN-NEXT:    vxor.vx v9, v10, a1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vor.vv v8, v8, v9
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <3 x half>, ptr %x
  %b = load <3 x float>, ptr %y
  %c = fneg <3 x float> %b
  %d = fptrunc <3 x float> %c to <3 x half>
  %e = call <3 x half> @llvm.copysign.v3f16(<3 x half> %a, <3 x half> %d)
  store <3 x half> %e, ptr %x
  ret void
}

define void @copysign_neg_ext_v2f64_v2f32(ptr %x, ptr %y) {
; CHECK-LABEL: copysign_neg_ext_v2f64_v2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a1)
; CHECK-NEXT:    vle64.v v9, (a0)
; CHECK-NEXT:    vfwcvt.f.f.v v10, v8
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, ma
; CHECK-NEXT:    vfsgnjn.vv v8, v9, v10
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x float>, ptr %y
  %c = fneg <2 x float> %b
  %d = fpext <2 x float> %c to <2 x double>
  %e = call <2 x double> @llvm.copysign.v2f64(<2 x double> %a, <2 x double> %d)
  store <2 x double> %e, ptr %x
  ret void
}

define void @sqrt_v8bf16(ptr %x) {
; RV32-LABEL: sqrt_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa5, fa5
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa5, fa5
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: sqrt_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.sqrt.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @sqrt_v6bf16(ptr %x) {
; RV32-LABEL: sqrt_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa5, fa5
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsqrt.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: sqrt_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsqrt.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = call <6 x bfloat> @llvm.sqrt.v6bf16(<6 x bfloat> %a)
  store <6 x bfloat> %b, ptr %x
  ret void
}

define void @sqrt_v8f16(ptr %x) {
; ZVFH-LABEL: sqrt_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfsqrt.v v8, v8
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: sqrt_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsqrt.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.sqrt.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @sqrt_v6f16(ptr %x) {
; ZVFH-LABEL: sqrt_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfsqrt.v v8, v8
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: sqrt_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsqrt.v v8, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = call <6 x half> @llvm.sqrt.v6f16(<6 x half> %a)
  store <6 x half> %b, ptr %x
  ret void
}

define void @sqrt_v4f32(ptr %x) {
; CHECK-LABEL: sqrt_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfsqrt.v v8, v8
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.sqrt.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @sqrt_v2f64(ptr %x) {
; CHECK-LABEL: sqrt_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfsqrt.v v8, v8
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.sqrt.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @fma_v8bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fma_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a4, a3, 2
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 5 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a2)
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    add a2, sp, a2
; RV32-NEXT:    addi a2, a2, 32
; RV32-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v10, (a1)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 2
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fma_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a4, a3, 2
; RV64-NEXT:    add a3, a4, a3
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 5 * vlenb
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    add a2, sp, a2
; RV64-NEXT:    addi a2, a2, 32
; RV64-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v10, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 2
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = load <8 x bfloat>, ptr %z
  %d = call <8 x bfloat> @llvm.fma.v8bf16(<8 x bfloat> %a, <8 x bfloat> %b, <8 x bfloat> %c)
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fma_v6bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fma_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a4, a3, 2
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 5 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a2)
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    add a2, sp, a2
; RV32-NEXT:    addi a2, a2, 32
; RV32-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v10, (a1)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 2
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fma_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a4, a3, 2
; RV64-NEXT:    add a3, a4, a3
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 5 * vlenb
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    add a2, sp, a2
; RV64-NEXT:    addi a2, a2, 32
; RV64-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v10, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 2
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = load <6 x bfloat>, ptr %z
  %d = call <6 x bfloat> @llvm.fma.v6bf16(<6 x bfloat> %a, <6 x bfloat> %b, <6 x bfloat> %c)
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fma_v8f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fma_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmacc.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fma_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a2)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a1)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = load <8 x half>, ptr %z
  %d = call <8 x half> @llvm.fma.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c)
  store <8 x half> %d, ptr %x
  ret void
}

define void @fma_v6f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fma_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmacc.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fma_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a2)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a1)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = load <6 x half>, ptr %z
  %d = call <6 x half> @llvm.fma.v6f16(<6 x half> %a, <6 x half> %b, <6 x half> %c)
  store <6 x half> %d, ptr %x
  ret void
}

define void @fma_v4f32(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fma_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vle32.v v10, (a2)
; CHECK-NEXT:    vfmacc.vv v10, v8, v9
; CHECK-NEXT:    vse32.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = load <4 x float>, ptr %z
  %d = call <4 x float> @llvm.fma.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c)
  store <4 x float> %d, ptr %x
  ret void
}

define void @fma_v2f64(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fma_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vle64.v v10, (a2)
; CHECK-NEXT:    vfmacc.vv v10, v8, v9
; CHECK-NEXT:    vse64.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = load <2 x double>, ptr %z
  %d = call <2 x double> @llvm.fma.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  store <2 x double> %d, ptr %x
  ret void
}

define void @fmsub_v8bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fmsub_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a3, a3, 2
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v9, (a2)
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    add a2, sp, a2
; RV32-NEXT:    addi a2, a2, 16
; RV32-NEXT:    vs1r.v v9, (a2) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a2, a0, 1
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s6, 524288
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s3, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s7, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s6, fa0
; RV32-NEXT:    slli s8, s8, 16
; RV32-NEXT:    fmv.w.x fa5, s8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s6
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s7, s7, 16
; RV32-NEXT:    fmv.w.x fa5, s7
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s5, s5, 16
; RV32-NEXT:    fmv.w.x fa5, s5
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s4, s4, 16
; RV32-NEXT:    fmv.w.x fa5, s4
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    slli s3, s3, 16
; RV32-NEXT:    fmv.w.x fa5, s3
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s4
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s2, s2, 16
; RV32-NEXT:    fmv.w.x fa5, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s1, s1, 16
; RV32-NEXT:    fmv.w.x fa5, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: fmsub_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -144
; RV64-NEXT:    .cfi_def_cfa_offset 144
; RV64-NEXT:    sd ra, 136(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 128(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 104(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 96(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s6, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s7, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s8, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs3, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs4, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs5, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    .cfi_offset s6, -64
; RV64-NEXT:    .cfi_offset s7, -72
; RV64-NEXT:    .cfi_offset s8, -80
; RV64-NEXT:    .cfi_offset fs0, -88
; RV64-NEXT:    .cfi_offset fs1, -96
; RV64-NEXT:    .cfi_offset fs2, -104
; RV64-NEXT:    .cfi_offset fs3, -112
; RV64-NEXT:    .cfi_offset fs4, -120
; RV64-NEXT:    .cfi_offset fs5, -128
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a3, a3, 2
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0x90, 0x01, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 144 + 4 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a3, a0, 1
; RV64-NEXT:    add a0, a3, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs2, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs3, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.x.w s2, fs5
; RV64-NEXT:    fmv.x.w s3, fs4
; RV64-NEXT:    fmv.x.w s4, fs3
; RV64-NEXT:    fmv.x.w s5, fs2
; RV64-NEXT:    fmv.x.w s6, fs1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    fmv.x.w s7, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa5, a1
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa4, a1
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s8, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s7, s7, 16
; RV64-NEXT:    fmv.w.x fa3, s7
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s8
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s6, s6, 16
; RV64-NEXT:    fmv.w.x fa3, s6
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s5, s5, 16
; RV64-NEXT:    fmv.w.x fa3, s5
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s4, s4, 16
; RV64-NEXT:    fmv.w.x fa3, s4
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s3, s3, 16
; RV64-NEXT:    fmv.w.x fa3, s3
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s2, s2, 16
; RV64-NEXT:    fmv.w.x fa3, s2
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s1, s1, 16
; RV64-NEXT:    fmv.w.x fa3, s1
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 136(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 128(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 104(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 96(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s6, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s7, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s8, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs3, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs4, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs5, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 144
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = load <8 x bfloat>, ptr %z
  %neg = fneg <8 x bfloat> %c
  %d = call <8 x bfloat> @llvm.fma.v8bf16(<8 x bfloat> %a, <8 x bfloat> %b, <8 x bfloat> %neg)
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fmsub_v6bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fmsub_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a3, a3, 2
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v9, (a2)
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    add a2, sp, a2
; RV32-NEXT:    addi a2, a2, 16
; RV32-NEXT:    vs1r.v v9, (a2) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a2, a0, 1
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s6, 524288
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s3, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s7, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s6, fa0
; RV32-NEXT:    slli s8, s8, 16
; RV32-NEXT:    fmv.w.x fa5, s8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s6
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s7, s7, 16
; RV32-NEXT:    fmv.w.x fa5, s7
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s5, s5, 16
; RV32-NEXT:    fmv.w.x fa5, s5
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s4, s4, 16
; RV32-NEXT:    fmv.w.x fa5, s4
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    slli s3, s3, 16
; RV32-NEXT:    fmv.w.x fa5, s3
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s4
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s2, s2, 16
; RV32-NEXT:    fmv.w.x fa5, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s1, s1, 16
; RV32-NEXT:    fmv.w.x fa5, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: fmsub_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -144
; RV64-NEXT:    .cfi_def_cfa_offset 144
; RV64-NEXT:    sd ra, 136(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 128(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 104(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 96(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s6, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s7, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s8, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs3, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs4, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs5, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    .cfi_offset s6, -64
; RV64-NEXT:    .cfi_offset s7, -72
; RV64-NEXT:    .cfi_offset s8, -80
; RV64-NEXT:    .cfi_offset fs0, -88
; RV64-NEXT:    .cfi_offset fs1, -96
; RV64-NEXT:    .cfi_offset fs2, -104
; RV64-NEXT:    .cfi_offset fs3, -112
; RV64-NEXT:    .cfi_offset fs4, -120
; RV64-NEXT:    .cfi_offset fs5, -128
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a3, a3, 2
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0x90, 0x01, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 144 + 4 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a3, a0, 1
; RV64-NEXT:    add a0, a3, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs2, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs3, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.x.w s2, fs5
; RV64-NEXT:    fmv.x.w s3, fs4
; RV64-NEXT:    fmv.x.w s4, fs3
; RV64-NEXT:    fmv.x.w s5, fs2
; RV64-NEXT:    fmv.x.w s6, fs1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    fmv.x.w s7, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa5, a1
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa4, a1
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s8, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s7, s7, 16
; RV64-NEXT:    fmv.w.x fa3, s7
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s8
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s6, s6, 16
; RV64-NEXT:    fmv.w.x fa3, s6
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s5, s5, 16
; RV64-NEXT:    fmv.w.x fa3, s5
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s4, s4, 16
; RV64-NEXT:    fmv.w.x fa3, s4
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s3, s3, 16
; RV64-NEXT:    fmv.w.x fa3, s3
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s2, s2, 16
; RV64-NEXT:    fmv.w.x fa3, s2
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    slli s1, s1, 16
; RV64-NEXT:    fmv.w.x fa3, s1
; RV64-NEXT:    fmadd.s fa0, fa4, fa5, fa3
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 136(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 128(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 104(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 96(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s6, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s7, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s8, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs3, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs4, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs5, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 144
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = load <6 x bfloat>, ptr %z
  %neg = fneg <6 x bfloat> %c
  %d = call <6 x bfloat> @llvm.fma.v6bf16(<6 x bfloat> %a, <6 x bfloat> %b, <6 x bfloat> %neg)
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fmsub_v8f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fmsub_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmsac.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmsub_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a2)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a1)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = load <8 x half>, ptr %z
  %neg = fneg <8 x half> %c
  %d = call <8 x half> @llvm.fma.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %neg)
  store <8 x half> %d, ptr %x
  ret void
}

define void @fmsub_v6f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fmsub_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmsac.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmsub_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a2)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a1)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = load <6 x half>, ptr %z
  %neg = fneg <6 x half> %c
  %d = call <6 x half> @llvm.fma.v6f16(<6 x half> %a, <6 x half> %b, <6 x half> %neg)
  store <6 x half> %d, ptr %x
  ret void
}

define void @fnmsub_v4f32(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fnmsub_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vle32.v v10, (a2)
; CHECK-NEXT:    vfnmsac.vv v10, v8, v9
; CHECK-NEXT:    vse32.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = load <4 x float>, ptr %z
  %neg = fneg <4 x float> %a
  %d = call <4 x float> @llvm.fma.v4f32(<4 x float> %neg, <4 x float> %b, <4 x float> %c)
  store <4 x float> %d, ptr %x
  ret void
}

define void @fnmadd_v2f64(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fnmadd_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vle64.v v10, (a2)
; CHECK-NEXT:    vfnmacc.vv v10, v8, v9
; CHECK-NEXT:    vse64.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = load <2 x double>, ptr %z
  %neg = fneg <2 x double> %b
  %neg2 = fneg <2 x double> %c
  %d = call <2 x double> @llvm.fma.v2f64(<2 x double> %a, <2 x double> %neg, <2 x double> %neg2)
  store <2 x double> %d, ptr %x
  ret void
}

define void @fadd_v16bf16(ptr %x, ptr %y) {
; RV32-LABEL: fadd_v16bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    mv a3, a2
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 6 * vlenb
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v10, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fadd_v16bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    mv a3, a2
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    add a2, a2, a3
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 6 * vlenb
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v10, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <16 x bfloat>, ptr %x
  %b = load <16 x bfloat>, ptr %y
  %c = fadd <16 x bfloat> %a, %b
  store <16 x bfloat> %c, ptr %x
  ret void
}

define void @fadd_v16f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fadd_v16f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v10, (a1)
; ZVFH-NEXT:    vfadd.vv v8, v8, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fadd_v16f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v10, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v12, v8
; ZVFHMIN-NEXT:    vse16.v v12, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <16 x half>, ptr %x
  %b = load <16 x half>, ptr %y
  %c = fadd <16 x half> %a, %b
  store <16 x half> %c, ptr %x
  ret void
}

define void @fadd_v8f32(ptr %x, ptr %y) {
; CHECK-LABEL: fadd_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v10, (a1)
; CHECK-NEXT:    vfadd.vv v8, v8, v10
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <8 x float>, ptr %x
  %b = load <8 x float>, ptr %y
  %c = fadd <8 x float> %a, %b
  store <8 x float> %c, ptr %x
  ret void
}

define void @fadd_v4f64(ptr %x, ptr %y) {
; CHECK-LABEL: fadd_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v10, (a1)
; CHECK-NEXT:    vfadd.vv v8, v8, v10
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x double>, ptr %x
  %b = load <4 x double>, ptr %y
  %c = fadd <4 x double> %a, %b
  store <4 x double> %c, ptr %x
  ret void
}

define void @fsub_v16bf16(ptr %x, ptr %y) {
; RV32-LABEL: fsub_v16bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    mv a3, a2
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 6 * vlenb
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v10, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fsub_v16bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    mv a3, a2
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    add a2, a2, a3
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 6 * vlenb
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v10, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <16 x bfloat>, ptr %x
  %b = load <16 x bfloat>, ptr %y
  %c = fsub <16 x bfloat> %a, %b
  store <16 x bfloat> %c, ptr %x
  ret void
}

define void @fsub_v16f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fsub_v16f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v10, (a1)
; ZVFH-NEXT:    vfsub.vv v8, v8, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fsub_v16f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v10, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v12, v8
; ZVFHMIN-NEXT:    vse16.v v12, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <16 x half>, ptr %x
  %b = load <16 x half>, ptr %y
  %c = fsub <16 x half> %a, %b
  store <16 x half> %c, ptr %x
  ret void
}

define void @fsub_v8f32(ptr %x, ptr %y) {
; CHECK-LABEL: fsub_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v10, (a1)
; CHECK-NEXT:    vfsub.vv v8, v8, v10
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <8 x float>, ptr %x
  %b = load <8 x float>, ptr %y
  %c = fsub <8 x float> %a, %b
  store <8 x float> %c, ptr %x
  ret void
}

define void @fsub_v4f64(ptr %x, ptr %y) {
; CHECK-LABEL: fsub_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v10, (a1)
; CHECK-NEXT:    vfsub.vv v8, v8, v10
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x double>, ptr %x
  %b = load <4 x double>, ptr %y
  %c = fsub <4 x double> %a, %b
  store <4 x double> %c, ptr %x
  ret void
}

define void @fmul_v16bf16(ptr %x, ptr %y) {
; RV32-LABEL: fmul_v16bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    mv a3, a2
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 6 * vlenb
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v10, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmul_v16bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    mv a3, a2
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    add a2, a2, a3
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 6 * vlenb
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v10, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <16 x bfloat>, ptr %x
  %b = load <16 x bfloat>, ptr %y
  %c = fmul <16 x bfloat> %a, %b
  store <16 x bfloat> %c, ptr %x
  ret void
}

define void @fmul_v16f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fmul_v16f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v10, (a1)
; ZVFH-NEXT:    vfmul.vv v8, v8, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmul_v16f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v10, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v12, v8
; ZVFHMIN-NEXT:    vse16.v v12, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <16 x half>, ptr %x
  %b = load <16 x half>, ptr %y
  %c = fmul <16 x half> %a, %b
  store <16 x half> %c, ptr %x
  ret void
}

define void @fmul_v8f32(ptr %x, ptr %y) {
; CHECK-LABEL: fmul_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v10, (a1)
; CHECK-NEXT:    vfmul.vv v8, v8, v10
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <8 x float>, ptr %x
  %b = load <8 x float>, ptr %y
  %c = fmul <8 x float> %a, %b
  store <8 x float> %c, ptr %x
  ret void
}

define void @fmul_v4f64(ptr %x, ptr %y) {
; CHECK-LABEL: fmul_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v10, (a1)
; CHECK-NEXT:    vfmul.vv v8, v8, v10
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x double>, ptr %x
  %b = load <4 x double>, ptr %y
  %c = fmul <4 x double> %a, %b
  store <4 x double> %c, ptr %x
  ret void
}

define void @fdiv_v16bf16(ptr %x, ptr %y) {
; RV32-LABEL: fdiv_v16bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    mv a3, a2
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 6 * vlenb
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v10, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fdiv.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fdiv_v16bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    mv a3, a2
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    add a2, a2, a3
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x06, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 6 * vlenb
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs2r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v10, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fdiv.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <16 x bfloat>, ptr %x
  %b = load <16 x bfloat>, ptr %y
  %c = fdiv <16 x bfloat> %a, %b
  store <16 x bfloat> %c, ptr %x
  ret void
}

define void @fdiv_v16f16(ptr %x, ptr %y) {
; ZVFH-LABEL: fdiv_v16f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v10, (a1)
; ZVFH-NEXT:    vfdiv.vv v8, v8, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fdiv_v16f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v10, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfdiv.vv v8, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v12, v8
; ZVFHMIN-NEXT:    vse16.v v12, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <16 x half>, ptr %x
  %b = load <16 x half>, ptr %y
  %c = fdiv <16 x half> %a, %b
  store <16 x half> %c, ptr %x
  ret void
}

define void @fdiv_v8f32(ptr %x, ptr %y) {
; CHECK-LABEL: fdiv_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v10, (a1)
; CHECK-NEXT:    vfdiv.vv v8, v8, v10
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <8 x float>, ptr %x
  %b = load <8 x float>, ptr %y
  %c = fdiv <8 x float> %a, %b
  store <8 x float> %c, ptr %x
  ret void
}

define void @fdiv_v4f64(ptr %x, ptr %y) {
; CHECK-LABEL: fdiv_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v10, (a1)
; CHECK-NEXT:    vfdiv.vv v8, v8, v10
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x double>, ptr %x
  %b = load <4 x double>, ptr %y
  %c = fdiv <4 x double> %a, %b
  store <4 x double> %c, ptr %x
  ret void
}

define void @fneg_v16bf16(ptr %x) {
; RV32-LABEL: fneg_v16bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -32
; RV32-NEXT:    .cfi_def_cfa_offset 32
; RV32-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 20(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 16(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 2
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x20, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 32 + 4 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s1, 524288
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s1
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 32
; RV32-NEXT:    ret
;
; RV64-LABEL: fneg_v16bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 2
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 4 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa5, fa5
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <16 x bfloat>, ptr %x
  %b = fneg <16 x bfloat> %a
  store <16 x bfloat> %b, ptr %x
  ret void
}

define void @fneg_v16f16(ptr %x) {
; ZVFH-LABEL: fneg_v16f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfneg.v v8, v8
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fneg_v16f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <16 x half>, ptr %x
  %b = fneg <16 x half> %a
  store <16 x half> %b, ptr %x
  ret void
}

define void @fneg_v8f32(ptr %x) {
; CHECK-LABEL: fneg_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfneg.v v8, v8
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <8 x float>, ptr %x
  %b = fneg <8 x float> %a
  store <8 x float> %b, ptr %x
  ret void
}

define void @fneg_v4f64(ptr %x) {
; CHECK-LABEL: fneg_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfneg.v v8, v8
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x double>, ptr %x
  %b = fneg <4 x double> %a
  store <4 x double> %b, ptr %x
  ret void
}

define void @fma_v16bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fma_v16bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a3, a3, 3
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x08, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 8 * vlenb
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vle16.v v8, (a2)
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    add a2, sp, a2
; RV32-NEXT:    addi a2, a2, 32
; RV32-NEXT:    vs2r.v v8, (a2) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v10, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a2, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v12, (a1)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs2r.v v12, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v12, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 8
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 9
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 10
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 11
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 12
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 13
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 14
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 15
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa3, a0
; RV32-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 3
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fma_v16bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a3, a3, 3
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x08, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 8 * vlenb
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 1
; RV64-NEXT:    add a2, sp, a2
; RV64-NEXT:    addi a2, a2, 32
; RV64-NEXT:    vs2r.v v8, (a2) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v10, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a2, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v12, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs2r.v v12, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v12, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 8
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 9
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 10
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 11
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 12
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 13
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 14
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl2r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 15
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa3, a0
; RV64-NEXT:    fmadd.s fa0, fa3, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl2r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 3
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <16 x bfloat>, ptr %x
  %b = load <16 x bfloat>, ptr %y
  %c = load <16 x bfloat>, ptr %z
  %d = call <16 x bfloat> @llvm.fma.v16bf16(<16 x bfloat> %a, <16 x bfloat> %b, <16 x bfloat> %c)
  store <16 x bfloat> %d, ptr %x
  ret void
}

define void @fma_v16f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fma_v16f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v10, (a1)
; ZVFH-NEXT:    vle16.v v12, (a2)
; ZVFH-NEXT:    vfmacc.vv v12, v8, v10
; ZVFH-NEXT:    vse16.v v12, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fma_v16f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a2)
; ZVFHMIN-NEXT:    vle16.v v10, (a0)
; ZVFHMIN-NEXT:    vle16.v v12, (a1)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v20, v10
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v20, v16
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v12, v8
; ZVFHMIN-NEXT:    vse16.v v12, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <16 x half>, ptr %x
  %b = load <16 x half>, ptr %y
  %c = load <16 x half>, ptr %z
  %d = call <16 x half> @llvm.fma.v16f16(<16 x half> %a, <16 x half> %b, <16 x half> %c)
  store <16 x half> %d, ptr %x
  ret void
}

define void @fma_v8f32(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fma_v8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v10, (a1)
; CHECK-NEXT:    vle32.v v12, (a2)
; CHECK-NEXT:    vfmacc.vv v12, v8, v10
; CHECK-NEXT:    vse32.v v12, (a0)
; CHECK-NEXT:    ret
  %a = load <8 x float>, ptr %x
  %b = load <8 x float>, ptr %y
  %c = load <8 x float>, ptr %z
  %d = call <8 x float> @llvm.fma.v8f32(<8 x float> %a, <8 x float> %b, <8 x float> %c)
  store <8 x float> %d, ptr %x
  ret void
}

define void @fma_v4f64(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fma_v4f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v10, (a1)
; CHECK-NEXT:    vle64.v v12, (a2)
; CHECK-NEXT:    vfmacc.vv v12, v8, v10
; CHECK-NEXT:    vse64.v v12, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x double>, ptr %x
  %b = load <4 x double>, ptr %y
  %c = load <4 x double>, ptr %z
  %d = call <4 x double> @llvm.fma.v4f64(<4 x double> %a, <4 x double> %b, <4 x double> %c)
  store <4 x double> %d, ptr %x
  ret void
}

define void @fadd_vf_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fadd_vf_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fadd_vf_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fadd <8 x bfloat> %a, %c
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fadd_vf_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fadd_vf_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fadd_vf_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fadd <6 x bfloat> %a, %c
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fadd_vf_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fadd_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfadd.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fadd_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fadd <8 x half> %a, %c
  store <8 x half> %d, ptr %x
  ret void
}

define void @fadd_vf_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fadd_vf_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfadd.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fadd_vf_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fadd <6 x half> %a, %c
  store <6 x half> %d, ptr %x
  ret void
}

define void @fadd_vf_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fadd_vf_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfadd.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fadd <4 x float> %a, %c
  store <4 x float> %d, ptr %x
  ret void
}

define void @fadd_vf_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fadd_vf_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfadd.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fadd <2 x double> %a, %c
  store <2 x double> %d, ptr %x
  ret void
}

define void @fadd_fv_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fadd_fv_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fadd_fv_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fadd <8 x bfloat> %c, %a
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fadd_fv_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fadd_fv_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fadd.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fadd_fv_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fadd.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fadd <6 x bfloat> %c, %a
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fadd_fv_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fadd_fv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfadd.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fadd_fv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fadd <8 x half> %c, %a
  store <8 x half> %d, ptr %x
  ret void
}

define void @fadd_fv_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fadd_fv_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfadd.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fadd_fv_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fadd <6 x half> %c, %a
  store <6 x half> %d, ptr %x
  ret void
}

define void @fadd_fv_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fadd_fv_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfadd.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fadd <4 x float> %c, %a
  store <4 x float> %d, ptr %x
  ret void
}

define void @fadd_fv_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fadd_fv_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfadd.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fadd <2 x double> %c, %a
  store <2 x double> %d, ptr %x
  ret void
}

define void @fsub_vf_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fsub_vf_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fsub_vf_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fsub <8 x bfloat> %a, %c
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fsub_vf_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fsub_vf_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v10, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v10, 6
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    srli a0, a0, 16
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vslidedown.vi v9, v10, 7
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fsub_vf_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v10, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v10, 6
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 48
; RV64-NEXT:    srli a0, a0, 48
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vslidedown.vi v9, v10, 7
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fsub <6 x bfloat> %a, %c
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fsub_vf_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fsub_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfsub.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fsub_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fsub <8 x half> %a, %c
  store <8 x half> %d, ptr %x
  ret void
}

define void @fsub_vf_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fsub_vf_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfsub.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fsub_vf_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fsub <6 x half> %a, %c
  store <6 x half> %d, ptr %x
  ret void
}

define void @fsub_vf_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fsub_vf_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfsub.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fsub <4 x float> %a, %c
  store <4 x float> %d, ptr %x
  ret void
}

define void @fsub_vf_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fsub_vf_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfsub.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fsub <2 x double> %a, %c
  store <2 x double> %d, ptr %x
  ret void
}

define void @fsub_fv_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fsub_fv_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fsub_fv_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fsub <8 x bfloat> %c, %a
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fsub_fv_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fsub_fv_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fsub.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fsub_fv_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsub.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fsub <6 x bfloat> %c, %a
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fsub_fv_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fsub_fv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfrsub.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fsub_fv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fsub <8 x half> %c, %a
  store <8 x half> %d, ptr %x
  ret void
}

define void @fsub_fv_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fsub_fv_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfrsub.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fsub_fv_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fsub <6 x half> %c, %a
  store <6 x half> %d, ptr %x
  ret void
}

define void @fsub_fv_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fsub_fv_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfrsub.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fsub <4 x float> %c, %a
  store <4 x float> %d, ptr %x
  ret void
}

define void @fsub_fv_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fsub_fv_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfrsub.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fsub <2 x double> %c, %a
  store <2 x double> %d, ptr %x
  ret void
}

define void @fmul_vf_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fmul_vf_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmul_vf_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fmul <8 x bfloat> %a, %c
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fmul_vf_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fmul_vf_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmul_vf_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fmul <6 x bfloat> %a, %c
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fmul_vf_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fmul_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfmul.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmul_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fmul <8 x half> %a, %c
  store <8 x half> %d, ptr %x
  ret void
}

define void @fmul_vf_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fmul_vf_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfmul.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmul_vf_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fmul <6 x half> %a, %c
  store <6 x half> %d, ptr %x
  ret void
}

define void @fmul_vf_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fmul_vf_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfmul.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fmul <4 x float> %a, %c
  store <4 x float> %d, ptr %x
  ret void
}

define void @fmul_vf_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fmul_vf_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfmul.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fmul <2 x double> %a, %c
  store <2 x double> %d, ptr %x
  ret void
}

define void @fmul_fv_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fmul_fv_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmul_fv_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fmul <8 x bfloat> %c, %a
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fmul_fv_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fmul_fv_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmul.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmul_fv_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmul.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fmul <6 x bfloat> %c, %a
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fmul_fv_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fmul_fv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfmul.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmul_fv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fmul <8 x half> %c, %a
  store <8 x half> %d, ptr %x
  ret void
}

define void @fmul_fv_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fmul_fv_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfmul.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmul_fv_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fmul <6 x half> %c, %a
  store <6 x half> %d, ptr %x
  ret void
}

define void @fmul_fv_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fmul_fv_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfmul.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fmul <4 x float> %c, %a
  store <4 x float> %d, ptr %x
  ret void
}

define void @fmul_fv_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fmul_fv_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfmul.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fmul <2 x double> %c, %a
  store <2 x double> %d, ptr %x
  ret void
}

define void @fdiv_vf_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fdiv_vf_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fdiv_vf_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fdiv <8 x bfloat> %a, %c
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fdiv_vf_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fdiv_vf_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fa5, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fdiv_vf_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fa5, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fdiv <6 x bfloat> %a, %c
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fdiv_vf_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fdiv_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfdiv.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fdiv_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfdiv.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fdiv <8 x half> %a, %c
  store <8 x half> %d, ptr %x
  ret void
}

define void @fdiv_vf_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fdiv_vf_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfdiv.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fdiv_vf_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfdiv.vv v8, v10, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fdiv <6 x half> %a, %c
  store <6 x half> %d, ptr %x
  ret void
}

define void @fdiv_vf_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fdiv_vf_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfdiv.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fdiv <4 x float> %a, %c
  store <4 x float> %d, ptr %x
  ret void
}

define void @fdiv_vf_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fdiv_vf_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfdiv.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fdiv <2 x double> %a, %c
  store <2 x double> %d, ptr %x
  ret void
}

define void @fdiv_fv_v8bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fdiv_fv_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fdiv_fv_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = insertelement <8 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <8 x bfloat> %b, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %d = fdiv <8 x bfloat> %c, %a
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fdiv_fv_v6bf16(ptr %x, bfloat %y) {
; RV32-LABEL: fdiv_fv_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fdiv.s fa0, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fdiv_fv_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fdiv.s fa0, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = insertelement <6 x bfloat> poison, bfloat %y, i32 0
  %c = shufflevector <6 x bfloat> %b, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %d = fdiv <6 x bfloat> %c, %a
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fdiv_fv_v8f16(ptr %x, half %y) {
; ZVFH-LABEL: fdiv_fv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfrdiv.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fdiv_fv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfdiv.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = insertelement <8 x half> poison, half %y, i32 0
  %c = shufflevector <8 x half> %b, <8 x half> poison, <8 x i32> zeroinitializer
  %d = fdiv <8 x half> %c, %a
  store <8 x half> %d, ptr %x
  ret void
}

define void @fdiv_fv_v6f16(ptr %x, half %y) {
; ZVFH-LABEL: fdiv_fv_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vfrdiv.vf v8, v8, fa0
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fdiv_fv_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfdiv.vv v8, v12, v10
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = insertelement <6 x half> poison, half %y, i32 0
  %c = shufflevector <6 x half> %b, <6 x half> poison, <6 x i32> zeroinitializer
  %d = fdiv <6 x half> %c, %a
  store <6 x half> %d, ptr %x
  ret void
}

define void @fdiv_fv_v4f32(ptr %x, float %y) {
; CHECK-LABEL: fdiv_fv_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfrdiv.vf v8, v8, fa0
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = insertelement <4 x float> poison, float %y, i32 0
  %c = shufflevector <4 x float> %b, <4 x float> poison, <4 x i32> zeroinitializer
  %d = fdiv <4 x float> %c, %a
  store <4 x float> %d, ptr %x
  ret void
}

define void @fdiv_fv_v2f64(ptr %x, double %y) {
; CHECK-LABEL: fdiv_fv_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vfrdiv.vf v8, v8, fa0
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = insertelement <2 x double> poison, double %y, i32 0
  %c = shufflevector <2 x double> %b, <2 x double> poison, <2 x i32> zeroinitializer
  %d = fdiv <2 x double> %c, %a
  store <2 x double> %d, ptr %x
  ret void
}

define void @fma_vf_v8bf16(ptr %x, ptr %y, bfloat %z) {
; RV32-LABEL: fma_vf_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v9, (s0)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fma_vf_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v9, (s0)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = insertelement <8 x bfloat> poison, bfloat %z, i32 0
  %d = shufflevector <8 x bfloat> %c, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %e = call <8 x bfloat> @llvm.fma.v8bf16(<8 x bfloat> %a, <8 x bfloat> %d, <8 x bfloat> %b)
  store <8 x bfloat> %e, ptr %x
  ret void
}

define void @fma_vf_v6bf16(ptr %x, ptr %y, bfloat %z) {
; RV32-LABEL: fma_vf_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v9, (s0)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fma_vf_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v9, (s0)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = insertelement <6 x bfloat> poison, bfloat %z, i32 0
  %d = shufflevector <6 x bfloat> %c, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %e = call <6 x bfloat> @llvm.fma.v6bf16(<6 x bfloat> %a, <6 x bfloat> %d, <6 x bfloat> %b)
  store <6 x bfloat> %e, ptr %x
  ret void
}

define void @fma_vf_v8f16(ptr %x, ptr %y, half %z) {
; ZVFH-LABEL: fma_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmacc.vf v9, fa0, v8
; ZVFH-NEXT:    vse16.v v9, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fma_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v10, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = insertelement <8 x half> poison, half %z, i32 0
  %d = shufflevector <8 x half> %c, <8 x half> poison, <8 x i32> zeroinitializer
  %e = call <8 x half> @llvm.fma.v8f16(<8 x half> %a, <8 x half> %d, <8 x half> %b)
  store <8 x half> %e, ptr %x
  ret void
}

define void @fma_vf_v6f16(ptr %x, ptr %y, half %z) {
; ZVFH-LABEL: fma_vf_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmacc.vf v9, fa0, v8
; ZVFH-NEXT:    vse16.v v9, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fma_vf_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v10, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = insertelement <6 x half> poison, half %z, i32 0
  %d = shufflevector <6 x half> %c, <6 x half> poison, <6 x i32> zeroinitializer
  %e = call <6 x half> @llvm.fma.v6f16(<6 x half> %a, <6 x half> %d, <6 x half> %b)
  store <6 x half> %e, ptr %x
  ret void
}

define void @fma_vf_v4f32(ptr %x, ptr %y, float %z) {
; CHECK-LABEL: fma_vf_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfmacc.vf v9, fa0, v8
; CHECK-NEXT:    vse32.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = insertelement <4 x float> poison, float %z, i32 0
  %d = shufflevector <4 x float> %c, <4 x float> poison, <4 x i32> zeroinitializer
  %e = call <4 x float> @llvm.fma.v4f32(<4 x float> %a, <4 x float> %d, <4 x float> %b)
  store <4 x float> %e, ptr %x
  ret void
}

define void @fma_vf_v2f64(ptr %x, ptr %y, double %z) {
; CHECK-LABEL: fma_vf_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfmacc.vf v9, fa0, v8
; CHECK-NEXT:    vse64.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = insertelement <2 x double> poison, double %z, i32 0
  %d = shufflevector <2 x double> %c, <2 x double> poison, <2 x i32> zeroinitializer
  %e = call <2 x double> @llvm.fma.v2f64(<2 x double> %a, <2 x double> %d, <2 x double> %b)
  store <2 x double> %e, ptr %x
  ret void
}

define void @fma_fv_v8bf16(ptr %x, ptr %y, bfloat %z) {
; RV32-LABEL: fma_fv_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v9, (s0)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fma_fv_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v9, (s0)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = insertelement <8 x bfloat> poison, bfloat %z, i32 0
  %d = shufflevector <8 x bfloat> %c, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %e = call <8 x bfloat> @llvm.fma.v8bf16(<8 x bfloat> %d, <8 x bfloat> %a, <8 x bfloat> %b)
  store <8 x bfloat> %e, ptr %x
  ret void
}

define void @fma_fv_v6bf16(ptr %x, ptr %y, bfloat %z) {
; RV32-LABEL: fma_fv_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v9, (s0)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fma_fv_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a2, a2, 2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x04, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 4 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v9, (s0)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fs0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = insertelement <6 x bfloat> poison, bfloat %z, i32 0
  %d = shufflevector <6 x bfloat> %c, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %e = call <6 x bfloat> @llvm.fma.v6bf16(<6 x bfloat> %d, <6 x bfloat> %a, <6 x bfloat> %b)
  store <6 x bfloat> %e, ptr %x
  ret void
}

define void @fma_fv_v8f16(ptr %x, ptr %y, half %z) {
; ZVFH-LABEL: fma_fv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmacc.vf v9, fa0, v8
; ZVFH-NEXT:    vse16.v v9, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fma_fv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vmv.v.x v10, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = insertelement <8 x half> poison, half %z, i32 0
  %d = shufflevector <8 x half> %c, <8 x half> poison, <8 x i32> zeroinitializer
  %e = call <8 x half> @llvm.fma.v8f16(<8 x half> %d, <8 x half> %a, <8 x half> %b)
  store <8 x half> %e, ptr %x
  ret void
}

define void @fma_fv_v6f16(ptr %x, ptr %y, half %z) {
; ZVFH-LABEL: fma_fv_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmacc.vf v9, fa0, v8
; ZVFH-NEXT:    vse16.v v9, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fma_fv_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    fmv.x.w a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v10, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = insertelement <6 x half> poison, half %z, i32 0
  %d = shufflevector <6 x half> %c, <6 x half> poison, <6 x i32> zeroinitializer
  %e = call <6 x half> @llvm.fma.v6f16(<6 x half> %d, <6 x half> %a, <6 x half> %b)
  store <6 x half> %e, ptr %x
  ret void
}

define void @fma_fv_v4f32(ptr %x, ptr %y, float %z) {
; CHECK-LABEL: fma_fv_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfmacc.vf v9, fa0, v8
; CHECK-NEXT:    vse32.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = insertelement <4 x float> poison, float %z, i32 0
  %d = shufflevector <4 x float> %c, <4 x float> poison, <4 x i32> zeroinitializer
  %e = call <4 x float> @llvm.fma.v4f32(<4 x float> %d, <4 x float> %a, <4 x float> %b)
  store <4 x float> %e, ptr %x
  ret void
}

define void @fma_fv_v2f64(ptr %x, ptr %y, double %z) {
; CHECK-LABEL: fma_fv_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfmacc.vf v9, fa0, v8
; CHECK-NEXT:    vse64.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = insertelement <2 x double> poison, double %z, i32 0
  %d = shufflevector <2 x double> %c, <2 x double> poison, <2 x i32> zeroinitializer
  %e = call <2 x double> @llvm.fma.v2f64(<2 x double> %d, <2 x double> %a, <2 x double> %b)
  store <2 x double> %e, ptr %x
  ret void
}

define void @fmsub_vf_v8bf16(ptr %x, ptr %y, bfloat %z) {
; RV32-LABEL: fmsub_vf_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -80
; RV32-NEXT:    .cfi_def_cfa_offset 80
; RV32-NEXT:    sw ra, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 72(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 68(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 64(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset fs0, -56
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a3, a2, 1
; RV32-NEXT:    add a2, a3, a2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xd0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 80 + 3 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v9, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s6, 524288
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s3, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s7, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s9, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    slli s5, s5, 16
; RV32-NEXT:    fmv.w.x fs0, s5
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    slli s9, s9, 16
; RV32-NEXT:    fmv.w.x fa5, s9
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s5
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s8, s8, 16
; RV32-NEXT:    fmv.w.x fa5, s8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s7, s7, 16
; RV32-NEXT:    fmv.w.x fa5, s7
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s4, s4, 16
; RV32-NEXT:    fmv.w.x fa5, s4
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    slli s3, s3, 16
; RV32-NEXT:    fmv.w.x fa5, s3
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s4
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s2, s2, 16
; RV32-NEXT:    fmv.w.x fa5, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s1, s1, 16
; RV32-NEXT:    fmv.w.x fa5, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 72(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 68(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 64(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 80
; RV32-NEXT:    ret
;
; RV64-LABEL: fmsub_vf_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -144
; RV64-NEXT:    .cfi_def_cfa_offset 144
; RV64-NEXT:    sd ra, 136(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 128(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 104(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 96(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s6, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s7, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s8, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs3, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs4, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs5, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    .cfi_offset s6, -64
; RV64-NEXT:    .cfi_offset s7, -72
; RV64-NEXT:    .cfi_offset s8, -80
; RV64-NEXT:    .cfi_offset fs0, -88
; RV64-NEXT:    .cfi_offset fs1, -96
; RV64-NEXT:    .cfi_offset fs2, -104
; RV64-NEXT:    .cfi_offset fs3, -112
; RV64-NEXT:    .cfi_offset fs4, -120
; RV64-NEXT:    .cfi_offset fs5, -128
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a3, a2, 1
; RV64-NEXT:    add a2, a3, a2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0x90, 0x01, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 144 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs2, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs3, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.x.w s2, fs5
; RV64-NEXT:    fmv.x.w s3, fs4
; RV64-NEXT:    fmv.x.w s5, fs3
; RV64-NEXT:    fmv.x.w s6, fs2
; RV64-NEXT:    fmv.x.w s7, fs1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    fmv.x.w s8, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli s4, s4, 16
; RV64-NEXT:    fmv.w.x fs0, s4
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa5, a1
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s8, s8, 16
; RV64-NEXT:    fmv.w.x fa4, s8
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s7, s7, 16
; RV64-NEXT:    fmv.w.x fa4, s7
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s6, s6, 16
; RV64-NEXT:    fmv.w.x fa4, s6
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s5, s5, 16
; RV64-NEXT:    fmv.w.x fa4, s5
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s3, s3, 16
; RV64-NEXT:    fmv.w.x fa4, s3
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s2, s2, 16
; RV64-NEXT:    fmv.w.x fa4, s2
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s1, s1, 16
; RV64-NEXT:    fmv.w.x fa4, s1
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 136(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 128(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 104(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 96(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s6, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s7, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s8, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs3, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs4, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs5, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 144
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = insertelement <8 x bfloat> poison, bfloat %z, i32 0
  %d = shufflevector <8 x bfloat> %c, <8 x bfloat> poison, <8 x i32> zeroinitializer
  %neg = fneg <8 x bfloat> %b
  %e = call <8 x bfloat> @llvm.fma.v8bf16(<8 x bfloat> %a, <8 x bfloat> %d, <8 x bfloat> %neg)
  store <8 x bfloat> %e, ptr %x
  ret void
}

define void @fmsub_vf_v6bf16(ptr %x, ptr %y, bfloat %z) {
; RV32-LABEL: fmsub_vf_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -80
; RV32-NEXT:    .cfi_def_cfa_offset 80
; RV32-NEXT:    sw ra, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 72(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 68(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 64(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset fs0, -56
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a3, a2, 1
; RV32-NEXT:    add a2, a3, a2
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xd0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 80 + 3 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v9, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vs1r.v v9, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v9, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    lui s6, 524288
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s3, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s7, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s9, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    xor a0, a0, s6
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    slli s5, s5, 16
; RV32-NEXT:    fmv.w.x fs0, s5
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s5, fa0
; RV32-NEXT:    slli s9, s9, 16
; RV32-NEXT:    fmv.w.x fa5, s9
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s5
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s8, s8, 16
; RV32-NEXT:    fmv.w.x fa5, s8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s7, s7, 16
; RV32-NEXT:    fmv.w.x fa5, s7
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s4, s4, 16
; RV32-NEXT:    fmv.w.x fa5, s4
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s4, fa0
; RV32-NEXT:    slli s3, s3, 16
; RV32-NEXT:    fmv.w.x fa5, s3
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s4
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s2, s2, 16
; RV32-NEXT:    fmv.w.x fa5, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmv.w.x fs0, zero
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    slli s1, s1, 16
; RV32-NEXT:    fmv.w.x fa5, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmadd.s fa0, fa4, fs0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 72(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 68(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 64(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 80
; RV32-NEXT:    ret
;
; RV64-LABEL: fmsub_vf_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -144
; RV64-NEXT:    .cfi_def_cfa_offset 144
; RV64-NEXT:    sd ra, 136(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 128(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 104(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 96(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s6, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s7, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s8, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs3, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs4, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs5, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset s3, -40
; RV64-NEXT:    .cfi_offset s4, -48
; RV64-NEXT:    .cfi_offset s5, -56
; RV64-NEXT:    .cfi_offset s6, -64
; RV64-NEXT:    .cfi_offset s7, -72
; RV64-NEXT:    .cfi_offset s8, -80
; RV64-NEXT:    .cfi_offset fs0, -88
; RV64-NEXT:    .cfi_offset fs1, -96
; RV64-NEXT:    .cfi_offset fs2, -104
; RV64-NEXT:    .cfi_offset fs3, -112
; RV64-NEXT:    .cfi_offset fs4, -120
; RV64-NEXT:    .cfi_offset fs5, -128
; RV64-NEXT:    csrr a2, vlenb
; RV64-NEXT:    slli a3, a2, 1
; RV64-NEXT:    add a2, a3, a2
; RV64-NEXT:    sub sp, sp, a2
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0x90, 0x01, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 144 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a1)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs2, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs3, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    fmv.x.w s2, fs5
; RV64-NEXT:    fmv.x.w s3, fs4
; RV64-NEXT:    fmv.x.w s5, fs3
; RV64-NEXT:    fmv.x.w s6, fs2
; RV64-NEXT:    fmv.x.w s7, fs1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fneg.s fa0, fa5
; RV64-NEXT:    fmv.x.w s8, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli s4, s4, 16
; RV64-NEXT:    fmv.w.x fs0, s4
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa5, a1
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s8, s8, 16
; RV64-NEXT:    fmv.w.x fa4, s8
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s7, s7, 16
; RV64-NEXT:    fmv.w.x fa4, s7
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s6, s6, 16
; RV64-NEXT:    fmv.w.x fa4, s6
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s5, s5, 16
; RV64-NEXT:    fmv.w.x fa4, s5
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s4, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s3, s3, 16
; RV64-NEXT:    fmv.w.x fa4, s3
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s4
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s2, s2, 16
; RV64-NEXT:    fmv.w.x fa4, s2
; RV64-NEXT:    fmv.w.x fs0, zero
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    slli s1, s1, 16
; RV64-NEXT:    fmv.w.x fa4, s1
; RV64-NEXT:    fmadd.s fa0, fa5, fs0, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 136(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 128(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 104(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 96(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s6, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s7, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s8, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs3, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs4, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs5, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 144
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = insertelement <6 x bfloat> poison, bfloat %z, i32 0
  %d = shufflevector <6 x bfloat> %c, <6 x bfloat> poison, <6 x i32> zeroinitializer
  %neg = fneg <6 x bfloat> %b
  %e = call <6 x bfloat> @llvm.fma.v6bf16(<6 x bfloat> %a, <6 x bfloat> %d, <6 x bfloat> %neg)
  store <6 x bfloat> %e, ptr %x
  ret void
}

define void @fmsub_vf_v8f16(ptr %x, ptr %y, half %z) {
; ZVFH-LABEL: fmsub_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmsac.vf v9, fa0, v8
; ZVFH-NEXT:    vse16.v v9, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmsub_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.w a2, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vmv.v.x v10, a2
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = insertelement <8 x half> poison, half %z, i32 0
  %d = shufflevector <8 x half> %c, <8 x half> poison, <8 x i32> zeroinitializer
  %neg = fneg <8 x half> %b
  %e = call <8 x half> @llvm.fma.v8f16(<8 x half> %a, <8 x half> %d, <8 x half> %neg)
  store <8 x half> %e, ptr %x
  ret void
}

define void @fmsub_vf_v6f16(ptr %x, ptr %y, half %z) {
; ZVFH-LABEL: fmsub_vf_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vfmsac.vf v9, fa0, v8
; ZVFH-NEXT:    vse16.v v9, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmsub_vf_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.w a2, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v10, a2
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    vxor.vx v8, v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmadd.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = insertelement <6 x half> poison, half %z, i32 0
  %d = shufflevector <6 x half> %c, <6 x half> poison, <6 x i32> zeroinitializer
  %neg = fneg <6 x half> %b
  %e = call <6 x half> @llvm.fma.v6f16(<6 x half> %a, <6 x half> %d, <6 x half> %neg)
  store <6 x half> %e, ptr %x
  ret void
}

define void @fnmsub_vf_v4f32(ptr %x, ptr %y, float %z) {
; CHECK-LABEL: fnmsub_vf_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfnmsac.vf v9, fa0, v8
; CHECK-NEXT:    vse32.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = insertelement <4 x float> poison, float %z, i32 0
  %d = shufflevector <4 x float> %c, <4 x float> poison, <4 x i32> zeroinitializer
  %neg = fneg <4 x float> %a
  %e = call <4 x float> @llvm.fma.v4f32(<4 x float> %neg, <4 x float> %d, <4 x float> %b)
  store <4 x float> %e, ptr %x
  ret void
}

define void @fnmadd_vf_v2f64(ptr %x, ptr %y, double %z) {
; CHECK-LABEL: fnmadd_vf_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfnmacc.vf v9, fa0, v8
; CHECK-NEXT:    vse64.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = insertelement <2 x double> poison, double %z, i32 0
  %d = shufflevector <2 x double> %c, <2 x double> poison, <2 x i32> zeroinitializer
  %neg = fneg <2 x double> %a
  %neg2 = fneg <2 x double> %b
  %e = call <2 x double> @llvm.fma.v2f64(<2 x double> %neg, <2 x double> %d, <2 x double> %neg2)
  store <2 x double> %e, ptr %x
  ret void
}

define void @fnmsub_fv_v4f32(ptr %x, ptr %y, float %z) {
; CHECK-LABEL: fnmsub_fv_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vfnmsac.vf v9, fa0, v8
; CHECK-NEXT:    vse32.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = insertelement <4 x float> poison, float %z, i32 0
  %d = shufflevector <4 x float> %c, <4 x float> poison, <4 x i32> zeroinitializer
  %neg = fneg <4 x float> %d
  %e = call <4 x float> @llvm.fma.v4f32(<4 x float> %neg, <4 x float> %a, <4 x float> %b)
  store <4 x float> %e, ptr %x
  ret void
}

define void @fnmadd_fv_v2f64(ptr %x, ptr %y, double %z) {
; CHECK-LABEL: fnmadd_fv_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vfnmacc.vf v9, fa0, v8
; CHECK-NEXT:    vse64.v v9, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = insertelement <2 x double> poison, double %z, i32 0
  %d = shufflevector <2 x double> %c, <2 x double> poison, <2 x i32> zeroinitializer
  %neg = fneg <2 x double> %d
  %neg2 = fneg <2 x double> %b
  %e = call <2 x double> @llvm.fma.v2f64(<2 x double> %neg, <2 x double> %a, <2 x double> %neg2)
  store <2 x double> %e, ptr %x
  ret void
}

define void @trunc_v8bf16(ptr %x) {
; RV32-LABEL: trunc_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 40(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 32(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    .cfi_offset fs1, -32
; RV32-NEXT:    .cfi_offset fs2, -40
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs2, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    beqz a0, .LBB169_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB169_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB169_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa5, rtz
; RV32-NEXT:    fcvt.s.w fa4, a0, rtz
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB169_4:
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    beqz a0, .LBB169_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa5, rtz
; RV32-NEXT:    fcvt.s.w fa4, a0, rtz
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB169_6:
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs1, fa0
; RV32-NEXT:    beqz a0, .LBB169_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa5, rtz
; RV32-NEXT:    fcvt.s.w fa4, a0, rtz
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB169_8:
; RV32-NEXT:    fmv.x.w s1, fs0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB169_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa5, rtz
; RV32-NEXT:    fcvt.s.w fa4, a0, rtz
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB169_10:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs1
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB169_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB169_12:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 6
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB169_14
; RV32-NEXT:  # %bb.13:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB169_14:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs2
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB169_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB169_16:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 40(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 32(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: trunc_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset fs0, -40
; RV64-NEXT:    .cfi_offset fs1, -48
; RV64-NEXT:    .cfi_offset fs2, -56
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xe0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 96 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs2, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    beqz a0, .LBB169_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB169_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB169_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa5, rtz
; RV64-NEXT:    fcvt.s.w fa4, a0, rtz
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB169_4:
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    beqz a0, .LBB169_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa5, rtz
; RV64-NEXT:    fcvt.s.w fa4, a0, rtz
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB169_6:
; RV64-NEXT:    fmv.x.w s2, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    beqz a0, .LBB169_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa5, rtz
; RV64-NEXT:    fcvt.s.w fa4, a0, rtz
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB169_8:
; RV64-NEXT:    fmv.x.w s1, fs0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB169_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa5, rtz
; RV64-NEXT:    fcvt.s.w fa4, a0, rtz
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB169_10:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs1
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB169_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB169_12:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 6
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB169_14
; RV64-NEXT:  # %bb.13:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB169_14:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs2
; RV64-NEXT:    addi a2, sp, 32
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB169_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB169_16:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.trunc.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @trunc_v6bf16(ptr %x) {
; RV32-LABEL: trunc_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    beqz a0, .LBB170_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB170_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs0
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB170_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB170_4:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 2
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB170_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB170_6:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB170_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB170_8:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB170_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB170_10:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB170_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    fcvt.s.w fa5, a0, rtz
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB170_12:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: trunc_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    beqz a0, .LBB170_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB170_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs0
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB170_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB170_4:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 2
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB170_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB170_6:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB170_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB170_8:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB170_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB170_10:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB170_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    fcvt.s.w fa5, a0, rtz
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB170_12:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = call <6 x bfloat> @llvm.trunc.v6bf16(<6 x bfloat> %a)
  store <6 x bfloat> %b, ptr %x
  ret void
}

define void @trunc_v8f16(ptr %x) {
; ZVFH-LABEL: trunc_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI171_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI171_0)(a1)
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    vfcvt.rtz.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: trunc_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    vfcvt.rtz.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.trunc.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @trunc_v6f16(ptr %x) {
; ZVFH-LABEL: trunc_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI172_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI172_0)(a1)
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    vfcvt.rtz.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: trunc_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    vfcvt.rtz.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = call <6 x half> @llvm.trunc.v6f16(<6 x half> %a)
  store <6 x half> %b, ptr %x
  ret void
}

define void @trunc_v4f32(ptr %x) {
; CHECK-LABEL: trunc_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    lui a1, 307200
; CHECK-NEXT:    fmv.w.x fa5, a1
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    vfcvt.rtz.x.f.v v9, v8, v0.t
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.trunc.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @trunc_v2f64(ptr %x) {
; CHECK-LABEL: trunc_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    lui a1, %hi(.LCPI174_0)
; CHECK-NEXT:    fld fa5, %lo(.LCPI174_0)(a1)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    vfcvt.rtz.x.f.v v9, v8, v0.t
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.trunc.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @ceil_v8bf16(ptr %x) {
; RV32-LABEL: ceil_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 40(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 32(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    .cfi_offset fs1, -32
; RV32-NEXT:    .cfi_offset fs2, -40
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs2, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    beqz a0, .LBB175_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB175_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB175_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa5, rup
; RV32-NEXT:    fcvt.s.w fa4, a0, rup
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB175_4:
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    beqz a0, .LBB175_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa5, rup
; RV32-NEXT:    fcvt.s.w fa4, a0, rup
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB175_6:
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs1, fa0
; RV32-NEXT:    beqz a0, .LBB175_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa5, rup
; RV32-NEXT:    fcvt.s.w fa4, a0, rup
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB175_8:
; RV32-NEXT:    fmv.x.w s1, fs0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB175_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa5, rup
; RV32-NEXT:    fcvt.s.w fa4, a0, rup
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB175_10:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs1
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB175_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB175_12:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 6
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB175_14
; RV32-NEXT:  # %bb.13:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB175_14:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs2
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB175_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB175_16:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 40(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 32(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: ceil_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset fs0, -40
; RV64-NEXT:    .cfi_offset fs1, -48
; RV64-NEXT:    .cfi_offset fs2, -56
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xe0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 96 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs2, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    beqz a0, .LBB175_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB175_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB175_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa5, rup
; RV64-NEXT:    fcvt.s.w fa4, a0, rup
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB175_4:
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    beqz a0, .LBB175_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa5, rup
; RV64-NEXT:    fcvt.s.w fa4, a0, rup
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB175_6:
; RV64-NEXT:    fmv.x.w s2, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    beqz a0, .LBB175_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa5, rup
; RV64-NEXT:    fcvt.s.w fa4, a0, rup
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB175_8:
; RV64-NEXT:    fmv.x.w s1, fs0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB175_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa5, rup
; RV64-NEXT:    fcvt.s.w fa4, a0, rup
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB175_10:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs1
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB175_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB175_12:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 6
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB175_14
; RV64-NEXT:  # %bb.13:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB175_14:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs2
; RV64-NEXT:    addi a2, sp, 32
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB175_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB175_16:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.ceil.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @ceil_v6bf16(ptr %x) {
; RV32-LABEL: ceil_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    beqz a0, .LBB176_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB176_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs0
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB176_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB176_4:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 2
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB176_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB176_6:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB176_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB176_8:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB176_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB176_10:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB176_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rup
; RV32-NEXT:    fcvt.s.w fa5, a0, rup
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB176_12:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: ceil_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    beqz a0, .LBB176_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB176_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs0
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB176_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB176_4:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 2
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB176_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB176_6:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB176_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB176_8:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB176_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB176_10:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB176_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rup
; RV64-NEXT:    fcvt.s.w fa5, a0, rup
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB176_12:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = call <6 x bfloat> @llvm.ceil.v6bf16(<6 x bfloat> %a)
  store <6 x bfloat> %b, ptr %x
  ret void
}

define void @ceil_v8f16(ptr %x) {
; ZVFH-LABEL: ceil_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI177_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI177_0)(a1)
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    fsrmi a1, 3
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    fsrm a1
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: ceil_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    fsrmi a1, 3
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    fsrm a1
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.ceil.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @ceil_v6f16(ptr %x) {
; ZVFH-LABEL: ceil_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI178_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI178_0)(a1)
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    fsrmi a1, 3
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    fsrm a1
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: ceil_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    fsrmi a1, 3
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    fsrm a1
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = call <6 x half> @llvm.ceil.v6f16(<6 x half> %a)
  store <6 x half> %b, ptr %x
  ret void
}

define void @ceil_v4f32(ptr %x) {
; CHECK-LABEL: ceil_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    lui a1, 307200
; CHECK-NEXT:    fmv.w.x fa5, a1
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    fsrmi a1, 3
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    fsrm a1
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.ceil.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @ceil_v2f64(ptr %x) {
; CHECK-LABEL: ceil_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    lui a1, %hi(.LCPI180_0)
; CHECK-NEXT:    fld fa5, %lo(.LCPI180_0)(a1)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    fsrmi a1, 3
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    fsrm a1
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.ceil.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @floor_v8bf16(ptr %x) {
; RV32-LABEL: floor_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 40(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 32(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    .cfi_offset fs1, -32
; RV32-NEXT:    .cfi_offset fs2, -40
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs2, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    beqz a0, .LBB181_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB181_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB181_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa5, rdn
; RV32-NEXT:    fcvt.s.w fa4, a0, rdn
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB181_4:
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    beqz a0, .LBB181_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa5, rdn
; RV32-NEXT:    fcvt.s.w fa4, a0, rdn
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB181_6:
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs1, fa0
; RV32-NEXT:    beqz a0, .LBB181_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa5, rdn
; RV32-NEXT:    fcvt.s.w fa4, a0, rdn
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB181_8:
; RV32-NEXT:    fmv.x.w s1, fs0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB181_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa5, rdn
; RV32-NEXT:    fcvt.s.w fa4, a0, rdn
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB181_10:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs1
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB181_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB181_12:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 6
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB181_14
; RV32-NEXT:  # %bb.13:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB181_14:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs2
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB181_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB181_16:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 40(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 32(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: floor_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset fs0, -40
; RV64-NEXT:    .cfi_offset fs1, -48
; RV64-NEXT:    .cfi_offset fs2, -56
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xe0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 96 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs2, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    beqz a0, .LBB181_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB181_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB181_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa5, rdn
; RV64-NEXT:    fcvt.s.w fa4, a0, rdn
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB181_4:
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    beqz a0, .LBB181_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa5, rdn
; RV64-NEXT:    fcvt.s.w fa4, a0, rdn
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB181_6:
; RV64-NEXT:    fmv.x.w s2, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    beqz a0, .LBB181_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa5, rdn
; RV64-NEXT:    fcvt.s.w fa4, a0, rdn
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB181_8:
; RV64-NEXT:    fmv.x.w s1, fs0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB181_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa5, rdn
; RV64-NEXT:    fcvt.s.w fa4, a0, rdn
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB181_10:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs1
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB181_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB181_12:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 6
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB181_14
; RV64-NEXT:  # %bb.13:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB181_14:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs2
; RV64-NEXT:    addi a2, sp, 32
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB181_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB181_16:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.floor.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @floor_v6bf16(ptr %x) {
; RV32-LABEL: floor_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    beqz a0, .LBB182_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB182_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs0
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB182_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB182_4:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 2
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB182_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB182_6:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB182_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB182_8:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB182_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB182_10:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB182_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rdn
; RV32-NEXT:    fcvt.s.w fa5, a0, rdn
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB182_12:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: floor_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    beqz a0, .LBB182_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB182_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs0
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB182_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB182_4:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 2
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB182_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB182_6:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB182_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB182_8:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB182_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB182_10:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB182_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rdn
; RV64-NEXT:    fcvt.s.w fa5, a0, rdn
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB182_12:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = call <6 x bfloat> @llvm.floor.v6bf16(<6 x bfloat> %a)
  store <6 x bfloat> %b, ptr %x
  ret void
}

define void @floor_v8f16(ptr %x) {
; ZVFH-LABEL: floor_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI183_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI183_0)(a1)
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    fsrmi a1, 2
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    fsrm a1
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: floor_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    fsrmi a1, 2
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    fsrm a1
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.floor.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @floor_v6f16(ptr %x) {
; ZVFH-LABEL: floor_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI184_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI184_0)(a1)
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    fsrmi a1, 2
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    fsrm a1
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: floor_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    fsrmi a1, 2
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    fsrm a1
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = call <6 x half> @llvm.floor.v6f16(<6 x half> %a)
  store <6 x half> %b, ptr %x
  ret void
}

define void @floor_v4f32(ptr %x) {
; CHECK-LABEL: floor_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    lui a1, 307200
; CHECK-NEXT:    fmv.w.x fa5, a1
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    fsrmi a1, 2
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    fsrm a1
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.floor.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @floor_v2f64(ptr %x) {
; CHECK-LABEL: floor_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    lui a1, %hi(.LCPI186_0)
; CHECK-NEXT:    fld fa5, %lo(.LCPI186_0)(a1)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    fsrmi a1, 2
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    fsrm a1
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.floor.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @round_v8bf16(ptr %x) {
; RV32-LABEL: round_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 40(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 32(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    .cfi_offset fs1, -32
; RV32-NEXT:    .cfi_offset fs2, -40
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs2, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    beqz a0, .LBB187_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB187_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB187_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa5, rmm
; RV32-NEXT:    fcvt.s.w fa4, a0, rmm
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB187_4:
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    beqz a0, .LBB187_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa5, rmm
; RV32-NEXT:    fcvt.s.w fa4, a0, rmm
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB187_6:
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs1, fa0
; RV32-NEXT:    beqz a0, .LBB187_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa5, rmm
; RV32-NEXT:    fcvt.s.w fa4, a0, rmm
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB187_8:
; RV32-NEXT:    fmv.x.w s1, fs0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB187_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa5, rmm
; RV32-NEXT:    fcvt.s.w fa4, a0, rmm
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB187_10:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs1
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB187_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB187_12:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 6
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB187_14
; RV32-NEXT:  # %bb.13:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB187_14:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs2
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB187_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB187_16:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 40(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 32(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: round_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset fs0, -40
; RV64-NEXT:    .cfi_offset fs1, -48
; RV64-NEXT:    .cfi_offset fs2, -56
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xe0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 96 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs2, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    beqz a0, .LBB187_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB187_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB187_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa5, rmm
; RV64-NEXT:    fcvt.s.w fa4, a0, rmm
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB187_4:
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    beqz a0, .LBB187_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa5, rmm
; RV64-NEXT:    fcvt.s.w fa4, a0, rmm
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB187_6:
; RV64-NEXT:    fmv.x.w s2, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    beqz a0, .LBB187_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa5, rmm
; RV64-NEXT:    fcvt.s.w fa4, a0, rmm
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB187_8:
; RV64-NEXT:    fmv.x.w s1, fs0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB187_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa5, rmm
; RV64-NEXT:    fcvt.s.w fa4, a0, rmm
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB187_10:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs1
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB187_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB187_12:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 6
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB187_14
; RV64-NEXT:  # %bb.13:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB187_14:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs2
; RV64-NEXT:    addi a2, sp, 32
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB187_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB187_16:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.round.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @round_v6bf16(ptr %x) {
; RV32-LABEL: round_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    beqz a0, .LBB188_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB188_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs0
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB188_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB188_4:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 2
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB188_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB188_6:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB188_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB188_8:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB188_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB188_10:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs0
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB188_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0, rmm
; RV32-NEXT:    fcvt.s.w fa5, a0, rmm
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB188_12:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: round_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x02, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 2 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    beqz a0, .LBB188_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB188_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs0
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB188_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB188_4:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 2
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB188_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB188_6:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB188_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB188_8:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB188_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB188_10:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs0
; RV64-NEXT:    addi a2, sp, 16
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB188_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0, rmm
; RV64-NEXT:    fcvt.s.w fa5, a0, rmm
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB188_12:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = call <6 x bfloat> @llvm.round.v6bf16(<6 x bfloat> %a)
  store <6 x bfloat> %b, ptr %x
  ret void
}

define void @round_v8f16(ptr %x) {
; ZVFH-LABEL: round_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI189_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI189_0)(a1)
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    fsrmi a1, 4
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    fsrm a1
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: round_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    fsrmi a1, 4
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    fsrm a1
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.round.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @round_v6f16(ptr %x) {
; ZVFH-LABEL: round_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI190_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI190_0)(a1)
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    fsrmi a1, 4
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    fsrm a1
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: round_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    fsrmi a1, 4
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    fsrm a1
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = call <6 x half> @llvm.round.v6f16(<6 x half> %a)
  store <6 x half> %b, ptr %x
  ret void
}

define void @round_v4f32(ptr %x) {
; CHECK-LABEL: round_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    lui a1, 307200
; CHECK-NEXT:    fmv.w.x fa5, a1
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    fsrmi a1, 4
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    fsrm a1
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.round.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @round_v2f64(ptr %x) {
; CHECK-LABEL: round_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    lui a1, %hi(.LCPI192_0)
; CHECK-NEXT:    fld fa5, %lo(.LCPI192_0)(a1)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    fsrmi a1, 4
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    fsrm a1
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.round.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @rint_v8bf16(ptr %x) {
; RV32-LABEL: rint_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 52(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 48(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 40(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 32(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    .cfi_offset fs1, -32
; RV32-NEXT:    .cfi_offset fs2, -40
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    lui a0, 307200
; RV32-NEXT:    fmv.w.x fs2, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    beqz a0, .LBB193_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, fa0
; RV32-NEXT:    fcvt.s.w fa5, a0
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB193_2:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB193_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, fa5
; RV32-NEXT:    fcvt.s.w fa4, a0
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB193_4:
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    beqz a0, .LBB193_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    fcvt.w.s a0, fa5
; RV32-NEXT:    fcvt.s.w fa4, a0
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB193_6:
; RV32-NEXT:    fmv.x.w s2, fa0
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs1, fa0
; RV32-NEXT:    beqz a0, .LBB193_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    fcvt.w.s a0, fa5
; RV32-NEXT:    fcvt.s.w fa4, a0
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB193_8:
; RV32-NEXT:    fmv.x.w s1, fs0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    fabs.s fa4, fa5
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    beqz a0, .LBB193_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    fcvt.w.s a0, fa5
; RV32-NEXT:    fcvt.s.w fa4, a0
; RV32-NEXT:    fsgnj.s fa5, fa4, fa5
; RV32-NEXT:  .LBB193_10:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs1
; RV32-NEXT:    fmv.s fa0, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fa5, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa4, fa0
; RV32-NEXT:    flt.s a0, fa4, fs2
; RV32-NEXT:    fmv.x.w s1, fa5
; RV32-NEXT:    beqz a0, .LBB193_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    fcvt.w.s a0, fa0
; RV32-NEXT:    fcvt.s.w fa5, a0
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB193_12:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    fmv.x.w s2, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 6
; RV32-NEXT:    vmv.x.s a0, v9
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a0, fa5, fs2
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a0, .LBB193_14
; RV32-NEXT:  # %bb.13:
; RV32-NEXT:    fcvt.w.s a0, fa0
; RV32-NEXT:    fcvt.s.w fa5, a0
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB193_14:
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, s2
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a1, v8
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    fmv.w.x fa0, a1
; RV32-NEXT:    fabs.s fa5, fa0
; RV32-NEXT:    flt.s a1, fa5, fs2
; RV32-NEXT:    addi a2, sp, 16
; RV32-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    beqz a1, .LBB193_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    fcvt.w.s a0, fa0
; RV32-NEXT:    fcvt.s.w fa5, a0
; RV32-NEXT:    fsgnj.s fa0, fa5, fa0
; RV32-NEXT:  .LBB193_16:
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 52(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 48(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 40(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 32(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: rint_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 72(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 64(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs1, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs2, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset s2, -32
; RV64-NEXT:    .cfi_offset fs0, -40
; RV64-NEXT:    .cfi_offset fs1, -48
; RV64-NEXT:    .cfi_offset fs2, -56
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xe0, 0x00, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 96 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    lui a0, 307200
; RV64-NEXT:    fmv.w.x fs2, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    beqz a0, .LBB193_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, fa0
; RV64-NEXT:    fcvt.s.w fa5, a0
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB193_2:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB193_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, fa5
; RV64-NEXT:    fcvt.s.w fa4, a0
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB193_4:
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    beqz a0, .LBB193_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    fcvt.w.s a0, fa5
; RV64-NEXT:    fcvt.s.w fa4, a0
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB193_6:
; RV64-NEXT:    fmv.x.w s2, fa0
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs1, fa0
; RV64-NEXT:    beqz a0, .LBB193_8
; RV64-NEXT:  # %bb.7:
; RV64-NEXT:    fcvt.w.s a0, fa5
; RV64-NEXT:    fcvt.s.w fa4, a0
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB193_8:
; RV64-NEXT:    fmv.x.w s1, fs0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fabs.s fa4, fa5
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    beqz a0, .LBB193_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    fcvt.w.s a0, fa5
; RV64-NEXT:    fcvt.s.w fa4, a0
; RV64-NEXT:    fsgnj.s fa5, fa4, fa5
; RV64-NEXT:  .LBB193_10:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs1
; RV64-NEXT:    fmv.s fa0, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fa5, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa4, fa0
; RV64-NEXT:    flt.s a0, fa4, fs2
; RV64-NEXT:    fmv.x.w s1, fa5
; RV64-NEXT:    beqz a0, .LBB193_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    fcvt.w.s a0, fa0
; RV64-NEXT:    fcvt.s.w fa5, a0
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB193_12:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    fmv.x.w s2, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 6
; RV64-NEXT:    vmv.x.s a0, v9
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a0, fa5, fs2
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vs1r.v v8, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a0, .LBB193_14
; RV64-NEXT:  # %bb.13:
; RV64-NEXT:    fcvt.w.s a0, fa0
; RV64-NEXT:    fcvt.s.w fa5, a0
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB193_14:
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, s2
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a1, v8
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa0, a1
; RV64-NEXT:    fabs.s fa5, fa0
; RV64-NEXT:    flt.s a1, fa5, fs2
; RV64-NEXT:    addi a2, sp, 32
; RV64-NEXT:    vl1r.v v8, (a2) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    beqz a1, .LBB193_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    fcvt.w.s a0, fa0
; RV64-NEXT:    fcvt.s.w fa5, a0
; RV64-NEXT:    fsgnj.s fa0, fa5, fa0
; RV64-NEXT:  .LBB193_16:
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 72(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 64(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs1, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs2, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.rint.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @rint_v8f16(ptr %x) {
; ZVFH-LABEL: rint_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI194_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI194_0)(a1)
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: rint_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.rint.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @rint_v4f32(ptr %x) {
; CHECK-LABEL: rint_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    lui a1, 307200
; CHECK-NEXT:    fmv.w.x fa5, a1
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.rint.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @rint_v2f64(ptr %x) {
; CHECK-LABEL: rint_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    lui a1, %hi(.LCPI196_0)
; CHECK-NEXT:    fld fa5, %lo(.LCPI196_0)(a1)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.rint.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @nearbyint_v8bf16(ptr %x) {
; RV32-LABEL: nearbyint_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 24(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a2, a1, 1
; RV32-NEXT:    add a1, a2, a1
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    fmv.x.w s1, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    fmv.x.w s1, fs0
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa0, a0
; RV32-NEXT:    call nearbyintf
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 16
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 24(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: nearbyint_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -48
; RV64-NEXT:    .cfi_def_cfa_offset 48
; RV64-NEXT:    sd ra, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 32(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    .cfi_offset fs0, -32
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a2, a1, 1
; RV64-NEXT:    add a1, a2, a1
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    fmv.x.w s1, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    fmv.x.w s1, fs0
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa0, a0
; RV64-NEXT:    call nearbyintf
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 16
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 32(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 48
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = call <8 x bfloat> @llvm.nearbyint.v8bf16(<8 x bfloat> %a)
  store <8 x bfloat> %b, ptr %x
  ret void
}

define void @nearbyint_v8f16(ptr %x) {
; ZVFH-LABEL: nearbyint_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    lui a1, %hi(.LCPI198_0)
; ZVFH-NEXT:    flh fa5, %lo(.LCPI198_0)(a1)
; ZVFH-NEXT:    vfabs.v v9, v8
; ZVFH-NEXT:    vmflt.vf v0, v9, fa5
; ZVFH-NEXT:    frflags a1
; ZVFH-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; ZVFH-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; ZVFH-NEXT:    fsflags a1
; ZVFH-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; ZVFH-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: nearbyint_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfabs.v v8, v10
; ZVFHMIN-NEXT:    lui a1, 307200
; ZVFHMIN-NEXT:    fmv.w.x fa5, a1
; ZVFHMIN-NEXT:    vmflt.vf v0, v8, fa5
; ZVFHMIN-NEXT:    frflags a1
; ZVFHMIN-NEXT:    vfcvt.x.f.v v8, v10, v0.t
; ZVFHMIN-NEXT:    vfcvt.f.x.v v8, v8, v0.t
; ZVFHMIN-NEXT:    fsflags a1
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; ZVFHMIN-NEXT:    vfsgnj.vv v10, v8, v10, v0.t
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = call <8 x half> @llvm.nearbyint.v8f16(<8 x half> %a)
  store <8 x half> %b, ptr %x
  ret void
}

define void @nearbyint_v4f32(ptr %x) {
; CHECK-LABEL: nearbyint_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    lui a1, 307200
; CHECK-NEXT:    fmv.w.x fa5, a1
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    frflags a1
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    fsflags a1
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse32.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = call <4 x float> @llvm.nearbyint.v4f32(<4 x float> %a)
  store <4 x float> %b, ptr %x
  ret void
}

define void @nearbyint_v2f64(ptr %x) {
; CHECK-LABEL: nearbyint_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    lui a1, %hi(.LCPI200_0)
; CHECK-NEXT:    fld fa5, %lo(.LCPI200_0)(a1)
; CHECK-NEXT:    vfabs.v v9, v8
; CHECK-NEXT:    vmflt.vf v0, v9, fa5
; CHECK-NEXT:    frflags a1
; CHECK-NEXT:    vfcvt.x.f.v v9, v8, v0.t
; CHECK-NEXT:    vfcvt.f.x.v v9, v9, v0.t
; CHECK-NEXT:    fsflags a1
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfsgnj.vv v8, v9, v8, v0.t
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = call <2 x double> @llvm.nearbyint.v2f64(<2 x double> %a)
  store <2 x double> %b, ptr %x
  ret void
}

define void @fmuladd_v8bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fmuladd_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a4, a3, 2
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 5 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v10, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v8, (a2)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 2
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmuladd_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a4, a3, 2
; RV64-NEXT:    add a3, a4, a3
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 5 * vlenb
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v10, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 2
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = load <8 x bfloat>, ptr %z
  %d = call <8 x bfloat> @llvm.fmuladd.v8bf16(<8 x bfloat> %a, <8 x bfloat> %b, <8 x bfloat> %c)
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fmuladd_v6bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fmuladd_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a4, a3, 2
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 5 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v10, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v8, (a2)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fadd.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 2
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmuladd_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a4, a3, 2
; RV64-NEXT:    add a3, a4, a3
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 5 * vlenb
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v10, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fadd.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 2
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = load <6 x bfloat>, ptr %z
  %d = call <6 x bfloat> @llvm.fmuladd.v6bf16(<6 x bfloat> %a, <6 x bfloat> %b, <6 x bfloat> %c)
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fmuladd_v8f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fmuladd_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmacc.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmuladd_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a2)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v11, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v11
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v8, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = load <8 x half>, ptr %z
  %d = call <8 x half> @llvm.fmuladd.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c)
  store <8 x half> %d, ptr %x
  ret void
}

define void @fmuladd_v6f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fmuladd_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmacc.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmuladd_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a2)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v11, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v11
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfadd.vv v8, v8, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = load <6 x half>, ptr %z
  %d = call <6 x half> @llvm.fmuladd.v6f16(<6 x half> %a, <6 x half> %b, <6 x half> %c)
  store <6 x half> %d, ptr %x
  ret void
}

define void @fmuladd_v4f32(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fmuladd_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vle32.v v10, (a2)
; CHECK-NEXT:    vfmacc.vv v10, v8, v9
; CHECK-NEXT:    vse32.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = load <4 x float>, ptr %z
  %d = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c)
  store <4 x float> %d, ptr %x
  ret void
}

define void @fmuladd_v2f64(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fmuladd_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vle64.v v10, (a2)
; CHECK-NEXT:    vfmacc.vv v10, v8, v9
; CHECK-NEXT:    vse64.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = load <2 x double>, ptr %z
  %d = call <2 x double> @llvm.fmuladd.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  store <2 x double> %d, ptr %x
  ret void
}

define void @fmsub_fmuladd_v8bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fmsub_fmuladd_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a4, a3, 2
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 5 * vlenb
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v10, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v8, (a2)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 2
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmsub_fmuladd_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a4, a3, 2
; RV64-NEXT:    add a3, a4, a3
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 5 * vlenb
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v10, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 2
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x bfloat>, ptr %x
  %b = load <8 x bfloat>, ptr %y
  %c = load <8 x bfloat>, ptr %z
  %neg = fneg <8 x bfloat> %c
  %d = call <8 x bfloat> @llvm.fmuladd.v8bf16(<8 x bfloat> %a, <8 x bfloat> %b, <8 x bfloat> %neg)
  store <8 x bfloat> %d, ptr %x
  ret void
}

define void @fmsub_fmuladd_v6bf16(ptr %x, ptr %y, ptr %z) {
; RV32-LABEL: fmsub_fmuladd_v6bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 40(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    csrr a3, vlenb
; RV32-NEXT:    slli a4, a3, 2
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    sub sp, sp, a3
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 5 * vlenb
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV32-NEXT:    vle16.v v10, (a1)
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV32-NEXT:    mv s0, a0
; RV32-NEXT:    vle16.v v9, (a0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vle16.v v8, (a2)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vslidedown.vi v8, v10, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    addi a1, a1, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w s1, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.x v8, a0
; RV32-NEXT:    vslide1down.vx v8, v8, s1
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fmul.s fa0, fa4, fa5
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa5, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vmv.x.s a0, v8
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    fmv.w.x fa4, a0
; RV32-NEXT:    fsub.s fa0, fa5, fa4
; RV32-NEXT:    call __truncsfbf2
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV32-NEXT:    vmv.v.i v0, 15
; RV32-NEXT:    addi a1, sp, 32
; RV32-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV32-NEXT:    vslide1down.vx v8, v8, a0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV32-NEXT:    vse16.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 2
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 40(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    ret
;
; RV64-LABEL: fmsub_fmuladd_v6bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    .cfi_offset s1, -24
; RV64-NEXT:    csrr a3, vlenb
; RV64-NEXT:    slli a4, a3, 2
; RV64-NEXT:    add a3, a4, a3
; RV64-NEXT:    sub sp, sp, a3
; RV64-NEXT:    .cfi_escape 0x0f, 0x0e, 0x72, 0x00, 0x11, 0xc0, 0x00, 0x22, 0x11, 0x05, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 64 + 5 * vlenb
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; RV64-NEXT:    vle16.v v10, (a1)
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vs1r.v v10, (a1) # Unknown-size Folded Spill
; RV64-NEXT:    mv s0, a0
; RV64-NEXT:    vle16.v v9, (a0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vle16.v v8, (a2)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    vslidedown.vi v8, v10, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lh a0, 32(a0) # 8-byte Folded Reload
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    addi a1, a1, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 5
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w s1, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 4
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vslide1down.vx v8, v8, s1
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 6
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fmul.s fa0, fa4, fa5
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v8, v8, 7
; RV64-NEXT:    vmv.x.s a0, v8
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa4, a0
; RV64-NEXT:    fsub.s fa0, fa5, fa4
; RV64-NEXT:    call __truncsfbf2
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; RV64-NEXT:    vmv.v.i v0, 15
; RV64-NEXT:    addi a1, sp, 32
; RV64-NEXT:    vl1r.v v8, (a1) # Unknown-size Folded Reload
; RV64-NEXT:    vslide1down.vx v8, v8, a0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 32
; RV64-NEXT:    vl1r.v v9, (a0) # Unknown-size Folded Reload
; RV64-NEXT:    vsetivli zero, 6, e16, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v9, 4, v0.t
; RV64-NEXT:    vse16.v v8, (s0)
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 2
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <6 x bfloat>, ptr %x
  %b = load <6 x bfloat>, ptr %y
  %c = load <6 x bfloat>, ptr %z
  %neg = fneg <6 x bfloat> %c
  %d = call <6 x bfloat> @llvm.fmuladd.v6bf16(<6 x bfloat> %a, <6 x bfloat> %b, <6 x bfloat> %neg)
  store <6 x bfloat> %d, ptr %x
  ret void
}

define void @fmsub_fmuladd_v8f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fmsub_fmuladd_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmsac.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmsub_fmuladd_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a2)
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v11, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v11
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v8, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <8 x half>, ptr %x
  %b = load <8 x half>, ptr %y
  %c = load <8 x half>, ptr %z
  %neg = fneg <8 x half> %c
  %d = call <8 x half> @llvm.fmuladd.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %neg)
  store <8 x half> %d, ptr %x
  ret void
}

define void @fmsub_fmuladd_v6f16(ptr %x, ptr %y, ptr %z) {
; ZVFH-LABEL: fmsub_fmuladd_v6f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFH-NEXT:    vle16.v v8, (a0)
; ZVFH-NEXT:    vle16.v v9, (a1)
; ZVFH-NEXT:    vle16.v v10, (a2)
; ZVFH-NEXT:    vfmsac.vv v10, v8, v9
; ZVFH-NEXT:    vse16.v v10, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmsub_fmuladd_v6f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a1)
; ZVFHMIN-NEXT:    vle16.v v9, (a0)
; ZVFHMIN-NEXT:    vle16.v v10, (a2)
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v14, v9
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmul.vv v8, v14, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v11, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v11
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfsub.vv v8, v8, v12
; ZVFHMIN-NEXT:    vsetivli zero, 6, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v8
; ZVFHMIN-NEXT:    vse16.v v10, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <6 x half>, ptr %x
  %b = load <6 x half>, ptr %y
  %c = load <6 x half>, ptr %z
  %neg = fneg <6 x half> %c
  %d = call <6 x half> @llvm.fmuladd.v6f16(<6 x half> %a, <6 x half> %b, <6 x half> %neg)
  store <6 x half> %d, ptr %x
  ret void
}

define void @fnmsub_fmuladd_v4f32(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fnmsub_fmuladd_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vle32.v v9, (a1)
; CHECK-NEXT:    vle32.v v10, (a2)
; CHECK-NEXT:    vfnmsac.vv v10, v8, v9
; CHECK-NEXT:    vse32.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <4 x float>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = load <4 x float>, ptr %z
  %neg = fneg <4 x float> %a
  %d = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %neg, <4 x float> %b, <4 x float> %c)
  store <4 x float> %d, ptr %x
  ret void
}

define void @fnmadd_fmuladd_v2f64(ptr %x, ptr %y, ptr %z) {
; CHECK-LABEL: fnmadd_fmuladd_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    vle64.v v9, (a1)
; CHECK-NEXT:    vle64.v v10, (a2)
; CHECK-NEXT:    vfnmacc.vv v10, v8, v9
; CHECK-NEXT:    vse64.v v10, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x double>, ptr %y
  %c = load <2 x double>, ptr %z
  %neg = fneg <2 x double> %b
  %neg2 = fneg <2 x double> %c
  %d = call <2 x double> @llvm.fmuladd.v2f64(<2 x double> %a, <2 x double> %neg, <2 x double> %neg2)
  store <2 x double> %d, ptr %x
  ret void
}
