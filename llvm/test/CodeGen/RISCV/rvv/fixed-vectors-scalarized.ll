; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=riscv32 -mattr=+v -verify-machineinstrs | FileCheck %s --check-prefixes=CHECK,RV32
; RUN: llc < %s -mtriple=riscv64 -mattr=+v -verify-machineinstrs | FileCheck %s --check-prefixes=CHECK,RV64

define <8 x float> @fpext_v8bf16(<8 x bfloat> %x) {
; RV32-LABEL: fpext_v8bf16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -32
; RV32-NEXT:    .cfi_def_cfa_offset 32
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    fmv.x.w a1, fa1
; RV32-NEXT:    fmv.x.w a2, fa2
; RV32-NEXT:    fmv.x.w a3, fa3
; RV32-NEXT:    fmv.x.w a4, fa4
; RV32-NEXT:    fmv.x.w a5, fa5
; RV32-NEXT:    fmv.x.w a6, fa6
; RV32-NEXT:    fmv.x.w a7, fa7
; RV32-NEXT:    slli a7, a7, 16
; RV32-NEXT:    sw a7, 28(sp)
; RV32-NEXT:    slli a6, a6, 16
; RV32-NEXT:    sw a6, 24(sp)
; RV32-NEXT:    slli a5, a5, 16
; RV32-NEXT:    sw a5, 20(sp)
; RV32-NEXT:    slli a4, a4, 16
; RV32-NEXT:    sw a4, 16(sp)
; RV32-NEXT:    slli a3, a3, 16
; RV32-NEXT:    sw a3, 12(sp)
; RV32-NEXT:    slli a2, a2, 16
; RV32-NEXT:    sw a2, 8(sp)
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    sw a1, 4(sp)
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    sw a0, 0(sp)
; RV32-NEXT:    addi a0, sp, 28
; RV32-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    addi a0, sp, 24
; RV32-NEXT:    vle32.v v9, (a0)
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV32-NEXT:    vslideup.vi v9, v8, 1
; RV32-NEXT:    addi a0, sp, 20
; RV32-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV32-NEXT:    vle32.v v10, (a0)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vle32.v v12, (a0)
; RV32-NEXT:    addi a0, sp, 12
; RV32-NEXT:    vle32.v v11, (a0)
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vle32.v v13, (a0)
; RV32-NEXT:    addi a0, sp, 4
; RV32-NEXT:    vle32.v v14, (a0)
; RV32-NEXT:    mv a0, sp
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV32-NEXT:    vslideup.vi v12, v10, 1
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vslideup.vi v12, v9, 2
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV32-NEXT:    vslideup.vi v13, v11, 1
; RV32-NEXT:    vslideup.vi v8, v14, 1
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vslideup.vi v8, v13, 2
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; RV32-NEXT:    vslideup.vi v8, v12, 4
; RV32-NEXT:    addi sp, sp, 32
; RV32-NEXT:    ret
;
; RV64-LABEL: fpext_v8bf16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -32
; RV64-NEXT:    .cfi_def_cfa_offset 32
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    fmv.x.w a1, fa1
; RV64-NEXT:    fmv.x.w a2, fa2
; RV64-NEXT:    fmv.x.w a3, fa3
; RV64-NEXT:    fmv.x.w a4, fa4
; RV64-NEXT:    fmv.x.w a5, fa5
; RV64-NEXT:    fmv.x.w a6, fa6
; RV64-NEXT:    fmv.x.w a7, fa7
; RV64-NEXT:    slli a7, a7, 16
; RV64-NEXT:    fmv.w.x fa5, a7
; RV64-NEXT:    fsw fa5, 28(sp)
; RV64-NEXT:    slli a6, a6, 16
; RV64-NEXT:    fmv.w.x fa5, a6
; RV64-NEXT:    fsw fa5, 24(sp)
; RV64-NEXT:    slli a5, a5, 16
; RV64-NEXT:    fmv.w.x fa5, a5
; RV64-NEXT:    fsw fa5, 20(sp)
; RV64-NEXT:    slli a4, a4, 16
; RV64-NEXT:    fmv.w.x fa5, a4
; RV64-NEXT:    fsw fa5, 16(sp)
; RV64-NEXT:    slli a3, a3, 16
; RV64-NEXT:    fmv.w.x fa5, a3
; RV64-NEXT:    fsw fa5, 12(sp)
; RV64-NEXT:    slli a2, a2, 16
; RV64-NEXT:    fmv.w.x fa5, a2
; RV64-NEXT:    fsw fa5, 8(sp)
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa5, a1
; RV64-NEXT:    fsw fa5, 4(sp)
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsw fa5, 0(sp)
; RV64-NEXT:    addi a0, sp, 28
; RV64-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    addi a0, sp, 24
; RV64-NEXT:    vle32.v v9, (a0)
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV64-NEXT:    vslideup.vi v9, v8, 1
; RV64-NEXT:    addi a0, sp, 20
; RV64-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV64-NEXT:    vle32.v v10, (a0)
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vle32.v v12, (a0)
; RV64-NEXT:    addi a0, sp, 12
; RV64-NEXT:    vle32.v v11, (a0)
; RV64-NEXT:    addi a0, sp, 8
; RV64-NEXT:    vle32.v v13, (a0)
; RV64-NEXT:    addi a0, sp, 4
; RV64-NEXT:    vle32.v v14, (a0)
; RV64-NEXT:    mv a0, sp
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV64-NEXT:    vslideup.vi v12, v10, 1
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vslideup.vi v12, v9, 2
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV64-NEXT:    vslideup.vi v13, v11, 1
; RV64-NEXT:    vslideup.vi v8, v14, 1
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vslideup.vi v8, v13, 2
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; RV64-NEXT:    vslideup.vi v8, v12, 4
; RV64-NEXT:    addi sp, sp, 32
; RV64-NEXT:    ret
  %y = fpext <8 x bfloat> %x to <8 x float>
  ret <8 x float> %y
}

define <8 x float> @fpext_v8f16(<8 x bfloat> %x) {
; RV32-LABEL: fpext_v8f16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -32
; RV32-NEXT:    .cfi_def_cfa_offset 32
; RV32-NEXT:    fmv.x.w a0, fa0
; RV32-NEXT:    fmv.x.w a1, fa1
; RV32-NEXT:    fmv.x.w a2, fa2
; RV32-NEXT:    fmv.x.w a3, fa3
; RV32-NEXT:    fmv.x.w a4, fa4
; RV32-NEXT:    fmv.x.w a5, fa5
; RV32-NEXT:    fmv.x.w a6, fa6
; RV32-NEXT:    fmv.x.w a7, fa7
; RV32-NEXT:    slli a7, a7, 16
; RV32-NEXT:    sw a7, 28(sp)
; RV32-NEXT:    slli a6, a6, 16
; RV32-NEXT:    sw a6, 24(sp)
; RV32-NEXT:    slli a5, a5, 16
; RV32-NEXT:    sw a5, 20(sp)
; RV32-NEXT:    slli a4, a4, 16
; RV32-NEXT:    sw a4, 16(sp)
; RV32-NEXT:    slli a3, a3, 16
; RV32-NEXT:    sw a3, 12(sp)
; RV32-NEXT:    slli a2, a2, 16
; RV32-NEXT:    sw a2, 8(sp)
; RV32-NEXT:    slli a1, a1, 16
; RV32-NEXT:    sw a1, 4(sp)
; RV32-NEXT:    slli a0, a0, 16
; RV32-NEXT:    sw a0, 0(sp)
; RV32-NEXT:    addi a0, sp, 28
; RV32-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    addi a0, sp, 24
; RV32-NEXT:    vle32.v v9, (a0)
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV32-NEXT:    vslideup.vi v9, v8, 1
; RV32-NEXT:    addi a0, sp, 20
; RV32-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV32-NEXT:    vle32.v v10, (a0)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vle32.v v12, (a0)
; RV32-NEXT:    addi a0, sp, 12
; RV32-NEXT:    vle32.v v11, (a0)
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vle32.v v13, (a0)
; RV32-NEXT:    addi a0, sp, 4
; RV32-NEXT:    vle32.v v14, (a0)
; RV32-NEXT:    mv a0, sp
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV32-NEXT:    vslideup.vi v12, v10, 1
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vslideup.vi v12, v9, 2
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV32-NEXT:    vslideup.vi v13, v11, 1
; RV32-NEXT:    vslideup.vi v8, v14, 1
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vslideup.vi v8, v13, 2
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; RV32-NEXT:    vslideup.vi v8, v12, 4
; RV32-NEXT:    addi sp, sp, 32
; RV32-NEXT:    ret
;
; RV64-LABEL: fpext_v8f16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -32
; RV64-NEXT:    .cfi_def_cfa_offset 32
; RV64-NEXT:    fmv.x.w a0, fa0
; RV64-NEXT:    fmv.x.w a1, fa1
; RV64-NEXT:    fmv.x.w a2, fa2
; RV64-NEXT:    fmv.x.w a3, fa3
; RV64-NEXT:    fmv.x.w a4, fa4
; RV64-NEXT:    fmv.x.w a5, fa5
; RV64-NEXT:    fmv.x.w a6, fa6
; RV64-NEXT:    fmv.x.w a7, fa7
; RV64-NEXT:    slli a7, a7, 16
; RV64-NEXT:    fmv.w.x fa5, a7
; RV64-NEXT:    fsw fa5, 28(sp)
; RV64-NEXT:    slli a6, a6, 16
; RV64-NEXT:    fmv.w.x fa5, a6
; RV64-NEXT:    fsw fa5, 24(sp)
; RV64-NEXT:    slli a5, a5, 16
; RV64-NEXT:    fmv.w.x fa5, a5
; RV64-NEXT:    fsw fa5, 20(sp)
; RV64-NEXT:    slli a4, a4, 16
; RV64-NEXT:    fmv.w.x fa5, a4
; RV64-NEXT:    fsw fa5, 16(sp)
; RV64-NEXT:    slli a3, a3, 16
; RV64-NEXT:    fmv.w.x fa5, a3
; RV64-NEXT:    fsw fa5, 12(sp)
; RV64-NEXT:    slli a2, a2, 16
; RV64-NEXT:    fmv.w.x fa5, a2
; RV64-NEXT:    fsw fa5, 8(sp)
; RV64-NEXT:    slli a1, a1, 16
; RV64-NEXT:    fmv.w.x fa5, a1
; RV64-NEXT:    fsw fa5, 4(sp)
; RV64-NEXT:    slli a0, a0, 16
; RV64-NEXT:    fmv.w.x fa5, a0
; RV64-NEXT:    fsw fa5, 0(sp)
; RV64-NEXT:    addi a0, sp, 28
; RV64-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    addi a0, sp, 24
; RV64-NEXT:    vle32.v v9, (a0)
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV64-NEXT:    vslideup.vi v9, v8, 1
; RV64-NEXT:    addi a0, sp, 20
; RV64-NEXT:    vsetivli zero, 1, e32, mf2, ta, ma
; RV64-NEXT:    vle32.v v10, (a0)
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vle32.v v12, (a0)
; RV64-NEXT:    addi a0, sp, 12
; RV64-NEXT:    vle32.v v11, (a0)
; RV64-NEXT:    addi a0, sp, 8
; RV64-NEXT:    vle32.v v13, (a0)
; RV64-NEXT:    addi a0, sp, 4
; RV64-NEXT:    vle32.v v14, (a0)
; RV64-NEXT:    mv a0, sp
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV64-NEXT:    vslideup.vi v12, v10, 1
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vslideup.vi v12, v9, 2
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; RV64-NEXT:    vslideup.vi v13, v11, 1
; RV64-NEXT:    vslideup.vi v8, v14, 1
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vslideup.vi v8, v13, 2
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; RV64-NEXT:    vslideup.vi v8, v12, 4
; RV64-NEXT:    addi sp, sp, 32
; RV64-NEXT:    ret
  %y = fpext <8 x bfloat> %x to <8 x float>
  ret <8 x float> %y
}
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
