; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+v,+d -verify-machineinstrs < %s | FileCheck %s

define <2 x double> @test_vp_compress_v2f64_masked(<2 x double> %src, <2 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v2f64_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m1, ta, ma
; CHECK-NEXT:    vcompress.vm v9, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v9
; CHECK-NEXT:    ret
  %dst = call <2 x double> @llvm.experimental.vp.compress.v2f64(<2 x double> %src, <2 x i1> %mask, i32 %evl)
  ret <2 x double> %dst
}

define <2 x float> @test_vp_compress_v2f32_masked(<2 x float> %src, <2 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v2f32_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, mf2, ta, ma
; CHECK-NEXT:    vcompress.vm v9, v8, v0
; CHECK-NEXT:    vmv1r.v v8, v9
; CHECK-NEXT:    ret
  %dst = call <2 x float> @llvm.experimental.vp.compress.v2f32(<2 x float> %src, <2 x i1> %mask, i32 %evl)
  ret <2 x float> %dst
}

define <4 x float> @test_vp_compress_v4f32_masked(<4 x float> %src, <4 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v4f32_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m1, ta, ma
; CHECK-NEXT:    vcompress.vm v9, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v9
; CHECK-NEXT:    ret
  %dst = call <4 x float> @llvm.experimental.vp.compress.v4f32(<4 x float> %src, <4 x i1> %mask, i32 %evl)
  ret <4 x float> %dst
}
define <4 x double> @test_vp_compress_v4f64_masked(<4 x double> %src, <4 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v4f64_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m2, ta, ma
; CHECK-NEXT:    vcompress.vm v10, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v10
; CHECK-NEXT:    ret
  %dst = call <4 x double> @llvm.experimental.vp.compress.v4f64(<4 x double> %src, <4 x i1> %mask, i32 %evl)
  ret <4 x double> %dst
}

define <8 x float> @test_vp_compress_v8f32_masked(<8 x float> %src, <8 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v8f32_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; CHECK-NEXT:    vcompress.vm v10, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v10
; CHECK-NEXT:    ret
  %dst = call <8 x float> @llvm.experimental.vp.compress.v8f32(<8 x float> %src, <8 x i1> %mask, i32 %evl)
  ret <8 x float> %dst
}

define <8 x double> @test_vp_compress_v8f64_masked(<8 x double> %src, <8 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v8f64_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vcompress.vm v12, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v12
; CHECK-NEXT:    ret
  %dst = call <8 x double> @llvm.experimental.vp.compress.v8f64(<8 x double> %src, <8 x i1> %mask, i32 %evl)
  ret <8 x double> %dst
}

define <16 x float> @test_vp_compress_v16f32_masked(<16 x float> %src, <16 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v16f32_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m4, ta, ma
; CHECK-NEXT:    vcompress.vm v12, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v12
; CHECK-NEXT:    ret
  %dst = call <16 x float> @llvm.experimental.vp.compress.v16f32(<16 x float> %src, <16 x i1> %mask, i32 %evl)
  ret <16 x float> %dst
}

define <16 x double> @test_vp_compress_v16f64_masked(<16 x double> %src, <16 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v16f64_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m8, ta, ma
; CHECK-NEXT:    vcompress.vm v16, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v16
; CHECK-NEXT:    ret
  %dst = call <16 x double> @llvm.experimental.vp.compress.v16f64(<16 x double> %src, <16 x i1> %mask, i32 %evl)
  ret <16 x double> %dst
}

define <32 x float> @test_vp_compress_v32f32_masked(<32 x float> %src, <32 x i1> %mask, i32 zeroext %evl) {
; CHECK-LABEL: test_vp_compress_v32f32_masked:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m8, ta, ma
; CHECK-NEXT:    vcompress.vm v16, v8, v0
; CHECK-NEXT:    vmv.v.v v8, v16
; CHECK-NEXT:    ret
  %dst = call <32 x float> @llvm.experimental.vp.compress.v32f32(<32 x float> %src, <32 x i1> %mask, i32 %evl)
  ret <32 x float> %dst
}

; LMUL = 1
declare <2 x double> @llvm.experimental.vp.compress.v2f64(<2 x double>,<2 x i1>,i32)
declare <2 x float> @llvm.experimental.vp.compress.v2f32(<2 x float>,<2 x i1>,i32)
declare <4 x float> @llvm.experimental.vp.compress.v4f32(<4 x float>,<4 x i1>,i32)

; LMUL = 2
declare <4 x double> @llvm.experimental.vp.compress.v4f64(<4 x double>,<4 x i1>,i32)
declare <8 x float> @llvm.experimental.vp.compress.v8f32(<8 x float>,<8 x i1>,i32)

; LMUL = 4
declare <8 x double> @llvm.experimental.vp.compress.v8f64(<8 x double>,<8 x i1>,i32)
declare <16 x float> @llvm.experimental.vp.compress.v16f32(<16 x float>,<16 x i1>,i32)

; LMUL = 8
declare <16 x double> @llvm.experimental.vp.compress.v16f64(<16 x double>,<16 x i1>,i32)
declare <32 x float> @llvm.experimental.vp.compress.v32f32(<32 x float>,<32 x i1>,i32)
