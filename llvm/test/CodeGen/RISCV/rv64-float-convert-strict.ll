; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -target-abi=lp64 -verify-machineinstrs < %s \
; RUN:   -disable-strictnode-mutation | FileCheck %s -check-prefix=RV64I
; RUN: llc -mtriple=riscv64 -mattr=+f -target-abi=lp64f -verify-machineinstrs < %s \
; RUN:   -disable-strictnode-mutation | FileCheck %s -check-prefix=RV64IF
; RUN: llc -mtriple=riscv64 -mattr=+zfinx -target-abi=lp64 -verify-machineinstrs < %s \
; RUN:   -disable-strictnode-mutation | FileCheck %s -check-prefix=RV64IFINX

define i128 @fptosi_f32_to_i128(float %a) nounwind strictfp {
; RV64I-LABEL: fptosi_f32_to_i128:
; RV64I:       # %bb.0:
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sext.w a0, a0
; RV64I-NEXT:    call __fixsfti
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
;
; RV64IF-LABEL: fptosi_f32_to_i128:
; RV64IF:       # %bb.0:
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    call __fixsfti
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    ret
;
; RV64IFINX-LABEL: fptosi_f32_to_i128:
; RV64IFINX:       # %bb.0:
; RV64IFINX-NEXT:    addi sp, sp, -16
; RV64IFINX-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IFINX-NEXT:    call __fixsfti
; RV64IFINX-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IFINX-NEXT:    addi sp, sp, 16
; RV64IFINX-NEXT:    ret
  %1 = call i128 @llvm.experimental.constrained.fptosi.i128.f32(float %a, metadata !"fpexcept.strict")
  ret i128 %1
}

define i128 @fptoui_f32_to_i128(float %a) nounwind strictfp {
; RV64I-LABEL: fptoui_f32_to_i128:
; RV64I:       # %bb.0:
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sext.w a0, a0
; RV64I-NEXT:    call __fixunssfti
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
;
; RV64IF-LABEL: fptoui_f32_to_i128:
; RV64IF:       # %bb.0:
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    call __fixunssfti
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    ret
;
; RV64IFINX-LABEL: fptoui_f32_to_i128:
; RV64IFINX:       # %bb.0:
; RV64IFINX-NEXT:    addi sp, sp, -16
; RV64IFINX-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IFINX-NEXT:    call __fixunssfti
; RV64IFINX-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IFINX-NEXT:    addi sp, sp, 16
; RV64IFINX-NEXT:    ret
  %1 = call i128 @llvm.experimental.constrained.fptoui.i128.f32(float %a, metadata !"fpexcept.strict")
  ret i128 %1
}

define float @sitofp_i128_to_f32(i128 %a) nounwind strictfp {
; RV64I-LABEL: sitofp_i128_to_f32:
; RV64I:       # %bb.0:
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    call __floattisf
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
;
; RV64IF-LABEL: sitofp_i128_to_f32:
; RV64IF:       # %bb.0:
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    call __floattisf
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    ret
;
; RV64IFINX-LABEL: sitofp_i128_to_f32:
; RV64IFINX:       # %bb.0:
; RV64IFINX-NEXT:    addi sp, sp, -16
; RV64IFINX-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IFINX-NEXT:    call __floattisf
; RV64IFINX-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IFINX-NEXT:    addi sp, sp, 16
; RV64IFINX-NEXT:    ret
  %1 = call float @llvm.experimental.constrained.sitofp.f32.i128(i128 %a, metadata !"round.dynamic", metadata !"fpexcept.strict")
  ret float %1
}

define float @uitofp_i128_to_f32(i128 %a) nounwind strictfp {
; RV64I-LABEL: uitofp_i128_to_f32:
; RV64I:       # %bb.0:
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    call __floatuntisf
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
;
; RV64IF-LABEL: uitofp_i128_to_f32:
; RV64IF:       # %bb.0:
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    call __floatuntisf
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    ret
;
; RV64IFINX-LABEL: uitofp_i128_to_f32:
; RV64IFINX:       # %bb.0:
; RV64IFINX-NEXT:    addi sp, sp, -16
; RV64IFINX-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IFINX-NEXT:    call __floatuntisf
; RV64IFINX-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IFINX-NEXT:    addi sp, sp, 16
; RV64IFINX-NEXT:    ret
  %1 = call float @llvm.experimental.constrained.uitofp.f32.i128(i128 %a, metadata !"round.dynamic", metadata !"fpexcept.strict")
  ret float %1
}
