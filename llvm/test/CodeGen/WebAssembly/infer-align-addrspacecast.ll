; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 3
; RUN: opt -passes='infer-alignment' -S < %s | FileCheck %s

target datalayout = "e-m:e-p:32:32-i64:64-n32:64-S128"
target triple = "wasm32-unknown-unknown"

$globalArrayAS0 = comdat any
$globalArrayAS1 = comdat any
$globalArrayAS10 = comdat any
$globalArrayAS20 = comdat any
@globalArrayAS0 = linkonce_odr hidden addrspace(0) global [4096 x i8] undef, comdat, align 16
@globalArrayAS1 = linkonce_odr hidden addrspace(1) global [4096 x i8] undef, comdat, align 16
@globalArrayAS10 = linkonce_odr hidden addrspace(10) global [4096 x i8] undef, comdat, align 16
@globalArrayAS20 = linkonce_odr hidden addrspace(20) global [4096 x i8] undef, comdat, align 16

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS10(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS10(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(1) @globalArrayAS1 to ptr), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(1) @globalArrayAS1 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS10_0(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS10_0(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(10) @globalArrayAS10 to ptr), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(10) @globalArrayAS10 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS20_0(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS20_0(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(20) @globalArrayAS20 to ptr), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(20) @globalArrayAS20 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS01(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS01(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(1) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(1)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(1) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(1) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(1)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(1) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS02(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS02(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(2) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(2)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(2) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(2) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(2)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(2) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS03(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS03(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(3) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(3)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(3) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(3) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(3)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(3) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS04(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS04(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(4) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(4)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(4) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(4) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(4)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(4) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS05(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS05(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(5) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(5)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(5) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(5) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(5)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(5) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS06(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS06(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(6) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(6)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(6) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(6) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(6)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(6) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS07(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS07(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(7) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(7)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(7) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(7) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(7)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(7) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS0_10(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS0_10(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(10) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(10)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(10) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(10) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(10)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(10) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS0_20(i32 %idx) unnamed_addr align 2 {
; CHECK-LABEL: define amdgpu_kernel void @infer_AS0_20(
; CHECK-SAME: i32 [[IDX:%.*]]) unnamed_addr align 2 {
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; CHECK-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; CHECK-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; CHECK-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(20) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(20)), i64 [[IDXPROM37_I21]]
; CHECK-NEXT:    [[L1:%.*]] = load float, ptr addrspace(20) [[ARRAYIDX38_I22]], align 16
; CHECK-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(20) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(20)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(20) %arrayidx38.i22, align 4
  ret void
}
