; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 3
; RUN: llc -mtriple=x86_64 -mattr=+avx512f < %s -o - | FileCheck %s --check-prefixes=AVX512
; RUN: llc -mtriple=x86_64 -mattr=+avx512vl < %s -o - | FileCheck %s --check-prefixes=AVX512VL

define half @test_half(half %x, i32 %exp) {
; AVX512-LABEL: test_half:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpextrw $0, %xmm0, %eax
; AVX512-NEXT:    vcvtsi2ss %edi, %xmm1, %xmm0
; AVX512-NEXT:    movzwl %ax, %eax
; AVX512-NEXT:    vmovd %eax, %xmm1
; AVX512-NEXT:    vcvtph2ps %xmm1, %xmm1
; AVX512-NEXT:    vscalefss %xmm0, %xmm1, %xmm0
; AVX512-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    vpinsrw $0, %eax, %xmm0, %xmm0
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_half:
; AVX512VL:       # %bb.0: # %entry
; AVX512VL-NEXT:    vpextrw $0, %xmm0, %eax
; AVX512VL-NEXT:    vcvtsi2ss %edi, %xmm1, %xmm0
; AVX512VL-NEXT:    movzwl %ax, %eax
; AVX512VL-NEXT:    vmovd %eax, %xmm1
; AVX512VL-NEXT:    vcvtph2ps %xmm1, %xmm1
; AVX512VL-NEXT:    vscalefss %xmm0, %xmm1, %xmm0
; AVX512VL-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; AVX512VL-NEXT:    vmovd %xmm0, %eax
; AVX512VL-NEXT:    vpinsrw $0, %eax, %xmm0, %xmm0
; AVX512VL-NEXT:    retq
entry:
  %r = tail call fast half @llvm.ldexp.f16.i32(half %x, i32 %exp)
  ret half %r
}

declare half @llvm.ldexp.f16.i32(half, i32) memory(none)

define float @test_float(float %x, i32 %exp) {
; AVX512-LABEL: test_float:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vcvtsi2ss %edi, %xmm1, %xmm1
; AVX512-NEXT:    vscalefss %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_float:
; AVX512VL:       # %bb.0: # %entry
; AVX512VL-NEXT:    vcvtsi2ss %edi, %xmm1, %xmm1
; AVX512VL-NEXT:    vscalefss %xmm1, %xmm0, %xmm0
; AVX512VL-NEXT:    retq
entry:
  %r = tail call fast float @ldexpf(float %x, i32 %exp)
  ret float %r
}

declare float @ldexpf(float, i32) memory(none)

define double @test_double(double %x, i32 %exp) {
; AVX512-LABEL: test_double:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vcvtsi2sd %edi, %xmm1, %xmm1
; AVX512-NEXT:    vscalefsd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_double:
; AVX512VL:       # %bb.0: # %entry
; AVX512VL-NEXT:    vcvtsi2sd %edi, %xmm1, %xmm1
; AVX512VL-NEXT:    vscalefsd %xmm1, %xmm0, %xmm0
; AVX512VL-NEXT:    retq
entry:
  %r = tail call fast double @ldexp(double %x, i32 %exp)
  ret double %r
}

declare double @ldexp(double, i32) memory(none)

define fp128 @testExpl(fp128 %x, i32 %exp) {
; AVX512-LABEL: testExpl:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    jmp ldexpl@PLT # TAILCALL
;
; AVX512VL-LABEL: testExpl:
; AVX512VL:       # %bb.0: # %entry
; AVX512VL-NEXT:    jmp ldexpl@PLT # TAILCALL
entry:
  %r = tail call fast fp128 @ldexpl(fp128 %x, i32 %exp)
  ret fp128 %r
}

declare fp128 @ldexpl(fp128, i32) memory(none)

define <4 x float> @test_ldexp_4xfloat(<4 x float> %x, <4 x i32> %exp) {
; AVX512-LABEL: test_ldexp_4xfloat:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vcvtdq2ps %xmm1, %xmm1
; AVX512-NEXT:    vscalefss %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_ldexp_4xfloat:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vcvtdq2ps %xmm1, %xmm1
; AVX512VL-NEXT:    vscalefps %xmm1, %xmm0, %xmm0
; AVX512VL-NEXT:    retq
  %r = call <4 x float> @llvm.ldexp.v4f32.v4i32(<4 x float> %x, <4 x i32> %exp)
  ret <4 x float> %r
}
declare <4 x float> @llvm.ldexp.v4f32.v4i32(<4 x float>, <4 x i32>)

define <2 x double> @test_ldexp_2xdouble(<2 x double> %x, <2 x i32> %exp) {
; AVX512-LABEL: test_ldexp_2xdouble:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vcvtdq2pd %xmm1, %xmm2
; AVX512-NEXT:    vscalefsd %xmm2, %xmm0, %xmm2
; AVX512-NEXT:    vshufpd {{.*#+}} xmm0 = xmm0[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm1 = xmm1[1,1,1,1]
; AVX512-NEXT:    vcvtdq2pd %xmm1, %xmm1
; AVX512-NEXT:    vscalefsd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vunpcklpd {{.*#+}} xmm0 = xmm2[0],xmm0[0]
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_ldexp_2xdouble:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vcvtdq2pd %xmm1, %xmm2
; AVX512VL-NEXT:    vscalefsd %xmm2, %xmm0, %xmm2
; AVX512VL-NEXT:    vshufpd {{.*#+}} xmm0 = xmm0[1,0]
; AVX512VL-NEXT:    vshufps {{.*#+}} xmm1 = xmm1[1,1,1,1]
; AVX512VL-NEXT:    vcvtdq2pd %xmm1, %xmm1
; AVX512VL-NEXT:    vscalefsd %xmm1, %xmm0, %xmm0
; AVX512VL-NEXT:    vunpcklpd {{.*#+}} xmm0 = xmm2[0],xmm0[0]
; AVX512VL-NEXT:    retq
  %r = call <2 x double> @llvm.ldexp.v2f64.v2i32(<2 x double> %x, <2 x i32> %exp)
  ret <2 x double> %r
}
declare <2 x double> @llvm.ldexp.v2f64.v2i32(<2 x double>, <2 x i32>)

define <8 x float> @test_ldexp_8xfloat(<8 x float> %x, <8 x i32> %exp) {
; AVX512-LABEL: test_ldexp_8xfloat:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vextractf128 $1, %ymm0, %xmm2
; AVX512-NEXT:    vextractf128 $1, %ymm1, %xmm3
; AVX512-NEXT:    vcvtdq2ps %xmm3, %xmm4
; AVX512-NEXT:    vscalefss %xmm4, %xmm2, %xmm4
; AVX512-NEXT:    vmovshdup {{.*#+}} xmm5 = xmm2[1,1,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm6 = xmm3[1,1,1,1]
; AVX512-NEXT:    vcvtdq2ps %xmm6, %xmm6
; AVX512-NEXT:    vscalefss %xmm6, %xmm5, %xmm5
; AVX512-NEXT:    vunpcklps {{.*#+}} xmm4 = xmm4[0],xmm5[0],xmm4[1],xmm5[1]
; AVX512-NEXT:    vshufpd {{.*#+}} xmm5 = xmm2[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm6 = xmm3[2,3,2,3]
; AVX512-NEXT:    vcvtdq2ps %xmm6, %xmm6
; AVX512-NEXT:    vscalefss %xmm6, %xmm5, %xmm5
; AVX512-NEXT:    vmovlhps {{.*#+}} xmm4 = xmm4[0],xmm5[0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm2 = xmm2[3,3,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm3 = xmm3[3,3,3,3]
; AVX512-NEXT:    vcvtdq2ps %xmm3, %xmm3
; AVX512-NEXT:    vscalefss %xmm3, %xmm2, %xmm2
; AVX512-NEXT:    vinsertps {{.*#+}} xmm2 = xmm4[0,1,2],xmm2[0]
; AVX512-NEXT:    vcvtdq2ps %xmm1, %xmm3
; AVX512-NEXT:    vscalefss %xmm3, %xmm0, %xmm3
; AVX512-NEXT:    vmovshdup {{.*#+}} xmm4 = xmm0[1,1,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm5 = xmm1[1,1,1,1]
; AVX512-NEXT:    vcvtdq2ps %xmm5, %xmm5
; AVX512-NEXT:    vscalefss %xmm5, %xmm4, %xmm4
; AVX512-NEXT:    vunpcklps {{.*#+}} xmm3 = xmm3[0],xmm4[0],xmm3[1],xmm4[1]
; AVX512-NEXT:    vshufpd {{.*#+}} xmm4 = xmm0[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm5 = xmm1[2,3,2,3]
; AVX512-NEXT:    vcvtdq2ps %xmm5, %xmm5
; AVX512-NEXT:    vscalefss %xmm5, %xmm4, %xmm4
; AVX512-NEXT:    vmovlhps {{.*#+}} xmm3 = xmm3[0],xmm4[0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm1 = xmm1[3,3,3,3]
; AVX512-NEXT:    vcvtdq2ps %xmm1, %xmm1
; AVX512-NEXT:    vscalefss %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vinsertps {{.*#+}} xmm0 = xmm3[0,1,2],xmm0[0]
; AVX512-NEXT:    vinsertf128 $1, %xmm2, %ymm0, %ymm0
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_ldexp_8xfloat:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vcvtdq2ps %ymm1, %ymm1
; AVX512VL-NEXT:    vscalefps %ymm1, %ymm0, %ymm0
; AVX512VL-NEXT:    retq
  %r = call <8 x float> @llvm.ldexp.v8f32.v8i32(<8 x float> %x, <8 x i32> %exp)
  ret <8 x float> %r
}
declare <8 x float> @llvm.ldexp.v8f32.v8i32(<8 x float>, <8 x i32>)

define <4 x double> @test_ldexp_4xdouble(<4 x double> %x, <4 x i32> %exp) {
; AVX512-LABEL: test_ldexp_4xdouble:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vextractf128 $1, %ymm0, %xmm2
; AVX512-NEXT:    vshufps {{.*#+}} xmm3 = xmm1[2,3,2,3]
; AVX512-NEXT:    vcvtdq2pd %xmm3, %xmm3
; AVX512-NEXT:    vscalefsd %xmm3, %xmm2, %xmm3
; AVX512-NEXT:    vcvtdq2pd %xmm1, %xmm4
; AVX512-NEXT:    vscalefsd %xmm4, %xmm0, %xmm4
; AVX512-NEXT:    vinsertf128 $1, %xmm3, %ymm4, %ymm3
; AVX512-NEXT:    vshufps {{.*#+}} xmm4 = xmm1[3,3,3,3]
; AVX512-NEXT:    vcvtdq2pd %xmm4, %xmm4
; AVX512-NEXT:    vshufpd {{.*#+}} xmm2 = xmm2[1,0]
; AVX512-NEXT:    vscalefsd %xmm4, %xmm2, %xmm2
; AVX512-NEXT:    vshufps {{.*#+}} xmm1 = xmm1[1,1,1,1]
; AVX512-NEXT:    vcvtdq2pd %xmm1, %xmm1
; AVX512-NEXT:    vshufpd {{.*#+}} xmm0 = xmm0[1,0]
; AVX512-NEXT:    vscalefsd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vinsertf128 $1, %xmm2, %ymm0, %ymm0
; AVX512-NEXT:    vunpcklpd {{.*#+}} ymm0 = ymm3[0],ymm0[0],ymm3[2],ymm0[2]
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_ldexp_4xdouble:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vcvtdq2pd %xmm1, %ymm1
; AVX512VL-NEXT:    vscalefpd %ymm1, %ymm0, %ymm0
; AVX512VL-NEXT:    retq
  %r = call <4 x double> @llvm.ldexp.v4f64.v4i32(<4 x double> %x, <4 x i32> %exp)
  ret <4 x double> %r
}
declare <4 x double> @llvm.ldexp.v4f64.v4i32(<4 x double>, <4 x i32>)

define <16 x float> @test_ldexp_16xfloat(<16 x float> %x, <16 x i32> %exp) {
; AVX512-LABEL: test_ldexp_16xfloat:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vextractf32x4 $3, %zmm0, %xmm2
; AVX512-NEXT:    vextractf32x4 $3, %zmm1, %xmm3
; AVX512-NEXT:    vcvtdq2ps %xmm3, %xmm4
; AVX512-NEXT:    vscalefss %xmm4, %xmm2, %xmm4
; AVX512-NEXT:    vmovshdup {{.*#+}} xmm5 = xmm2[1,1,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm6 = xmm3[1,1,1,1]
; AVX512-NEXT:    vcvtdq2ps %xmm6, %xmm6
; AVX512-NEXT:    vscalefss %xmm6, %xmm5, %xmm5
; AVX512-NEXT:    vunpcklps {{.*#+}} xmm4 = xmm4[0],xmm5[0],xmm4[1],xmm5[1]
; AVX512-NEXT:    vshufpd {{.*#+}} xmm5 = xmm2[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm6 = xmm3[2,3,2,3]
; AVX512-NEXT:    vcvtdq2ps %xmm6, %xmm6
; AVX512-NEXT:    vscalefss %xmm6, %xmm5, %xmm5
; AVX512-NEXT:    vmovlhps {{.*#+}} xmm4 = xmm4[0],xmm5[0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm2 = xmm2[3,3,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm3 = xmm3[3,3,3,3]
; AVX512-NEXT:    vcvtdq2ps %xmm3, %xmm3
; AVX512-NEXT:    vscalefss %xmm3, %xmm2, %xmm2
; AVX512-NEXT:    vinsertps {{.*#+}} xmm2 = xmm4[0,1,2],xmm2[0]
; AVX512-NEXT:    vextractf32x4 $2, %zmm0, %xmm3
; AVX512-NEXT:    vextractf32x4 $2, %zmm1, %xmm4
; AVX512-NEXT:    vcvtdq2ps %xmm4, %xmm5
; AVX512-NEXT:    vscalefss %xmm5, %xmm3, %xmm5
; AVX512-NEXT:    vmovshdup {{.*#+}} xmm6 = xmm3[1,1,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm7 = xmm4[1,1,1,1]
; AVX512-NEXT:    vcvtdq2ps %xmm7, %xmm7
; AVX512-NEXT:    vscalefss %xmm7, %xmm6, %xmm6
; AVX512-NEXT:    vunpcklps {{.*#+}} xmm5 = xmm5[0],xmm6[0],xmm5[1],xmm6[1]
; AVX512-NEXT:    vshufpd {{.*#+}} xmm6 = xmm3[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm7 = xmm4[2,3,2,3]
; AVX512-NEXT:    vcvtdq2ps %xmm7, %xmm7
; AVX512-NEXT:    vscalefss %xmm7, %xmm6, %xmm6
; AVX512-NEXT:    vmovlhps {{.*#+}} xmm5 = xmm5[0],xmm6[0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm3 = xmm3[3,3,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm4 = xmm4[3,3,3,3]
; AVX512-NEXT:    vcvtdq2ps %xmm4, %xmm4
; AVX512-NEXT:    vscalefss %xmm4, %xmm3, %xmm3
; AVX512-NEXT:    vinsertps {{.*#+}} xmm3 = xmm5[0,1,2],xmm3[0]
; AVX512-NEXT:    vinsertf128 $1, %xmm2, %ymm3, %ymm2
; AVX512-NEXT:    vextractf128 $1, %ymm0, %xmm3
; AVX512-NEXT:    vextractf128 $1, %ymm1, %xmm4
; AVX512-NEXT:    vcvtdq2ps %xmm4, %xmm5
; AVX512-NEXT:    vscalefss %xmm5, %xmm3, %xmm5
; AVX512-NEXT:    vmovshdup {{.*#+}} xmm6 = xmm3[1,1,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm7 = xmm4[1,1,1,1]
; AVX512-NEXT:    vcvtdq2ps %xmm7, %xmm7
; AVX512-NEXT:    vscalefss %xmm7, %xmm6, %xmm6
; AVX512-NEXT:    vunpcklps {{.*#+}} xmm5 = xmm5[0],xmm6[0],xmm5[1],xmm6[1]
; AVX512-NEXT:    vshufpd {{.*#+}} xmm6 = xmm3[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm7 = xmm4[2,3,2,3]
; AVX512-NEXT:    vcvtdq2ps %xmm7, %xmm7
; AVX512-NEXT:    vscalefss %xmm7, %xmm6, %xmm6
; AVX512-NEXT:    vmovlhps {{.*#+}} xmm5 = xmm5[0],xmm6[0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm3 = xmm3[3,3,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm4 = xmm4[3,3,3,3]
; AVX512-NEXT:    vcvtdq2ps %xmm4, %xmm4
; AVX512-NEXT:    vscalefss %xmm4, %xmm3, %xmm3
; AVX512-NEXT:    vinsertps {{.*#+}} xmm3 = xmm5[0,1,2],xmm3[0]
; AVX512-NEXT:    vcvtdq2ps %xmm1, %xmm4
; AVX512-NEXT:    vscalefss %xmm4, %xmm0, %xmm4
; AVX512-NEXT:    vmovshdup {{.*#+}} xmm5 = xmm0[1,1,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm6 = xmm1[1,1,1,1]
; AVX512-NEXT:    vcvtdq2ps %xmm6, %xmm6
; AVX512-NEXT:    vscalefss %xmm6, %xmm5, %xmm5
; AVX512-NEXT:    vunpcklps {{.*#+}} xmm4 = xmm4[0],xmm5[0],xmm4[1],xmm5[1]
; AVX512-NEXT:    vshufpd {{.*#+}} xmm5 = xmm0[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm6 = xmm1[2,3,2,3]
; AVX512-NEXT:    vcvtdq2ps %xmm6, %xmm6
; AVX512-NEXT:    vscalefss %xmm6, %xmm5, %xmm5
; AVX512-NEXT:    vmovlhps {{.*#+}} xmm4 = xmm4[0],xmm5[0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; AVX512-NEXT:    vshufps {{.*#+}} xmm1 = xmm1[3,3,3,3]
; AVX512-NEXT:    vcvtdq2ps %xmm1, %xmm1
; AVX512-NEXT:    vscalefss %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vinsertps {{.*#+}} xmm0 = xmm4[0,1,2],xmm0[0]
; AVX512-NEXT:    vinsertf128 $1, %xmm3, %ymm0, %ymm0
; AVX512-NEXT:    vinsertf64x4 $1, %ymm2, %zmm0, %zmm0
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_ldexp_16xfloat:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vcvtdq2ps %zmm1, %zmm1
; AVX512VL-NEXT:    vscalefps %zmm1, %zmm0, %zmm0
; AVX512VL-NEXT:    retq
  %r = call <16 x float> @llvm.ldexp.v16f32.v16i32(<16 x float> %x, <16 x i32> %exp)
  ret <16 x float> %r
}
declare <16 x float> @llvm.ldexp.v16f32.v16i32(<16 x float>, <16 x i32>)

define <8 x double> @test_ldexp_8xdouble(<8 x double> %x, <8 x i32> %exp) {
; AVX512-LABEL: test_ldexp_8xdouble:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vextractf32x4 $3, %zmm0, %xmm2
; AVX512-NEXT:    vextractf128 $1, %ymm1, %xmm3
; AVX512-NEXT:    vshufps {{.*#+}} xmm4 = xmm3[2,3,2,3]
; AVX512-NEXT:    vcvtdq2pd %xmm4, %xmm4
; AVX512-NEXT:    vscalefsd %xmm4, %xmm2, %xmm4
; AVX512-NEXT:    vextractf32x4 $2, %zmm0, %xmm5
; AVX512-NEXT:    vcvtdq2pd %xmm3, %xmm6
; AVX512-NEXT:    vscalefsd %xmm6, %xmm5, %xmm6
; AVX512-NEXT:    vinsertf128 $1, %xmm4, %ymm6, %ymm4
; AVX512-NEXT:    vextractf128 $1, %ymm0, %xmm6
; AVX512-NEXT:    vshufps {{.*#+}} xmm7 = xmm1[2,3,2,3]
; AVX512-NEXT:    vcvtdq2pd %xmm7, %xmm7
; AVX512-NEXT:    vscalefsd %xmm7, %xmm6, %xmm7
; AVX512-NEXT:    vcvtdq2pd %xmm1, %xmm8
; AVX512-NEXT:    vscalefsd %xmm8, %xmm0, %xmm8
; AVX512-NEXT:    vinsertf128 $1, %xmm7, %ymm8, %ymm7
; AVX512-NEXT:    vinsertf64x4 $1, %ymm4, %zmm7, %zmm4
; AVX512-NEXT:    vshufpd {{.*#+}} xmm2 = xmm2[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm7 = xmm3[3,3,3,3]
; AVX512-NEXT:    vcvtdq2pd %xmm7, %xmm7
; AVX512-NEXT:    vscalefsd %xmm7, %xmm2, %xmm2
; AVX512-NEXT:    vshufpd {{.*#+}} xmm5 = xmm5[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm3 = xmm3[1,1,1,1]
; AVX512-NEXT:    vcvtdq2pd %xmm3, %xmm3
; AVX512-NEXT:    vscalefsd %xmm3, %xmm5, %xmm3
; AVX512-NEXT:    vinsertf128 $1, %xmm2, %ymm3, %ymm2
; AVX512-NEXT:    vshufps {{.*#+}} xmm3 = xmm1[3,3,3,3]
; AVX512-NEXT:    vcvtdq2pd %xmm3, %xmm3
; AVX512-NEXT:    vshufpd {{.*#+}} xmm5 = xmm6[1,0]
; AVX512-NEXT:    vscalefsd %xmm3, %xmm5, %xmm3
; AVX512-NEXT:    vshufpd {{.*#+}} xmm0 = xmm0[1,0]
; AVX512-NEXT:    vshufps {{.*#+}} xmm1 = xmm1[1,1,1,1]
; AVX512-NEXT:    vcvtdq2pd %xmm1, %xmm1
; AVX512-NEXT:    vscalefsd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vinsertf128 $1, %xmm3, %ymm0, %ymm0
; AVX512-NEXT:    vinsertf64x4 $1, %ymm2, %zmm0, %zmm0
; AVX512-NEXT:    vunpcklpd {{.*#+}} zmm0 = zmm4[0],zmm0[0],zmm4[2],zmm0[2],zmm4[4],zmm0[4],zmm4[6],zmm0[6]
; AVX512-NEXT:    retq
;
; AVX512VL-LABEL: test_ldexp_8xdouble:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vcvtdq2pd %ymm1, %zmm1
; AVX512VL-NEXT:    vscalefpd %zmm1, %zmm0, %zmm0
; AVX512VL-NEXT:    retq
  %r = call <8 x double> @llvm.ldexp.v8f64.v8i32(<8 x double> %x, <8 x i32> %exp)
  ret <8 x double> %r
}
declare <8 x double> @llvm.ldexp.v8f64.v8i32(<8 x double>, <8 x i32>)
