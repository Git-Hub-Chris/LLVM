; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mcpu=skylake-avx512 | FileCheck %s

define dso_local void @foo_16_ne(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_16_ne:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovw %ecx, %k1
; CHECK-NEXT:    vmovups (%rdx), %zmm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %zmm1 {%k1} {z}
; CHECK-NEXT:    vaddps %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = and i32 %mask, 65535
  %.splatinsert = insertelement <16 x i32> poison, i32 %0, i64 0
  %.splat = shufflevector <16 x i32> %.splatinsert, <16 x i32> poison, <16 x i32> zeroinitializer
  %1 = and <16 x i32> %.splat, <i32 1, i32 2, i32 4, i32 8, i32 16, i32 32, i32 64, i32 128, i32 256, i32 512, i32 1024, i32 2048, i32 4096, i32 8192, i32 16384, i32 32768>
  %hir.cmp.45 = icmp ne <16 x i32> %1, zeroinitializer
  %2 = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %b, i32 4, <16 x i1> %hir.cmp.45, <16 x float> poison)
  %3 = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %a, i32 4, <16 x i1> %hir.cmp.45, <16 x float> poison)
  %4 = fadd reassoc nsz arcp contract afn <16 x float> %2, %3
  tail call void @llvm.masked.store.v16f32.p0(<16 x float> %4, ptr %c, i32 4, <16 x i1> %hir.cmp.45)
  ret void
}

; Function Attrs: mustprogress nounwind uwtable
define dso_local void @foo_16_eq(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_16_eq:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovw %ecx, %k0
; CHECK-NEXT:    knotw %k0, %k1
; CHECK-NEXT:    vmovups (%rdx), %zmm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %zmm1 {%k1} {z}
; CHECK-NEXT:    vaddps %zmm1, %zmm0, %zmm0
; CHECK-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = and i32 %mask, 65535
  %.splatinsert = insertelement <16 x i32> poison, i32 %0, i64 0
  %.splat = shufflevector <16 x i32> %.splatinsert, <16 x i32> poison, <16 x i32> zeroinitializer
  %1 = and <16 x i32> %.splat, <i32 1, i32 2, i32 4, i32 8, i32 16, i32 32, i32 64, i32 128, i32 256, i32 512, i32 1024, i32 2048, i32 4096, i32 8192, i32 16384, i32 32768>
  %hir.cmp.45 = icmp eq <16 x i32> %1, zeroinitializer
  %2 = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %b, i32 4, <16 x i1> %hir.cmp.45, <16 x float> poison)
  %3 = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %a, i32 4, <16 x i1> %hir.cmp.45, <16 x float> poison)
  %4 = fadd reassoc nsz arcp contract afn <16 x float> %2, %3
  tail call void @llvm.masked.store.v16f32.p0(<16 x float> %4, ptr %c, i32 4, <16 x i1> %hir.cmp.45)
  ret void
}

define dso_local void @foo_8_ne(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_8_ne:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovb %ecx, %k1
; CHECK-NEXT:    vmovups (%rdx), %ymm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %ymm1 {%k1} {z}
; CHECK-NEXT:    vaddps %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    vmovups %ymm0, (%rdi) {%k1}
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = and i32 %mask, 65535
  %.splatinsert = insertelement <8 x i32> poison, i32 %0, i64 0
  %.splat = shufflevector <8 x i32> %.splatinsert, <8 x i32> poison, <8 x i32> zeroinitializer
  %1 = and <8 x i32> %.splat, <i32 1, i32 2, i32 4, i32 8, i32 16, i32 32, i32 64, i32 128>
  %hir.cmp.45 = icmp ne <8 x i32> %1, zeroinitializer
  %2 = tail call <8 x float> @llvm.masked.load.v8f32.p0(ptr %b, i32 4, <8 x i1> %hir.cmp.45, <8 x float> poison)
  %3 = tail call <8 x float> @llvm.masked.load.v8f32.p0(ptr %a, i32 4, <8 x i1> %hir.cmp.45, <8 x float> poison)
  %4 = fadd reassoc nsz arcp contract afn <8 x float> %2, %3
  tail call void @llvm.masked.store.v8f32.p0(<8 x float> %4, ptr %c, i32 4, <8 x i1> %hir.cmp.45)
  ret void
}

define dso_local void @foo_8_eq(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_8_eq:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovb %ecx, %k0
; CHECK-NEXT:    knotb %k0, %k1
; CHECK-NEXT:    vmovups (%rdx), %ymm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %ymm1 {%k1} {z}
; CHECK-NEXT:    vaddps %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    vmovups %ymm0, (%rdi) {%k1}
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = and i32 %mask, 65535
  %.splatinsert = insertelement <8 x i32> poison, i32 %0, i64 0
  %.splat = shufflevector <8 x i32> %.splatinsert, <8 x i32> poison, <8 x i32> zeroinitializer
  %1 = and <8 x i32> %.splat, <i32 1, i32 2, i32 4, i32 8, i32 16, i32 32, i32 64, i32 128>
  %hir.cmp.45 = icmp eq <8 x i32> %1, zeroinitializer
  %2 = tail call <8 x float> @llvm.masked.load.v8f32.p0(ptr %b, i32 4, <8 x i1> %hir.cmp.45, <8 x float> poison)
  %3 = tail call <8 x float> @llvm.masked.load.v8f32.p0(ptr %a, i32 4, <8 x i1> %hir.cmp.45, <8 x float> poison)
  %4 = fadd reassoc nsz arcp contract afn <8 x float> %2, %3
  tail call void @llvm.masked.store.v8f32.p0(<8 x float> %4, ptr %c, i32 4, <8 x i1> %hir.cmp.45)
  ret void
}

define dso_local void @foo_4_ne(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_4_ne:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovb %ecx, %k1
; CHECK-NEXT:    vmovups (%rdx), %xmm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %xmm1 {%k1} {z}
; CHECK-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; CHECK-NEXT:    retq
entry:
  %0 = and i32 %mask, 65535
  %.splatinsert = insertelement <4 x i32> poison, i32 %0, i64 0
  %.splat = shufflevector <4 x i32> %.splatinsert, <4 x i32> poison, <4 x i32> zeroinitializer
  %1 = and <4 x i32> %.splat, <i32 1, i32 2, i32 4, i32 8>
  %hir.cmp.45 = icmp ne <4 x i32> %1, zeroinitializer
  %2 = tail call <4 x float> @llvm.masked.load.v4f32.p0(ptr %b, i32 4, <4 x i1> %hir.cmp.45, <4 x float> poison)
  %3 = tail call <4 x float> @llvm.masked.load.v4f32.p0(ptr %a, i32 4, <4 x i1> %hir.cmp.45, <4 x float> poison)
  %4 = fadd reassoc nsz arcp contract afn <4 x float> %2, %3
  tail call void @llvm.masked.store.v4f32.p0(<4 x float> %4, ptr %c, i32 4, <4 x i1> %hir.cmp.45)
  ret void
}

define dso_local void @foo_4_eq(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_4_eq:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovb %ecx, %k0
; CHECK-NEXT:    knotb %k0, %k1
; CHECK-NEXT:    vmovups (%rdx), %xmm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %xmm1 {%k1} {z}
; CHECK-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; CHECK-NEXT:    retq
entry:
  %0 = and i32 %mask, 65535
  %.splatinsert = insertelement <4 x i32> poison, i32 %0, i64 0
  %.splat = shufflevector <4 x i32> %.splatinsert, <4 x i32> poison, <4 x i32> zeroinitializer
  %1 = and <4 x i32> %.splat, <i32 1, i32 2, i32 4, i32 8>
  %hir.cmp.45 = icmp eq <4 x i32> %1, zeroinitializer
  %2 = tail call <4 x float> @llvm.masked.load.v4f32.p0(ptr %b, i32 4, <4 x i1> %hir.cmp.45, <4 x float> poison)
  %3 = tail call <4 x float> @llvm.masked.load.v4f32.p0(ptr %a, i32 4, <4 x i1> %hir.cmp.45, <4 x float> poison)
  %4 = fadd reassoc nsz arcp contract afn <4 x float> %2, %3
  tail call void @llvm.masked.store.v4f32.p0(<4 x float> %4, ptr %c, i32 4, <4 x i1> %hir.cmp.45)
  ret void
}

define dso_local void @foo_2_ne(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_2_ne:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovb %ecx, %k0
; CHECK-NEXT:    kshiftlb $6, %k0, %k0
; CHECK-NEXT:    kshiftrb $6, %k0, %k1
; CHECK-NEXT:    vmovups (%rdx), %xmm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %xmm1 {%k1} {z}
; CHECK-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; CHECK-NEXT:    retq
entry:
  %.splatinsert = insertelement <2 x i32> poison, i32 %mask, i64 0
  %.splat = shufflevector <2 x i32> %.splatinsert, <2 x i32> poison, <2 x i32> zeroinitializer
  %0 = and <2 x i32> %.splat, <i32 1, i32 2>
  %hir.cmp.44 = icmp ne <2 x i32> %0, zeroinitializer
  %1 = tail call <2 x float> @llvm.masked.load.v2f32.p0(ptr %b, i32 4, <2 x i1> %hir.cmp.44, <2 x float> poison)
  %2 = tail call <2 x float> @llvm.masked.load.v2f32.p0(ptr %a, i32 4, <2 x i1> %hir.cmp.44, <2 x float> poison)
  %3 = fadd reassoc nsz arcp contract afn <2 x float> %1, %2
  tail call void @llvm.masked.store.v2f32.p0(<2 x float> %3, ptr %c, i32 4, <2 x i1> %hir.cmp.44)
  ret void
}

define dso_local void @foo_2_eq(ptr nocapture noundef writeonly %c, ptr nocapture noundef readonly %a, ptr nocapture noundef readonly %b, i32 noundef %mask) {
; CHECK-LABEL: foo_2_eq:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    kmovb %ecx, %k0
; CHECK-NEXT:    knotb %k0, %k0
; CHECK-NEXT:    kshiftlb $6, %k0, %k0
; CHECK-NEXT:    kshiftrb $6, %k0, %k1
; CHECK-NEXT:    vmovups (%rdx), %xmm0 {%k1} {z}
; CHECK-NEXT:    vmovups (%rsi), %xmm1 {%k1} {z}
; CHECK-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; CHECK-NEXT:    retq
entry:
  %.splatinsert = insertelement <2 x i32> poison, i32 %mask, i64 0
  %.splat = shufflevector <2 x i32> %.splatinsert, <2 x i32> poison, <2 x i32> zeroinitializer
  %0 = and <2 x i32> %.splat, <i32 1, i32 2>
  %hir.cmp.44 = icmp eq <2 x i32> %0, zeroinitializer
  %1 = tail call <2 x float> @llvm.masked.load.v2f32.p0(ptr %b, i32 4, <2 x i1> %hir.cmp.44, <2 x float> poison)
  %2 = tail call <2 x float> @llvm.masked.load.v2f32.p0(ptr %a, i32 4, <2 x i1> %hir.cmp.44, <2 x float> poison)
  %3 = fadd reassoc nsz arcp contract afn <2 x float> %1, %2
  tail call void @llvm.masked.store.v2f32.p0(<2 x float> %3, ptr %c, i32 4, <2 x i1> %hir.cmp.44)
  ret void
}

declare <2 x float> @llvm.masked.load.v2f32.p0(ptr nocapture, i32 immarg, <2 x i1>, <2 x float>) #1

declare void @llvm.masked.store.v2f32.p0(<2 x float>, ptr nocapture, i32 immarg, <2 x i1>) #2

declare <4 x float> @llvm.masked.load.v4f32.p0(ptr nocapture, i32 immarg, <4 x i1>, <4 x float>) #1

declare void @llvm.masked.store.v4f32.p0(<4 x float>, ptr nocapture, i32 immarg, <4 x i1>) #2

declare <8 x float> @llvm.masked.load.v8f32.p0(ptr nocapture, i32 immarg, <8 x i1>, <8 x float>)

declare void @llvm.masked.store.v8f32.p0(<8 x float>, ptr nocapture, i32 immarg, <8 x i1>)

declare <16 x float> @llvm.masked.load.v16f32.p0(ptr nocapture, i32 immarg, <16 x i1>, <16 x float>)

declare void @llvm.masked.store.v16f32.p0(<16 x float>, ptr nocapture, i32 immarg, <16 x i1>)
