; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 3
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+sse2 | FileCheck %s --check-prefixes=X86
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx | FileCheck %s --check-prefixes=X64
; RUN: llc < %s -mtriple=x86_64-unknown-unknown | FileCheck %s --check-prefixes=CHECK-SSE
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx2 | FileCheck %s --check-prefixes=CHECK-AVX2
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512f | FileCheck %s --check-prefixes=CHECK-NO-FASTFMA
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mcpu=skx -fp-contract=fast | FileCheck %s --check-prefixes=CHECK-FMA

define i32 @sext_known_nonzero(i16 %xx) {
; X86-LABEL: sext_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    movzwl	%ax, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sext_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $256, %eax # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    movzwl	%ax, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %x = shl nuw nsw i16 256, %xx
  %z = sext i16 %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define <2 x float> @fmul_pow_shl_cnt_vec_fail_expensive_cast(<2 x i64> %cnt) nounwind {
; CHECK-SSE-LABEL: fmul_pow_shl_cnt_vec_fail_expensive_cast:
; CHECK-SSE:       # %bb.0:
; CHECK-SSE-NEXT:    pshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; CHECK-SSE-NEXT:    movdqa {{.*#+}} xmm2 = [2,2]
; CHECK-SSE-NEXT:    movdqa	%xmm2, %xmm3
; CHECK-SSE-NEXT:    psllq	%xmm1, %xmm3
; CHECK-SSE-NEXT:    psllq	%xmm0, %xmm2
; CHECK-SSE-NEXT:    movq	%xmm2, %rax
; CHECK-SSE-NEXT:    xorps	%xmm0, %xmm0
; CHECK-SSE-NEXT:    cvtsi2ss	%rax, %xmm0
; CHECK-SSE-NEXT:    pshufd	$238, %xmm3, %xmm1              # xmm1 = xmm3[2,3,2,3]
; CHECK-SSE-NEXT:    movq	%xmm1, %rax
; CHECK-SSE-NEXT:    xorps	%xmm1, %xmm1
; CHECK-SSE-NEXT:    cvtsi2ss	%rax, %xmm1
; CHECK-SSE-NEXT:    unpcklps	%xmm1, %xmm0                    # xmm0 = xmm0[0],xmm1[0],xmm0[1],xmm1[1]
;
; CHECK-AVX2-LABEL: fmul_pow_shl_cnt_vec_fail_expensive_cast:
; CHECK-AVX2:       # %bb.0:
; CHECK-AVX2-NEXT:	vpmovsxbq {{.*#+}} xmm1 = [2,2]
; CHECK-AVX2-NEXT:	vpsllvq	%xmm0, %xmm1, %xmm0
; CHECK-AVX2-NEXT:	vpextrq	$1, %xmm0, %rax
; CHECK-AVX2-NEXT:	vcvtsi2ss	%rax, %xmm2, %xmm1
; CHECK-AVX2-NEXT:	vmovq	%xmm0, %rax
; CHECK-AVX2-NEXT:	vcvtsi2ss	%rax, %xmm2, %xmm0
; CHECK-AVX2-NEXT:	vinsertps	{{.*#+}} xmm0 = xmm0[0],xmm1[0],zero,zero
; CHECK-AVX2-NEXT:	vbroadcastss {{.*#+}} xmm1 = [1.5E+1,1.5E+1,1.5E+1,1.5E+1]
; CHECK-AVX2-NEXT:	vmulps	%xmm1, %xmm0, %xmm0
; CHECK-AVX2-NEXT:	retq
;
; CHECK-NO-FASTFMA-LABEL: fmul_pow_shl_cnt_vec_fail_expensive_cast:
; CHECK-NO-FASTFMA:       # %bb.0:
; CHECK-NO-FASTFMA-NEXT:	vpmovsxbq	{{.*#+}} xmm1 = [2,2]
; CHECK-NO-FASTFMA-NEXT:	vpsllvq	%xmm0, %xmm1, %xmm0
; CHECK-NO-FASTFMA-NEXT:	vpextrq	$1, %xmm0, %rax
; CHECK-NO-FASTFMA-NEXT:	vcvtsi2ss	%rax, %xmm2, %xmm1
; CHECK-NO-FASTFMA-NEXT:	vmovq	%xmm0, %rax
; CHECK-NO-FASTFMA-NEXT:	vcvtsi2ss	%rax, %xmm2, %xmm0
; CHECK-NO-FASTFMA-NEXT:	vinsertps	{{.*#+}} xmm0 = xmm0[0],xmm1[0],zero,zero
; CHECK-NO-FASTFMA-NEXT:	vbroadcastss	{{.*#+}} xmm1 = [1.5E+1,1.5E+1,1.5E+1,1.5E+1]
; CHECK-NO-FASTFMA-NEXT:	vmulps	%xmm1, %xmm0, %xmm0
; CHECK-NO-FASTFMA-NEXT:	retq
;
; CHECK-FMA-LABEL: fmul_pow_shl_cnt_vec_fail_expensive_cast:
; CHECK-FMA:       # %bb.0:
; CHECK-FMA-NEXT:	vpbroadcastq	{{.*#+}} xmm1 = [2,2]
; CHECK-FMA-NEXT:	vpsllvq	%xmm0, %xmm1, %xmm0
; CHECK-FMA-NEXT:	vcvtqq2ps	%xmm0, %xmm0
; CHECK-FMA-NEXT:	vmulps {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to4}, %xmm0, %xmm0
; CHECK-FMA-NEXT:	retq
  %shl = shl nsw nuw <2 x i64> <i64 2, i64 2>, %cnt
  %conv = uitofp <2 x i64> %shl to <2 x float>
  %mul = fmul <2 x float> <float 15.000000e+00, float 15.000000e+00>, %conv
  ret <2 x float> %mul
}

define <4 x float> @fmul_pow_shl_cnt_vec_preserve_fma(<4 x i32> %cnt, <4 x float> %add) nounwind {
; CHECK-SSE-LABEL: fmul_pow_shl_cnt_vec_preserve_fma:
; CHECK-SSE:       # %bb.0:
; CHECK-SSE-NEXT:    pslld $23, %xmm0
; CHECK-SSE-NEXT:    paddd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; CHECK-SSE-NEXT:    addps %xmm1, %xmm0
; CHECK-SSE-NEXT:    retq
;
; CHECK-AVX2-LABEL: fmul_pow_shl_cnt_vec_preserve_fma:
; CHECK-AVX2:       # %bb.0:
; CHECK-AVX2-NEXT:    vpslld $23, %xmm0, %xmm0
; CHECK-AVX2-NEXT:    vpbroadcastd {{.*#+}} xmm2 = [1092616192,1092616192,1092616192,1092616192]
; CHECK-AVX2-NEXT:    vpaddd %xmm2, %xmm0, %xmm0
; CHECK-AVX2-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-AVX2-NEXT:    retq
;
; CHECK-NO-FASTFMA-LABEL: fmul_pow_shl_cnt_vec_preserve_fma:
; CHECK-NO-FASTFMA:       # %bb.0:
; CHECK-NO-FASTFMA-NEXT:    vpslld $23, %xmm0, %xmm0
; CHECK-NO-FASTFMA-NEXT:    vpbroadcastd {{.*#+}} xmm2 = [1092616192,1092616192,1092616192,1092616192]
; CHECK-NO-FASTFMA-NEXT:    vpaddd %xmm2, %xmm0, %xmm0
; CHECK-NO-FASTFMA-NEXT:    vaddps %xmm1, %xmm0, %xmm0
; CHECK-NO-FASTFMA-NEXT:    retq
;
; CHECK-FMA-LABEL: fmul_pow_shl_cnt_vec_preserve_fma:
; CHECK-FMA:       # %bb.0:
; CHECK-FMA-NEXT:    vpbroadcastd {{.*#+}} xmm2 = [2,2,2,2]
; CHECK-FMA-NEXT:    vpsllvd %xmm0, %xmm2, %xmm0
; CHECK-FMA-NEXT:	   vcvtdq2ps	%xmm0, %xmm0
; CHECK-FMA-NEXT:    vfmadd132ps {{.*#+}} xmm0 = (xmm0 * mem) + xmm1
; CHECK-FMA-NEXT:    retq
  %shl = shl nsw nuw <4 x i32> <i32 2, i32 2, i32 2, i32 2>, %cnt
  %conv = uitofp <4 x i32> %shl to <4 x float>
  %mul = fmul <4 x float> <float 5.000000e+00, float 5.000000e+00, float 5.000000e+00, float 5.000000e+00>, %conv
  %res = fadd <4 x float> %mul, %add
  ret <4 x float> %res
}
