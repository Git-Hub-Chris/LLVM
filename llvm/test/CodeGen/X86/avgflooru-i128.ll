; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=x86_64 < %s | FileCheck %s

define i128 @avgflooru_i128(i128 %x, i128 %y) {
; CHECK-LABEL: avgflooru_i128:
; CHECK:       # %bb.0: # %start
; CHECK-NEXT:    movq %rdi, %rax
; CHECK-NEXT:    addq %rdx, %rax
; CHECK-NEXT:    adcq %rcx, %rsi
; CHECK-NEXT:    setb %cl
; CHECK-NEXT:    setb %dl
; CHECK-NEXT:    movzbl %dl, %edi
; CHECK-NEXT:    shlq $63, %rdi
; CHECK-NEXT:    xorl %edx, %edx
; CHECK-NEXT:    testb %cl, %cl
; CHECK-NEXT:    cmovneq %rdi, %rdx
; CHECK-NEXT:    shrdq $1, %rsi, %rax
; CHECK-NEXT:    shrq %rsi
; CHECK-NEXT:    orq %rsi, %rdx
; CHECK-NEXT:    retq
start:
  %xor = xor i128 %y, %x
  %lshr = lshr i128 %xor, 1
  %and = and i128 %y, %x
  %add = add i128 %lshr, %and
  ret i128 %add
}

declare void @use(i8)

define i128 @avgflooru_i128_multi_use(i128 %x, i128 %y) nounwind {
; CHECK-LABEL: avgflooru_i128_multi_use:
; CHECK:       # %bb.0: # %start
; CHECK-NEXT:    pushq %r15
; CHECK-NEXT:    pushq %r14
; CHECK-NEXT:    pushq %r13
; CHECK-NEXT:    pushq %r12
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    movq %rsi, %r15
; CHECK-NEXT:    movq %rdi, %r12
; CHECK-NEXT:    movq %rdi, %rbx
; CHECK-NEXT:    addq %rdx, %rbx
; CHECK-NEXT:    movq %rsi, %r14
; CHECK-NEXT:    adcq %rcx, %r14
; CHECK-NEXT:    setb %al
; CHECK-NEXT:    setb %sil
; CHECK-NEXT:    movzbl %sil, %esi
; CHECK-NEXT:    shlq $63, %rsi
; CHECK-NEXT:    xorl %r13d, %r13d
; CHECK-NEXT:    testb %al, %al
; CHECK-NEXT:    cmovneq %rsi, %r13
; CHECK-NEXT:    xorq %rcx, %r15
; CHECK-NEXT:    xorq %rdx, %r12
; CHECK-NEXT:    movq %r12, %rdi
; CHECK-NEXT:    movq %r15, %rsi
; CHECK-NEXT:    callq use@PLT
; CHECK-NEXT:    shrdq $1, %r15, %r12
; CHECK-NEXT:    shrq %r15
; CHECK-NEXT:    movq %r12, %rdi
; CHECK-NEXT:    movq %r15, %rsi
; CHECK-NEXT:    callq use@PLT
; CHECK-NEXT:    shrdq $1, %r14, %rbx
; CHECK-NEXT:    shrq %r14
; CHECK-NEXT:    orq %r13, %r14
; CHECK-NEXT:    movq %rbx, %rax
; CHECK-NEXT:    movq %r14, %rdx
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    popq %r12
; CHECK-NEXT:    popq %r13
; CHECK-NEXT:    popq %r14
; CHECK-NEXT:    popq %r15
; CHECK-NEXT:    retq
start:
  %xor = xor i128 %y, %x
  call void @use(i128 %xor)
  %lshr = lshr i128 %xor, 1
  call void @use(i128 %lshr)
  %and = and i128 %y, %x
  %add = add i128 %lshr, %and
  ret i128 %add
}

; This test case shouldn't combine because it's not
; an avgflooru operation

define i128 @avgflooru_i128_negative(i128 %x, i128 %y) {
; CHECK-LABEL: avgflooru_i128_negative:
; CHECK:       # %bb.0: # %start
; CHECK-NEXT:    movq %rdi, %rax
; CHECK-NEXT:    andq %rsi, %rcx
; CHECK-NEXT:    notq %rsi
; CHECK-NEXT:    andq %rdi, %rdx
; CHECK-NEXT:    notq %rax
; CHECK-NEXT:    addq %rdx, %rax
; CHECK-NEXT:    adcq %rcx, %rsi
; CHECK-NEXT:    movq %rsi, %rdx
; CHECK-NEXT:    retq
start:
  %xor = xor i128 %x, -1
  %and = and i128 %y, %x
  %add = add i128 %xor, %and
  ret i128 %add
}

; This negative test case shouldn't combine, i32 is already properly 
; handled in terms of legalization, compared to the i128

define i32 @avgflooru_i128_negative2(i32 %x, i32 %y) {
; CHECK-LABEL: avgflooru_i128_negative2:
; CHECK:       # %bb.0: # %start
; CHECK-NEXT:    movl %edi, %ecx
; CHECK-NEXT:    movl %esi, %eax
; CHECK-NEXT:    addq %rcx, %rax
; CHECK-NEXT:    shrq %rax
; CHECK-NEXT:    # kill: def $eax killed $eax killed $rax
; CHECK-NEXT:    retq
start:
  %xor = xor i32 %y, %x
  %lshr = lshr i32 %xor, 1
  %and = and i32 %y, %x
  %add = add i32 %lshr, %and
  ret i32 %add
}

define <2 x i128> @avgflooru_i128_vec(<2 x i128> %x, <2 x i128> %y) {
; CHECK-LABEL: avgflooru_i128_vec:
; CHECK:       # %bb.0: # %start
; CHECK-NEXT:    movq %rdi, %rax
; CHECK-NEXT:    addq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    adcq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    setb %dil
; CHECK-NEXT:    setb %r9b
; CHECK-NEXT:    movzbl %r9b, %r9d
; CHECK-NEXT:    shlq $63, %r9
; CHECK-NEXT:    xorl %r10d, %r10d
; CHECK-NEXT:    testb %dil, %dil
; CHECK-NEXT:    cmoveq %r10, %r9
; CHECK-NEXT:    addq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    adcq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    setb %dil
; CHECK-NEXT:    setb %r11b
; CHECK-NEXT:    movzbl %r11b, %r11d
; CHECK-NEXT:    shlq $63, %r11
; CHECK-NEXT:    testb %dil, %dil
; CHECK-NEXT:    cmoveq %r10, %r11
; CHECK-NEXT:    movq %rdx, %rdi
; CHECK-NEXT:    shrq %rdi
; CHECK-NEXT:    orq %r9, %rdi
; CHECK-NEXT:    movq %r8, %r9
; CHECK-NEXT:    shrq %r9
; CHECK-NEXT:    orq %r11, %r9
; CHECK-NEXT:    shldq $63, %rsi, %rdx
; CHECK-NEXT:    shldq $63, %rcx, %r8
; CHECK-NEXT:    movq %r8, 16(%rax)
; CHECK-NEXT:    movq %rdx, (%rax)
; CHECK-NEXT:    movq %r9, 24(%rax)
; CHECK-NEXT:    movq %rdi, 8(%rax)
; CHECK-NEXT:    retq
start:
  %xor = xor <2 x i128> %y, %x
  %lshr = lshr <2 x i128> %xor, <i128 1, i128 1>
  %and = and <2 x i128> %y, %x
  %add = add <2 x i128> %lshr, %and
  ret <2 x i128> %add
}
