; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=aarch64 < %s | FileCheck %s

define i128 @avgflooru_i128(i128 %x, i128 %y) {
; CHECK-LABEL: avgflooru_i128:
; CHECK:       // %bb.0: // %start
; CHECK-NEXT:    adds x8, x0, x2
; CHECK-NEXT:    adcs x9, x1, x3
; CHECK-NEXT:    cset w10, hs
; CHECK-NEXT:    extr x0, x9, x8, #1
; CHECK-NEXT:    lsl x10, x10, #63
; CHECK-NEXT:    csel x1, x10, xzr, hs
; CHECK-NEXT:    bfxil x1, x9, #1, #63
; CHECK-NEXT:    ret
start:
  %xor = xor i128 %y, %x
  %lshr = lshr i128 %xor, 1
  %and = and i128 %y, %x
  %add = add i128 %lshr, %and
  ret i128 %add
}

declare void @use(i8)

define i128 @avgflooru_i128_multi_use(i128 %x, i128 %y) nounwind {
; CHECK-LABEL: avgflooru_i128_multi_use:
; CHECK:       // %bb.0: // %start
; CHECK-NEXT:    str x30, [sp, #-64]! // 8-byte Folded Spill
; CHECK-NEXT:    stp x24, x23, [sp, #16] // 16-byte Folded Spill
; CHECK-NEXT:    eor x23, x3, x1
; CHECK-NEXT:    eor x24, x2, x0
; CHECK-NEXT:    stp x22, x21, [sp, #32] // 16-byte Folded Spill
; CHECK-NEXT:    mov x21, x1
; CHECK-NEXT:    mov x22, x0
; CHECK-NEXT:    mov x0, x24
; CHECK-NEXT:    mov x1, x23
; CHECK-NEXT:    stp x20, x19, [sp, #48] // 16-byte Folded Spill
; CHECK-NEXT:    mov x19, x3
; CHECK-NEXT:    mov x20, x2
; CHECK-NEXT:    bl use
; CHECK-NEXT:    extr x0, x23, x24, #1
; CHECK-NEXT:    lsr x1, x23, #1
; CHECK-NEXT:    bl use
; CHECK-NEXT:    adds x8, x22, x20
; CHECK-NEXT:    ldp x24, x23, [sp, #16] // 16-byte Folded Reload
; CHECK-NEXT:    adcs x9, x21, x19
; CHECK-NEXT:    ldp x20, x19, [sp, #48] // 16-byte Folded Reload
; CHECK-NEXT:    cset w10, hs
; CHECK-NEXT:    ldp x22, x21, [sp, #32] // 16-byte Folded Reload
; CHECK-NEXT:    lsl x10, x10, #63
; CHECK-NEXT:    extr x0, x9, x8, #1
; CHECK-NEXT:    csel x1, x10, xzr, hs
; CHECK-NEXT:    bfxil x1, x9, #1, #63
; CHECK-NEXT:    ldr x30, [sp], #64 // 8-byte Folded Reload
; CHECK-NEXT:    ret
start:
  %xor = xor i128 %y, %x
  call void @use(i128 %xor)
  %lshr = lshr i128 %xor, 1
  call void @use(i128 %lshr)
  %and = and i128 %y, %x
  %add = add i128 %lshr, %and
  ret i128 %add
}

; the 'avgflooru_i128_negative` shouldn't combine because it's not
; an avgflooru operation, which is what we're targeting 

define i128 @avgflooru_i128_negative(i128 %x, i128 %y) {
; CHECK-LABEL: avgflooru_i128_negative:
; CHECK:       // %bb.0: // %start
; CHECK-NEXT:    mvn x8, x0
; CHECK-NEXT:    and x9, x2, x0
; CHECK-NEXT:    mvn x10, x1
; CHECK-NEXT:    and x11, x3, x1
; CHECK-NEXT:    adds x0, x8, x9
; CHECK-NEXT:    adc x1, x10, x11
; CHECK-NEXT:    ret
start:
  %xor = xor i128 %x, -1
  %and = and i128 %y, %x
  %add = add i128 %xor, %and
  ret i128 %add
}

; This negative test case shouldn't work, i32 is already properly 
; handled in terms of legalization, compared to the i128

define i32 @avgflooru_i128_negative2(i32 %x, i32 %y) {
; CHECK-LABEL: avgflooru_i128_negative2:
; CHECK:       // %bb.0: // %start
; CHECK-NEXT:    mov w8, w1
; CHECK-NEXT:    add x8, x8, w0, uxtw
; CHECK-NEXT:    lsr x0, x8, #1
; CHECK-NEXT:    // kill: def $w0 killed $w0 killed $x0
; CHECK-NEXT:    ret
start:
  %xor = xor i32 %y, %x
  %lshr = lshr i32 %xor, 1
  %and = and i32 %y, %x
  %add = add i32 %lshr, %and
  ret i32 %add
}

define <2 x i128> @avgflooru_i128_vec(<2 x i128> %x, <2 x i128> %y) {
; CHECK-LABEL: avgflooru_i128_vec:
; CHECK:       // %bb.0: // %start
; CHECK-NEXT:    adds x8, x0, x4
; CHECK-NEXT:    adcs x9, x1, x5
; CHECK-NEXT:    cset w10, hs
; CHECK-NEXT:    extr x0, x9, x8, #1
; CHECK-NEXT:    lsl x10, x10, #63
; CHECK-NEXT:    csel x1, x10, xzr, hs
; CHECK-NEXT:    adds x10, x2, x6
; CHECK-NEXT:    adcs x11, x3, x7
; CHECK-NEXT:    bfxil x1, x9, #1, #63
; CHECK-NEXT:    cset w12, hs
; CHECK-NEXT:    extr x10, x11, x10, #1
; CHECK-NEXT:    lsl x12, x12, #63
; CHECK-NEXT:    fmov d0, x10
; CHECK-NEXT:    csel x3, x12, xzr, hs
; CHECK-NEXT:    bfxil x3, x11, #1, #63
; CHECK-NEXT:    mov v0.d[1], x3
; CHECK-NEXT:    fmov x2, d0
; CHECK-NEXT:    ret
start:
  %xor = xor <2 x i128> %y, %x
  %lshr = lshr <2 x i128> %xor, <i128 1, i128 1>
  %and = and <2 x i128> %y, %x
  %add = add <2 x i128> %lshr, %and
  ret <2 x i128> %add
}
