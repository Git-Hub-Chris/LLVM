; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 3
; RUN: opt  -passes=asan -S < %s | FileCheck %s --check-prefix=OPT
; RUN: opt < %s -passes='asan,default<O3>' -o - | llc -O3 -mtriple=amdgcn-hsa-amdhsa -mcpu=gfx90a -o - | FileCheck %s --check-prefix=LLC-W64
; RUN: opt < %s -passes='asan,default<O3>' -o - | llc -O3 -mtriple=amdgcn-hsa-amdhsa -mcpu=gfx1100 -amdgpu-enable-delay-alu=0 -mattr=+wavefrontsize32,-wavefrontsize64 -o - | FileCheck %s --check-prefix=LLC-W32

; This test contains checks for opt and llc, to update use:
;   utils/update_test_checks.py --force-update
;   utils/update_llc_test_checks.py --force-update
;
; --force-update allows to override "Assertions have been autogenerated by" guard
target triple = "amdgcn-amd-amdhsa"

declare i32 @llvm.amdgcn.workitem.id.x() #0

define protected amdgpu_kernel void @global_loadstore_uniform(ptr addrspace(1) %ptr) sanitize_address {
; OPT-LABEL: define protected amdgpu_kernel void @global_loadstore_uniform(
; OPT-SAME: ptr addrspace(1) [[PTR:%.*]]) #[[ATTR1:[0-9]+]] {
; OPT-NEXT:  entry:
; OPT-NEXT:    [[TMP0:%.*]] = ptrtoint ptr addrspace(1) [[PTR]] to i64
; OPT-NEXT:    [[TMP1:%.*]] = lshr i64 [[TMP0]], 3
; OPT-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 2147450880
; OPT-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
; OPT-NEXT:    [[TMP4:%.*]] = load i8, ptr [[TMP3]], align 1
; OPT-NEXT:    [[TMP5:%.*]] = icmp ne i8 [[TMP4]], 0
; OPT-NEXT:    br i1 [[TMP5]], label [[TMP6:%.*]], label [[TMP12:%.*]], !prof [[PROF0:![0-9]+]]
; OPT:       6:
; OPT-NEXT:    [[TMP7:%.*]] = and i64 [[TMP0]], 7
; OPT-NEXT:    [[TMP8:%.*]] = add i64 [[TMP7]], 3
; OPT-NEXT:    [[TMP9:%.*]] = trunc i64 [[TMP8]] to i8
; OPT-NEXT:    [[TMP10:%.*]] = icmp sge i8 [[TMP9]], [[TMP4]]
; OPT-NEXT:    br i1 [[TMP10]], label [[TMP11:%.*]], label [[TMP12]]
; OPT:       11:
; OPT-NEXT:    call void @__asan_report_load4(i64 [[TMP0]]) #[[ATTR3:[0-9]+]]
; OPT-NEXT:    unreachable
; OPT:       12:
; OPT-NEXT:    [[VAL:%.*]] = load volatile i32, ptr addrspace(1) [[PTR]], align 4
; OPT-NEXT:    store volatile i32 [[VAL]], ptr addrspace(1) [[PTR]], align 4
; OPT-NEXT:    ret void
;
; LLC-W64-LABEL: global_loadstore_uniform:
; LLC-W64:       ; %bb.0: ; %entry
; LLC-W64-NEXT:    s_load_dwordx2 s[34:35], s[8:9], 0x0
; LLC-W64-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; LLC-W64-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; LLC-W64-NEXT:    s_add_u32 s0, s0, s17
; LLC-W64-NEXT:    s_addc_u32 s1, s1, 0
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    s_lshr_b64 s[12:13], s[34:35], 3
; LLC-W64-NEXT:    v_mov_b32_e32 v1, s12
; LLC-W64-NEXT:    v_add_co_u32_e32 v2, vcc, 0x7fff8000, v1
; LLC-W64-NEXT:    v_mov_b32_e32 v1, s13
; LLC-W64-NEXT:    v_addc_co_u32_e32 v3, vcc, 0, v1, vcc
; LLC-W64-NEXT:    flat_load_sbyte v1, v[2:3]
; LLC-W64-NEXT:    s_mov_b32 s32, 0
; LLC-W64-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W64-NEXT:    v_cmp_eq_u16_e64 s[36:37], 0, v1
; LLC-W64-NEXT:    v_cmp_ne_u16_e32 vcc, 0, v1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[38:39], vcc
; LLC-W64-NEXT:    s_cbranch_execnz .LBB0_3
; LLC-W64-NEXT:  ; %bb.1: ; %Flow
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[38:39]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[36:37]
; LLC-W64-NEXT:    s_cbranch_execnz .LBB0_6
; LLC-W64-NEXT:  .LBB0_2: ; %UnifiedReturnBlock
; LLC-W64-NEXT:    s_endpgm
; LLC-W64-NEXT:  .LBB0_3:
; LLC-W64-NEXT:    v_and_b32_e64 v2, s34, 7
; LLC-W64-NEXT:    v_add_u16_e32 v2, 3, v2
; LLC-W64-NEXT:    v_cmp_ge_i16_e32 vcc, v2, v1
; LLC-W64-NEXT:    s_mov_b64 s[12:13], -1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[40:41], vcc
; LLC-W64-NEXT:    s_cbranch_execz .LBB0_5
; LLC-W64-NEXT:  ; %bb.4:
; LLC-W64-NEXT:    s_add_u32 s8, s8, 8
; LLC-W64-NEXT:    s_addc_u32 s9, s9, 0
; LLC-W64-NEXT:    s_getpc_b64 s[12:13]
; LLC-W64-NEXT:    s_add_u32 s12, s12, __asan_report_load4@gotpcrel32@lo+4
; LLC-W64-NEXT:    s_addc_u32 s13, s13, __asan_report_load4@gotpcrel32@hi+12
; LLC-W64-NEXT:    s_load_dwordx2 s[18:19], s[12:13], 0x0
; LLC-W64-NEXT:    s_mov_b32 s12, s14
; LLC-W64-NEXT:    s_mov_b32 s13, s15
; LLC-W64-NEXT:    s_mov_b32 s14, s16
; LLC-W64-NEXT:    v_mov_b32_e32 v31, v0
; LLC-W64-NEXT:    v_mov_b32_e32 v0, s34
; LLC-W64-NEXT:    v_mov_b32_e32 v1, s35
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    s_swappc_b64 s[30:31], s[18:19]
; LLC-W64-NEXT:    s_xor_b64 s[12:13], exec, -1
; LLC-W64-NEXT:    ; divergent unreachable
; LLC-W64-NEXT:  .LBB0_5: ; %Flow2
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[40:41]
; LLC-W64-NEXT:    s_andn2_b64 s[4:5], s[36:37], exec
; LLC-W64-NEXT:    s_and_b64 s[6:7], s[12:13], exec
; LLC-W64-NEXT:    s_or_b64 s[36:37], s[4:5], s[6:7]
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[38:39]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[36:37]
; LLC-W64-NEXT:    s_cbranch_execz .LBB0_2
; LLC-W64-NEXT:  .LBB0_6:
; LLC-W64-NEXT:    v_mov_b32_e32 v0, 0
; LLC-W64-NEXT:    global_load_dword v1, v0, s[34:35] glc
; LLC-W64-NEXT:    s_waitcnt vmcnt(0)
; LLC-W64-NEXT:    global_store_dword v0, v1, s[34:35]
; LLC-W64-NEXT:    s_waitcnt vmcnt(0)
; LLC-W64-NEXT:    s_endpgm
;
; LLC-W32-LABEL: global_loadstore_uniform:
; LLC-W32:       ; %bb.0: ; %entry
; LLC-W32-NEXT:    s_load_b64 s[34:35], s[4:5], 0x0
; LLC-W32-NEXT:    s_mov_b64 s[10:11], s[6:7]
; LLC-W32-NEXT:    s_mov_b32 s36, exec_lo
; LLC-W32-NEXT:    s_mov_b32 s32, 0
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    s_lshr_b64 s[6:7], s[34:35], 3
; LLC-W32-NEXT:    v_add_co_u32 v1, s6, 0x7fff8000, s6
; LLC-W32-NEXT:    v_add_co_ci_u32_e64 v2, null, 0, s7, s6
; LLC-W32-NEXT:    flat_load_i8 v1, v[1:2]
; LLC-W32-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W32-NEXT:    v_cmp_eq_u16_e64 s33, 0, v1
; LLC-W32-NEXT:    v_cmpx_ne_u16_e32 0, v1
; LLC-W32-NEXT:    s_cbranch_execnz .LBB0_3
; LLC-W32-NEXT:  ; %bb.1: ; %Flow
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s36
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s33
; LLC-W32-NEXT:    s_cbranch_execnz .LBB0_6
; LLC-W32-NEXT:  .LBB0_2: ; %UnifiedReturnBlock
; LLC-W32-NEXT:    s_endpgm
; LLC-W32-NEXT:  .LBB0_3:
; LLC-W32-NEXT:    v_and_b32_e64 v2, s34, 7
; LLC-W32-NEXT:    s_mov_b32 s6, -1
; LLC-W32-NEXT:    s_mov_b32 s37, exec_lo
; LLC-W32-NEXT:    v_add_nc_u16 v2, v2, 3
; LLC-W32-NEXT:    v_cmpx_ge_i16_e64 v2, v1
; LLC-W32-NEXT:    s_cbranch_execz .LBB0_5
; LLC-W32-NEXT:  ; %bb.4:
; LLC-W32-NEXT:    s_add_u32 s8, s4, 8
; LLC-W32-NEXT:    s_addc_u32 s9, s5, 0
; LLC-W32-NEXT:    s_getpc_b64 s[4:5]
; LLC-W32-NEXT:    s_add_u32 s4, s4, __asan_report_load4@gotpcrel32@lo+4
; LLC-W32-NEXT:    s_addc_u32 s5, s5, __asan_report_load4@gotpcrel32@hi+12
; LLC-W32-NEXT:    v_dual_mov_b32 v31, v0 :: v_dual_mov_b32 v0, s34
; LLC-W32-NEXT:    s_load_b64 s[16:17], s[4:5], 0x0
; LLC-W32-NEXT:    v_mov_b32_e32 v1, s35
; LLC-W32-NEXT:    s_mov_b64 s[4:5], s[0:1]
; LLC-W32-NEXT:    s_mov_b64 s[6:7], s[2:3]
; LLC-W32-NEXT:    s_mov_b32 s12, s13
; LLC-W32-NEXT:    s_mov_b32 s13, s14
; LLC-W32-NEXT:    s_mov_b32 s14, s15
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    s_swappc_b64 s[30:31], s[16:17]
; LLC-W32-NEXT:    ; divergent unreachable
; LLC-W32-NEXT:    s_xor_b32 s6, exec_lo, -1
; LLC-W32-NEXT:  .LBB0_5: ; %Flow2
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s37
; LLC-W32-NEXT:    s_and_not1_b32 s0, s33, exec_lo
; LLC-W32-NEXT:    s_and_b32 s1, s6, exec_lo
; LLC-W32-NEXT:    s_or_b32 s33, s0, s1
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s36
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s33
; LLC-W32-NEXT:    s_cbranch_execz .LBB0_2
; LLC-W32-NEXT:  .LBB0_6:
; LLC-W32-NEXT:    v_mov_b32_e32 v0, 0
; LLC-W32-NEXT:    global_load_b32 v1, v0, s[34:35] glc dlc
; LLC-W32-NEXT:    s_waitcnt vmcnt(0)
; LLC-W32-NEXT:    global_store_b32 v0, v1, s[34:35] dlc
; LLC-W32-NEXT:    s_waitcnt_vscnt null, 0x0
; LLC-W32-NEXT:    s_nop 0
; LLC-W32-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; LLC-W32-NEXT:    s_endpgm
entry:
  %val = load volatile i32, ptr addrspace(1) %ptr, align 4
  store volatile i32 %val, ptr addrspace(1) %ptr, align 4
  ret void
}

define protected amdgpu_kernel void @generic_loadstore_uniform(ptr addrspace(0) %ptr) sanitize_address {
; OPT-LABEL: define protected amdgpu_kernel void @generic_loadstore_uniform(
; OPT-SAME: ptr [[PTR:%.*]]) #[[ATTR1]] {
; OPT-NEXT:  entry:
; OPT-NEXT:    [[TMP0:%.*]] = call i1 @llvm.amdgcn.is.shared(ptr [[PTR]])
; OPT-NEXT:    [[TMP1:%.*]] = call i1 @llvm.amdgcn.is.private(ptr [[PTR]])
; OPT-NEXT:    [[TMP2:%.*]] = or i1 [[TMP0]], [[TMP1]]
; OPT-NEXT:    [[TMP3:%.*]] = xor i1 [[TMP2]], true
; OPT-NEXT:    br i1 [[TMP3]], label [[TMP4:%.*]], label [[TMP18:%.*]]
; OPT:       4:
; OPT-NEXT:    [[TMP5:%.*]] = ptrtoint ptr [[PTR]] to i64
; OPT-NEXT:    [[TMP6:%.*]] = lshr i64 [[TMP5]], 3
; OPT-NEXT:    [[TMP7:%.*]] = add i64 [[TMP6]], 2147450880
; OPT-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
; OPT-NEXT:    [[TMP9:%.*]] = load i8, ptr [[TMP8]], align 1
; OPT-NEXT:    [[TMP10:%.*]] = icmp ne i8 [[TMP9]], 0
; OPT-NEXT:    br i1 [[TMP10]], label [[TMP11:%.*]], label [[TMP17:%.*]], !prof [[PROF0]]
; OPT:       11:
; OPT-NEXT:    [[TMP12:%.*]] = and i64 [[TMP5]], 7
; OPT-NEXT:    [[TMP13:%.*]] = add i64 [[TMP12]], 3
; OPT-NEXT:    [[TMP14:%.*]] = trunc i64 [[TMP13]] to i8
; OPT-NEXT:    [[TMP15:%.*]] = icmp sge i8 [[TMP14]], [[TMP9]]
; OPT-NEXT:    br i1 [[TMP15]], label [[TMP16:%.*]], label [[TMP17]]
; OPT:       16:
; OPT-NEXT:    call void @__asan_report_load4(i64 [[TMP5]]) #[[ATTR3]]
; OPT-NEXT:    unreachable
; OPT:       17:
; OPT-NEXT:    br label [[TMP18]]
; OPT:       18:
; OPT-NEXT:    [[VAL:%.*]] = load volatile i32, ptr [[PTR]], align 4
; OPT-NEXT:    store volatile i32 [[VAL]], ptr [[PTR]], align 4
; OPT-NEXT:    ret void
;
; LLC-W64-LABEL: generic_loadstore_uniform:
; LLC-W64:       ; %bb.0: ; %entry
; LLC-W64-NEXT:    s_load_dwordx2 s[36:37], s[8:9], 0x0
; LLC-W64-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; LLC-W64-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; LLC-W64-NEXT:    s_add_u32 s0, s0, s17
; LLC-W64-NEXT:    s_addc_u32 s1, s1, 0
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    s_lshr_b64 s[12:13], s[36:37], 3
; LLC-W64-NEXT:    v_mov_b32_e32 v1, s12
; LLC-W64-NEXT:    v_add_co_u32_e32 v2, vcc, 0x7fff8000, v1
; LLC-W64-NEXT:    v_mov_b32_e32 v1, s13
; LLC-W64-NEXT:    v_addc_co_u32_e32 v3, vcc, 0, v1, vcc
; LLC-W64-NEXT:    flat_load_sbyte v1, v[2:3]
; LLC-W64-NEXT:    s_mov_b64 s[12:13], -1
; LLC-W64-NEXT:    s_mov_b32 s32, 0
; LLC-W64-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W64-NEXT:    v_cmp_ne_u16_e32 vcc, 0, v1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[38:39], vcc
; LLC-W64-NEXT:    s_cbranch_execnz .LBB1_3
; LLC-W64-NEXT:  ; %bb.1: ; %Flow
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[38:39]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[12:13]
; LLC-W64-NEXT:    s_cbranch_execnz .LBB1_6
; LLC-W64-NEXT:  .LBB1_2: ; %UnifiedReturnBlock
; LLC-W64-NEXT:    s_endpgm
; LLC-W64-NEXT:  .LBB1_3:
; LLC-W64-NEXT:    v_and_b32_e64 v2, s36, 7
; LLC-W64-NEXT:    v_add_u16_e32 v2, 3, v2
; LLC-W64-NEXT:    v_cmp_lt_i16_e64 s[34:35], v2, v1
; LLC-W64-NEXT:    v_cmp_ge_i16_e32 vcc, v2, v1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[40:41], vcc
; LLC-W64-NEXT:    s_cbranch_execz .LBB1_5
; LLC-W64-NEXT:  ; %bb.4:
; LLC-W64-NEXT:    s_add_u32 s8, s8, 8
; LLC-W64-NEXT:    s_addc_u32 s9, s9, 0
; LLC-W64-NEXT:    s_getpc_b64 s[12:13]
; LLC-W64-NEXT:    s_add_u32 s12, s12, __asan_report_load4@gotpcrel32@lo+4
; LLC-W64-NEXT:    s_addc_u32 s13, s13, __asan_report_load4@gotpcrel32@hi+12
; LLC-W64-NEXT:    s_load_dwordx2 s[18:19], s[12:13], 0x0
; LLC-W64-NEXT:    s_mov_b32 s12, s14
; LLC-W64-NEXT:    s_mov_b32 s13, s15
; LLC-W64-NEXT:    s_mov_b32 s14, s16
; LLC-W64-NEXT:    v_mov_b32_e32 v31, v0
; LLC-W64-NEXT:    v_mov_b32_e32 v0, s36
; LLC-W64-NEXT:    v_mov_b32_e32 v1, s37
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    s_swappc_b64 s[30:31], s[18:19]
; LLC-W64-NEXT:    ; divergent unreachable
; LLC-W64-NEXT:  .LBB1_5: ; %Flow2
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[40:41]
; LLC-W64-NEXT:    s_orn2_b64 s[12:13], s[34:35], exec
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[38:39]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[12:13]
; LLC-W64-NEXT:    s_cbranch_execz .LBB1_2
; LLC-W64-NEXT:  .LBB1_6:
; LLC-W64-NEXT:    v_pk_mov_b32 v[0:1], s[36:37], s[36:37] op_sel:[0,1]
; LLC-W64-NEXT:    flat_load_dword v2, v[0:1] glc
; LLC-W64-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W64-NEXT:    flat_store_dword v[0:1], v2
; LLC-W64-NEXT:    s_waitcnt vmcnt(0)
; LLC-W64-NEXT:    s_endpgm
;
; LLC-W32-LABEL: generic_loadstore_uniform:
; LLC-W32:       ; %bb.0: ; %entry
; LLC-W32-NEXT:    s_load_b64 s[34:35], s[4:5], 0x0
; LLC-W32-NEXT:    s_mov_b64 s[10:11], s[6:7]
; LLC-W32-NEXT:    s_mov_b32 s36, exec_lo
; LLC-W32-NEXT:    s_mov_b32 s32, 0
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    s_lshr_b64 s[6:7], s[34:35], 3
; LLC-W32-NEXT:    v_add_co_u32 v1, s6, 0x7fff8000, s6
; LLC-W32-NEXT:    v_add_co_ci_u32_e64 v2, null, 0, s7, s6
; LLC-W32-NEXT:    s_mov_b32 s6, -1
; LLC-W32-NEXT:    flat_load_i8 v1, v[1:2]
; LLC-W32-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W32-NEXT:    v_cmpx_ne_u16_e32 0, v1
; LLC-W32-NEXT:    s_cbranch_execnz .LBB1_3
; LLC-W32-NEXT:  ; %bb.1: ; %Flow
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s36
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s6
; LLC-W32-NEXT:    s_cbranch_execnz .LBB1_6
; LLC-W32-NEXT:  .LBB1_2: ; %UnifiedReturnBlock
; LLC-W32-NEXT:    s_endpgm
; LLC-W32-NEXT:  .LBB1_3:
; LLC-W32-NEXT:    v_and_b32_e64 v2, s34, 7
; LLC-W32-NEXT:    s_mov_b32 s37, exec_lo
; LLC-W32-NEXT:    v_add_nc_u16 v2, v2, 3
; LLC-W32-NEXT:    v_cmp_lt_i16_e64 s33, v2, v1
; LLC-W32-NEXT:    v_cmpx_ge_i16_e64 v2, v1
; LLC-W32-NEXT:    s_cbranch_execz .LBB1_5
; LLC-W32-NEXT:  ; %bb.4:
; LLC-W32-NEXT:    s_add_u32 s8, s4, 8
; LLC-W32-NEXT:    s_addc_u32 s9, s5, 0
; LLC-W32-NEXT:    s_getpc_b64 s[4:5]
; LLC-W32-NEXT:    s_add_u32 s4, s4, __asan_report_load4@gotpcrel32@lo+4
; LLC-W32-NEXT:    s_addc_u32 s5, s5, __asan_report_load4@gotpcrel32@hi+12
; LLC-W32-NEXT:    v_dual_mov_b32 v31, v0 :: v_dual_mov_b32 v0, s34
; LLC-W32-NEXT:    s_load_b64 s[16:17], s[4:5], 0x0
; LLC-W32-NEXT:    v_mov_b32_e32 v1, s35
; LLC-W32-NEXT:    s_mov_b64 s[4:5], s[0:1]
; LLC-W32-NEXT:    s_mov_b64 s[6:7], s[2:3]
; LLC-W32-NEXT:    s_mov_b32 s12, s13
; LLC-W32-NEXT:    s_mov_b32 s13, s14
; LLC-W32-NEXT:    s_mov_b32 s14, s15
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    s_swappc_b64 s[30:31], s[16:17]
; LLC-W32-NEXT:    ; divergent unreachable
; LLC-W32-NEXT:  .LBB1_5: ; %Flow2
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s37
; LLC-W32-NEXT:    s_or_not1_b32 s6, s33, exec_lo
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s36
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s6
; LLC-W32-NEXT:    s_cbranch_execz .LBB1_2
; LLC-W32-NEXT:  .LBB1_6:
; LLC-W32-NEXT:    v_dual_mov_b32 v0, s34 :: v_dual_mov_b32 v1, s35
; LLC-W32-NEXT:    flat_load_b32 v2, v[0:1] glc dlc
; LLC-W32-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W32-NEXT:    flat_store_b32 v[0:1], v2 dlc
; LLC-W32-NEXT:    s_waitcnt_vscnt null, 0x0
; LLC-W32-NEXT:    s_endpgm
entry:
  %val = load volatile i32, ptr addrspace(0) %ptr, align 4
  store volatile i32 %val, ptr addrspace(0) %ptr, align 4
  ret void
}

define protected amdgpu_kernel void @global_store_nonuniform(ptr addrspace(1) %ptr) sanitize_address {
; OPT-LABEL: define protected amdgpu_kernel void @global_store_nonuniform(
; OPT-SAME: ptr addrspace(1) [[PTR:%.*]]) #[[ATTR1]] {
; OPT-NEXT:  entry:
; OPT-NEXT:    [[TID:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; OPT-NEXT:    [[TID64:%.*]] = zext i32 [[TID]] to i64
; OPT-NEXT:    [[PP1:%.*]] = getelementptr inbounds i64, ptr addrspace(1) [[PTR]], i64 [[TID64]]
; OPT-NEXT:    [[TMP0:%.*]] = ptrtoint ptr addrspace(1) [[PP1]] to i64
; OPT-NEXT:    [[TMP1:%.*]] = lshr i64 [[TMP0]], 3
; OPT-NEXT:    [[TMP2:%.*]] = add i64 [[TMP1]], 2147450880
; OPT-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[TMP2]] to ptr
; OPT-NEXT:    [[TMP4:%.*]] = load i8, ptr [[TMP3]], align 1
; OPT-NEXT:    [[TMP5:%.*]] = icmp ne i8 [[TMP4]], 0
; OPT-NEXT:    br i1 [[TMP5]], label [[TMP6:%.*]], label [[TMP12:%.*]], !prof [[PROF0]]
; OPT:       6:
; OPT-NEXT:    [[TMP7:%.*]] = and i64 [[TMP0]], 7
; OPT-NEXT:    [[TMP8:%.*]] = add i64 [[TMP7]], 3
; OPT-NEXT:    [[TMP9:%.*]] = trunc i64 [[TMP8]] to i8
; OPT-NEXT:    [[TMP10:%.*]] = icmp sge i8 [[TMP9]], [[TMP4]]
; OPT-NEXT:    br i1 [[TMP10]], label [[TMP11:%.*]], label [[TMP12]]
; OPT:       11:
; OPT-NEXT:    call void @__asan_report_store4(i64 [[TMP0]]) #[[ATTR3]]
; OPT-NEXT:    unreachable
; OPT:       12:
; OPT-NEXT:    store i32 42, ptr addrspace(1) [[PP1]], align 4
; OPT-NEXT:    ret void
;
; LLC-W64-LABEL: global_store_nonuniform:
; LLC-W64:       ; %bb.0: ; %entry
; LLC-W64-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; LLC-W64-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; LLC-W64-NEXT:    s_load_dwordx2 s[12:13], s[8:9], 0x0
; LLC-W64-NEXT:    v_and_b32_e32 v1, 0x3ff, v0
; LLC-W64-NEXT:    v_lshlrev_b32_e32 v1, 3, v1
; LLC-W64-NEXT:    s_add_u32 s0, s0, s17
; LLC-W64-NEXT:    s_addc_u32 s1, s1, 0
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    v_mov_b32_e32 v2, s13
; LLC-W64-NEXT:    v_add_co_u32_e32 v40, vcc, s12, v1
; LLC-W64-NEXT:    v_addc_co_u32_e32 v41, vcc, 0, v2, vcc
; LLC-W64-NEXT:    v_lshrrev_b64 v[2:3], 3, v[40:41]
; LLC-W64-NEXT:    v_add_co_u32_e32 v2, vcc, 0x7fff8000, v2
; LLC-W64-NEXT:    v_addc_co_u32_e32 v3, vcc, 0, v3, vcc
; LLC-W64-NEXT:    flat_load_sbyte v1, v[2:3]
; LLC-W64-NEXT:    s_mov_b32 s32, 0
; LLC-W64-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W64-NEXT:    v_cmp_eq_u16_e64 s[34:35], 0, v1
; LLC-W64-NEXT:    v_cmp_ne_u16_e32 vcc, 0, v1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[36:37], vcc
; LLC-W64-NEXT:    s_cbranch_execnz .LBB2_3
; LLC-W64-NEXT:  ; %bb.1: ; %Flow
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[36:37]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[34:35]
; LLC-W64-NEXT:    s_cbranch_execnz .LBB2_6
; LLC-W64-NEXT:  .LBB2_2: ; %UnifiedReturnBlock
; LLC-W64-NEXT:    s_endpgm
; LLC-W64-NEXT:  .LBB2_3:
; LLC-W64-NEXT:    v_and_b32_e32 v2, 7, v40
; LLC-W64-NEXT:    v_add_u16_e32 v2, 3, v2
; LLC-W64-NEXT:    v_cmp_ge_i16_e32 vcc, v2, v1
; LLC-W64-NEXT:    s_mov_b64 s[12:13], -1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[38:39], vcc
; LLC-W64-NEXT:    s_cbranch_execz .LBB2_5
; LLC-W64-NEXT:  ; %bb.4:
; LLC-W64-NEXT:    s_add_u32 s8, s8, 8
; LLC-W64-NEXT:    s_addc_u32 s9, s9, 0
; LLC-W64-NEXT:    s_getpc_b64 s[12:13]
; LLC-W64-NEXT:    s_add_u32 s12, s12, __asan_report_store4@gotpcrel32@lo+4
; LLC-W64-NEXT:    s_addc_u32 s13, s13, __asan_report_store4@gotpcrel32@hi+12
; LLC-W64-NEXT:    s_load_dwordx2 s[18:19], s[12:13], 0x0
; LLC-W64-NEXT:    s_mov_b32 s12, s14
; LLC-W64-NEXT:    s_mov_b32 s13, s15
; LLC-W64-NEXT:    s_mov_b32 s14, s16
; LLC-W64-NEXT:    v_mov_b32_e32 v31, v0
; LLC-W64-NEXT:    v_mov_b32_e32 v0, v40
; LLC-W64-NEXT:    v_mov_b32_e32 v1, v41
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    s_swappc_b64 s[30:31], s[18:19]
; LLC-W64-NEXT:    s_xor_b64 s[12:13], exec, -1
; LLC-W64-NEXT:    ; divergent unreachable
; LLC-W64-NEXT:  .LBB2_5: ; %Flow2
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[38:39]
; LLC-W64-NEXT:    s_andn2_b64 s[4:5], s[34:35], exec
; LLC-W64-NEXT:    s_and_b64 s[6:7], s[12:13], exec
; LLC-W64-NEXT:    s_or_b64 s[34:35], s[4:5], s[6:7]
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[36:37]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[34:35]
; LLC-W64-NEXT:    s_cbranch_execz .LBB2_2
; LLC-W64-NEXT:  .LBB2_6:
; LLC-W64-NEXT:    v_mov_b32_e32 v0, 42
; LLC-W64-NEXT:    global_store_dword v[40:41], v0, off
; LLC-W64-NEXT:    s_endpgm
;
; LLC-W32-LABEL: global_store_nonuniform:
; LLC-W32:       ; %bb.0: ; %entry
; LLC-W32-NEXT:    s_mov_b64 s[10:11], s[6:7]
; LLC-W32-NEXT:    s_load_b64 s[6:7], s[4:5], 0x0
; LLC-W32-NEXT:    v_and_b32_e32 v1, 0x3ff, v0
; LLC-W32-NEXT:    s_mov_b32 s34, exec_lo
; LLC-W32-NEXT:    s_mov_b32 s32, 0
; LLC-W32-NEXT:    v_lshlrev_b32_e32 v1, 3, v1
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    v_add_co_u32 v40, s6, s6, v1
; LLC-W32-NEXT:    v_add_co_ci_u32_e64 v41, null, s7, 0, s6
; LLC-W32-NEXT:    v_lshrrev_b64 v[1:2], 3, v[40:41]
; LLC-W32-NEXT:    v_add_co_u32 v1, vcc_lo, 0x7fff8000, v1
; LLC-W32-NEXT:    v_add_co_ci_u32_e32 v2, vcc_lo, 0, v2, vcc_lo
; LLC-W32-NEXT:    flat_load_i8 v1, v[1:2]
; LLC-W32-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W32-NEXT:    v_cmp_eq_u16_e64 s33, 0, v1
; LLC-W32-NEXT:    v_cmpx_ne_u16_e32 0, v1
; LLC-W32-NEXT:    s_cbranch_execnz .LBB2_3
; LLC-W32-NEXT:  ; %bb.1: ; %Flow
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s34
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s33
; LLC-W32-NEXT:    s_cbranch_execnz .LBB2_6
; LLC-W32-NEXT:  .LBB2_2: ; %UnifiedReturnBlock
; LLC-W32-NEXT:    s_endpgm
; LLC-W32-NEXT:  .LBB2_3:
; LLC-W32-NEXT:    v_and_b32_e32 v2, 7, v40
; LLC-W32-NEXT:    s_mov_b32 s6, -1
; LLC-W32-NEXT:    s_mov_b32 s35, exec_lo
; LLC-W32-NEXT:    v_add_nc_u16 v2, v2, 3
; LLC-W32-NEXT:    v_cmpx_ge_i16_e64 v2, v1
; LLC-W32-NEXT:    s_cbranch_execz .LBB2_5
; LLC-W32-NEXT:  ; %bb.4:
; LLC-W32-NEXT:    s_add_u32 s8, s4, 8
; LLC-W32-NEXT:    s_addc_u32 s9, s5, 0
; LLC-W32-NEXT:    s_getpc_b64 s[4:5]
; LLC-W32-NEXT:    s_add_u32 s4, s4, __asan_report_store4@gotpcrel32@lo+4
; LLC-W32-NEXT:    s_addc_u32 s5, s5, __asan_report_store4@gotpcrel32@hi+12
; LLC-W32-NEXT:    v_mov_b32_e32 v31, v0
; LLC-W32-NEXT:    s_load_b64 s[16:17], s[4:5], 0x0
; LLC-W32-NEXT:    v_dual_mov_b32 v0, v40 :: v_dual_mov_b32 v1, v41
; LLC-W32-NEXT:    s_mov_b64 s[4:5], s[0:1]
; LLC-W32-NEXT:    s_mov_b64 s[6:7], s[2:3]
; LLC-W32-NEXT:    s_mov_b32 s12, s13
; LLC-W32-NEXT:    s_mov_b32 s13, s14
; LLC-W32-NEXT:    s_mov_b32 s14, s15
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    s_swappc_b64 s[30:31], s[16:17]
; LLC-W32-NEXT:    ; divergent unreachable
; LLC-W32-NEXT:    s_xor_b32 s6, exec_lo, -1
; LLC-W32-NEXT:  .LBB2_5: ; %Flow2
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s35
; LLC-W32-NEXT:    s_and_not1_b32 s0, s33, exec_lo
; LLC-W32-NEXT:    s_and_b32 s1, s6, exec_lo
; LLC-W32-NEXT:    s_or_b32 s33, s0, s1
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s34
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s33
; LLC-W32-NEXT:    s_cbranch_execz .LBB2_2
; LLC-W32-NEXT:  .LBB2_6:
; LLC-W32-NEXT:    v_mov_b32_e32 v0, 42
; LLC-W32-NEXT:    global_store_b32 v[40:41], v0, off
; LLC-W32-NEXT:    s_nop 0
; LLC-W32-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; LLC-W32-NEXT:    s_endpgm
entry:
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %tid64 = zext i32 %tid to i64

  %pp1 = getelementptr inbounds i64, ptr addrspace(1) %ptr, i64 %tid64
  store i32 42, ptr addrspace(1) %pp1, align 4
  ret void
}

define protected amdgpu_kernel void @generic_store_nonuniform(ptr addrspace(0) %ptr) sanitize_address {
; OPT-LABEL: define protected amdgpu_kernel void @generic_store_nonuniform(
; OPT-SAME: ptr [[PTR:%.*]]) #[[ATTR1]] {
; OPT-NEXT:  entry:
; OPT-NEXT:    [[TID:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; OPT-NEXT:    [[TID64:%.*]] = zext i32 [[TID]] to i64
; OPT-NEXT:    [[PP1:%.*]] = getelementptr inbounds i64, ptr [[PTR]], i64 [[TID64]]
; OPT-NEXT:    [[TMP0:%.*]] = call i1 @llvm.amdgcn.is.shared(ptr [[PP1]])
; OPT-NEXT:    [[TMP1:%.*]] = call i1 @llvm.amdgcn.is.private(ptr [[PP1]])
; OPT-NEXT:    [[TMP2:%.*]] = or i1 [[TMP0]], [[TMP1]]
; OPT-NEXT:    [[TMP3:%.*]] = xor i1 [[TMP2]], true
; OPT-NEXT:    br i1 [[TMP3]], label [[TMP4:%.*]], label [[TMP18:%.*]]
; OPT:       4:
; OPT-NEXT:    [[TMP5:%.*]] = ptrtoint ptr [[PP1]] to i64
; OPT-NEXT:    [[TMP6:%.*]] = lshr i64 [[TMP5]], 3
; OPT-NEXT:    [[TMP7:%.*]] = add i64 [[TMP6]], 2147450880
; OPT-NEXT:    [[TMP8:%.*]] = inttoptr i64 [[TMP7]] to ptr
; OPT-NEXT:    [[TMP9:%.*]] = load i8, ptr [[TMP8]], align 1
; OPT-NEXT:    [[TMP10:%.*]] = icmp ne i8 [[TMP9]], 0
; OPT-NEXT:    br i1 [[TMP10]], label [[TMP11:%.*]], label [[TMP17:%.*]], !prof [[PROF0]]
; OPT:       11:
; OPT-NEXT:    [[TMP12:%.*]] = and i64 [[TMP5]], 7
; OPT-NEXT:    [[TMP13:%.*]] = add i64 [[TMP12]], 3
; OPT-NEXT:    [[TMP14:%.*]] = trunc i64 [[TMP13]] to i8
; OPT-NEXT:    [[TMP15:%.*]] = icmp sge i8 [[TMP14]], [[TMP9]]
; OPT-NEXT:    br i1 [[TMP15]], label [[TMP16:%.*]], label [[TMP17]]
; OPT:       16:
; OPT-NEXT:    call void @__asan_report_store4(i64 [[TMP5]]) #[[ATTR3]]
; OPT-NEXT:    unreachable
; OPT:       17:
; OPT-NEXT:    br label [[TMP18]]
; OPT:       18:
; OPT-NEXT:    store i32 42, ptr [[PP1]], align 4
; OPT-NEXT:    ret void
;
; LLC-W64-LABEL: generic_store_nonuniform:
; LLC-W64:       ; %bb.0: ; %entry
; LLC-W64-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; LLC-W64-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; LLC-W64-NEXT:    s_load_dwordx2 s[12:13], s[8:9], 0x0
; LLC-W64-NEXT:    v_and_b32_e32 v1, 0x3ff, v0
; LLC-W64-NEXT:    v_lshlrev_b32_e32 v1, 3, v1
; LLC-W64-NEXT:    s_add_u32 s0, s0, s17
; LLC-W64-NEXT:    s_addc_u32 s1, s1, 0
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    v_mov_b32_e32 v2, s13
; LLC-W64-NEXT:    v_add_co_u32_e32 v40, vcc, s12, v1
; LLC-W64-NEXT:    v_addc_co_u32_e32 v41, vcc, 0, v2, vcc
; LLC-W64-NEXT:    v_lshrrev_b64 v[2:3], 3, v[40:41]
; LLC-W64-NEXT:    v_add_co_u32_e32 v2, vcc, 0x7fff8000, v2
; LLC-W64-NEXT:    v_addc_co_u32_e32 v3, vcc, 0, v3, vcc
; LLC-W64-NEXT:    flat_load_sbyte v1, v[2:3]
; LLC-W64-NEXT:    s_mov_b64 s[12:13], -1
; LLC-W64-NEXT:    s_mov_b32 s32, 0
; LLC-W64-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W64-NEXT:    v_cmp_ne_u16_e32 vcc, 0, v1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[36:37], vcc
; LLC-W64-NEXT:    s_cbranch_execnz .LBB3_3
; LLC-W64-NEXT:  ; %bb.1: ; %Flow
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[36:37]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[12:13]
; LLC-W64-NEXT:    s_cbranch_execnz .LBB3_6
; LLC-W64-NEXT:  .LBB3_2: ; %UnifiedReturnBlock
; LLC-W64-NEXT:    s_endpgm
; LLC-W64-NEXT:  .LBB3_3:
; LLC-W64-NEXT:    v_and_b32_e32 v2, 7, v40
; LLC-W64-NEXT:    v_add_u16_e32 v2, 3, v2
; LLC-W64-NEXT:    v_cmp_lt_i16_e64 s[34:35], v2, v1
; LLC-W64-NEXT:    v_cmp_ge_i16_e32 vcc, v2, v1
; LLC-W64-NEXT:    s_and_saveexec_b64 s[38:39], vcc
; LLC-W64-NEXT:    s_cbranch_execz .LBB3_5
; LLC-W64-NEXT:  ; %bb.4:
; LLC-W64-NEXT:    s_add_u32 s8, s8, 8
; LLC-W64-NEXT:    s_addc_u32 s9, s9, 0
; LLC-W64-NEXT:    s_getpc_b64 s[12:13]
; LLC-W64-NEXT:    s_add_u32 s12, s12, __asan_report_store4@gotpcrel32@lo+4
; LLC-W64-NEXT:    s_addc_u32 s13, s13, __asan_report_store4@gotpcrel32@hi+12
; LLC-W64-NEXT:    s_load_dwordx2 s[18:19], s[12:13], 0x0
; LLC-W64-NEXT:    s_mov_b32 s12, s14
; LLC-W64-NEXT:    s_mov_b32 s13, s15
; LLC-W64-NEXT:    s_mov_b32 s14, s16
; LLC-W64-NEXT:    v_mov_b32_e32 v31, v0
; LLC-W64-NEXT:    v_mov_b32_e32 v0, v40
; LLC-W64-NEXT:    v_mov_b32_e32 v1, v41
; LLC-W64-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W64-NEXT:    s_swappc_b64 s[30:31], s[18:19]
; LLC-W64-NEXT:    ; divergent unreachable
; LLC-W64-NEXT:  .LBB3_5: ; %Flow2
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[38:39]
; LLC-W64-NEXT:    s_orn2_b64 s[12:13], s[34:35], exec
; LLC-W64-NEXT:    s_or_b64 exec, exec, s[36:37]
; LLC-W64-NEXT:    s_and_saveexec_b64 s[4:5], s[12:13]
; LLC-W64-NEXT:    s_cbranch_execz .LBB3_2
; LLC-W64-NEXT:  .LBB3_6:
; LLC-W64-NEXT:    v_mov_b32_e32 v0, 42
; LLC-W64-NEXT:    global_store_dword v[40:41], v0, off
; LLC-W64-NEXT:    s_endpgm
;
; LLC-W32-LABEL: generic_store_nonuniform:
; LLC-W32:       ; %bb.0: ; %entry
; LLC-W32-NEXT:    s_mov_b64 s[10:11], s[6:7]
; LLC-W32-NEXT:    s_load_b64 s[6:7], s[4:5], 0x0
; LLC-W32-NEXT:    v_and_b32_e32 v1, 0x3ff, v0
; LLC-W32-NEXT:    s_mov_b32 s34, exec_lo
; LLC-W32-NEXT:    s_mov_b32 s32, 0
; LLC-W32-NEXT:    v_lshlrev_b32_e32 v1, 3, v1
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    v_add_co_u32 v40, s6, s6, v1
; LLC-W32-NEXT:    v_add_co_ci_u32_e64 v41, null, s7, 0, s6
; LLC-W32-NEXT:    s_mov_b32 s6, -1
; LLC-W32-NEXT:    v_lshrrev_b64 v[1:2], 3, v[40:41]
; LLC-W32-NEXT:    v_add_co_u32 v1, vcc_lo, 0x7fff8000, v1
; LLC-W32-NEXT:    v_add_co_ci_u32_e32 v2, vcc_lo, 0, v2, vcc_lo
; LLC-W32-NEXT:    flat_load_i8 v1, v[1:2]
; LLC-W32-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; LLC-W32-NEXT:    v_cmpx_ne_u16_e32 0, v1
; LLC-W32-NEXT:    s_cbranch_execnz .LBB3_3
; LLC-W32-NEXT:  ; %bb.1: ; %Flow
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s34
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s6
; LLC-W32-NEXT:    s_cbranch_execnz .LBB3_6
; LLC-W32-NEXT:  .LBB3_2: ; %UnifiedReturnBlock
; LLC-W32-NEXT:    s_endpgm
; LLC-W32-NEXT:  .LBB3_3:
; LLC-W32-NEXT:    v_and_b32_e32 v2, 7, v40
; LLC-W32-NEXT:    s_mov_b32 s35, exec_lo
; LLC-W32-NEXT:    v_add_nc_u16 v2, v2, 3
; LLC-W32-NEXT:    v_cmp_lt_i16_e64 s33, v2, v1
; LLC-W32-NEXT:    v_cmpx_ge_i16_e64 v2, v1
; LLC-W32-NEXT:    s_cbranch_execz .LBB3_5
; LLC-W32-NEXT:  ; %bb.4:
; LLC-W32-NEXT:    s_add_u32 s8, s4, 8
; LLC-W32-NEXT:    s_addc_u32 s9, s5, 0
; LLC-W32-NEXT:    s_getpc_b64 s[4:5]
; LLC-W32-NEXT:    s_add_u32 s4, s4, __asan_report_store4@gotpcrel32@lo+4
; LLC-W32-NEXT:    s_addc_u32 s5, s5, __asan_report_store4@gotpcrel32@hi+12
; LLC-W32-NEXT:    v_mov_b32_e32 v31, v0
; LLC-W32-NEXT:    s_load_b64 s[16:17], s[4:5], 0x0
; LLC-W32-NEXT:    v_dual_mov_b32 v0, v40 :: v_dual_mov_b32 v1, v41
; LLC-W32-NEXT:    s_mov_b64 s[4:5], s[0:1]
; LLC-W32-NEXT:    s_mov_b64 s[6:7], s[2:3]
; LLC-W32-NEXT:    s_mov_b32 s12, s13
; LLC-W32-NEXT:    s_mov_b32 s13, s14
; LLC-W32-NEXT:    s_mov_b32 s14, s15
; LLC-W32-NEXT:    s_waitcnt lgkmcnt(0)
; LLC-W32-NEXT:    s_swappc_b64 s[30:31], s[16:17]
; LLC-W32-NEXT:    ; divergent unreachable
; LLC-W32-NEXT:  .LBB3_5: ; %Flow2
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s35
; LLC-W32-NEXT:    s_or_not1_b32 s6, s33, exec_lo
; LLC-W32-NEXT:    s_or_b32 exec_lo, exec_lo, s34
; LLC-W32-NEXT:    s_and_saveexec_b32 s0, s6
; LLC-W32-NEXT:    s_cbranch_execz .LBB3_2
; LLC-W32-NEXT:  .LBB3_6:
; LLC-W32-NEXT:    v_mov_b32_e32 v0, 42
; LLC-W32-NEXT:    global_store_b32 v[40:41], v0, off
; LLC-W32-NEXT:    s_nop 0
; LLC-W32-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; LLC-W32-NEXT:    s_endpgm
entry:
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %tid64 = zext i32 %tid to i64

  %pp1 = getelementptr inbounds i64, ptr addrspace(0) %ptr, i64 %tid64
  store i32 42, ptr addrspace(0) %pp1, align 4
  ret void
}

attributes #0 = { nounwind readnone }
