; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt < %s -msan-check-access-address=0 -S -passes="msan" 2>&1 | FileCheck %s --check-prefixes=ORIGIN
; RUN: opt < %s -msan-check-access-address=0 -S -passes="msan" -msan-kernel=1 2>&1 | FileCheck %s --check-prefixes=ORIGIN2
; Test that code using va_start can be compiled on i386.

target datalayout = "e-p:32:32:32-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:32:64-f32:32:32-f64:32:64-v64:64:64-v128:128:128-a0:0:64-f80:32:32-n8:16:32-S128"
target triple = "i386-unknown-linux-gnu"

define void @VaStart(ptr %s, ...) {
; ORIGIN-LABEL: define void @VaStart(
; ORIGIN-SAME: ptr [[S:%.*]], ...) {
; ORIGIN-NEXT:  [[ENTRY:.*:]]
; ORIGIN-NEXT:    [[TMP5:%.*]] = load i64, ptr @__msan_va_arg_overflow_size_tls, align 4
; ORIGIN-NEXT:    [[TMP6:%.*]] = trunc i64 [[TMP5]] to i32
; ORIGIN-NEXT:    [[TMP7:%.*]] = add i32 0, [[TMP6]]
; ORIGIN-NEXT:    [[TMP3:%.*]] = alloca i8, i32 [[TMP7]], align 8
; ORIGIN-NEXT:    call void @llvm.memset.p0.i32(ptr align 8 [[TMP3]], i8 0, i32 [[TMP7]], i1 false)
; ORIGIN-NEXT:    [[TMP4:%.*]] = call i32 @llvm.umin.i32(i32 [[TMP7]], i32 800)
; ORIGIN-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 8 [[TMP3]], ptr align 8 @__msan_va_arg_tls, i32 [[TMP4]], i1 false)
; ORIGIN-NEXT:    call void @llvm.donothing()
; ORIGIN-NEXT:    [[VL:%.*]] = alloca ptr, align 4
; ORIGIN-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[VL]] to i32
; ORIGIN-NEXT:    [[TMP1:%.*]] = and i32 [[TMP0]], 2147483647
; ORIGIN-NEXT:    [[TMP2:%.*]] = inttoptr i32 [[TMP1]] to ptr
; ORIGIN-NEXT:    call void @llvm.memset.p0.i32(ptr align 4 [[TMP2]], i8 0, i32 4, i1 false)
; ORIGIN-NEXT:    [[TMP8:%.*]] = ptrtoint ptr [[VL]] to i32
; ORIGIN-NEXT:    [[TMP9:%.*]] = and i32 [[TMP8]], 2147483647
; ORIGIN-NEXT:    [[TMP10:%.*]] = inttoptr i32 [[TMP9]] to ptr
; ORIGIN-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP10]], i8 0, i64 4, i1 false)
; ORIGIN-NEXT:    call void @llvm.va_start.p0(ptr [[VL]])
; ORIGIN-NEXT:    [[TMP11:%.*]] = ptrtoint ptr [[VL]] to i32
; ORIGIN-NEXT:    [[TMP12:%.*]] = inttoptr i32 [[TMP11]] to ptr
; ORIGIN-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[TMP12]], align 4
; ORIGIN-NEXT:    [[TMP14:%.*]] = ptrtoint ptr [[TMP13]] to i32
; ORIGIN-NEXT:    [[TMP15:%.*]] = and i32 [[TMP14]], 2147483647
; ORIGIN-NEXT:    [[TMP16:%.*]] = inttoptr i32 [[TMP15]] to ptr
; ORIGIN-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP16]], ptr align 4 [[TMP3]], i32 [[TMP7]], i1 false)
; ORIGIN-NEXT:    ret void
;
; ORIGIN2-LABEL: define void @VaStart(
; ORIGIN2-SAME: ptr [[S:%.*]], ...) {
; ORIGIN2-NEXT:  [[ENTRY:.*:]]
; ORIGIN2-NEXT:    [[TMP0:%.*]] = call ptr @__msan_get_context_state()
; ORIGIN2-NEXT:    [[PARAM_SHADOW:%.*]] = getelementptr { [100 x i64], [100 x i64], [100 x i64], [100 x i64], i64, [200 x i32], i32, i32 }, ptr [[TMP0]], i32 0, i32 0
; ORIGIN2-NEXT:    [[RETVAL_SHADOW:%.*]] = getelementptr { [100 x i64], [100 x i64], [100 x i64], [100 x i64], i64, [200 x i32], i32, i32 }, ptr [[TMP0]], i32 0, i32 1
; ORIGIN2-NEXT:    [[VA_ARG_SHADOW:%.*]] = getelementptr { [100 x i64], [100 x i64], [100 x i64], [100 x i64], i64, [200 x i32], i32, i32 }, ptr [[TMP0]], i32 0, i32 2
; ORIGIN2-NEXT:    [[VA_ARG_ORIGIN:%.*]] = getelementptr { [100 x i64], [100 x i64], [100 x i64], [100 x i64], i64, [200 x i32], i32, i32 }, ptr [[TMP0]], i32 0, i32 3
; ORIGIN2-NEXT:    [[VA_ARG_OVERFLOW_SIZE:%.*]] = getelementptr { [100 x i64], [100 x i64], [100 x i64], [100 x i64], i64, [200 x i32], i32, i32 }, ptr [[TMP0]], i32 0, i32 4
; ORIGIN2-NEXT:    [[PARAM_ORIGIN:%.*]] = getelementptr { [100 x i64], [100 x i64], [100 x i64], [100 x i64], i64, [200 x i32], i32, i32 }, ptr [[TMP0]], i32 0, i32 5
; ORIGIN2-NEXT:    [[RETVAL_ORIGIN:%.*]] = getelementptr { [100 x i64], [100 x i64], [100 x i64], [100 x i64], i64, [200 x i32], i32, i32 }, ptr [[TMP0]], i32 0, i32 6
; ORIGIN2-NEXT:    [[TMP1:%.*]] = load i64, ptr [[VA_ARG_OVERFLOW_SIZE]], align 4
; ORIGIN2-NEXT:    [[TMP2:%.*]] = trunc i64 [[TMP1]] to i32
; ORIGIN2-NEXT:    [[TMP3:%.*]] = add i32 0, [[TMP2]]
; ORIGIN2-NEXT:    [[TMP4:%.*]] = alloca i8, i32 [[TMP3]], align 8
; ORIGIN2-NEXT:    call void @llvm.memset.p0.i32(ptr align 8 [[TMP4]], i8 0, i32 [[TMP3]], i1 false)
; ORIGIN2-NEXT:    [[TMP5:%.*]] = call i32 @llvm.umin.i32(i32 [[TMP3]], i32 800)
; ORIGIN2-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 8 [[TMP4]], ptr align 8 [[VA_ARG_SHADOW]], i32 [[TMP5]], i1 false)
; ORIGIN2-NEXT:    call void @llvm.donothing()
; ORIGIN2-NEXT:    [[VL:%.*]] = alloca ptr, align 4
; ORIGIN2-NEXT:    call void @__msan_unpoison_alloca(ptr [[VL]], i32 4)
; ORIGIN2-NEXT:    [[TMP6:%.*]] = call { ptr, ptr } @__msan_metadata_ptr_for_store_1(ptr [[VL]])
; ORIGIN2-NEXT:    [[TMP7:%.*]] = extractvalue { ptr, ptr } [[TMP6]], 0
; ORIGIN2-NEXT:    [[TMP8:%.*]] = extractvalue { ptr, ptr } [[TMP6]], 1
; ORIGIN2-NEXT:    call void @llvm.memset.p0.i64(ptr align 8 [[TMP7]], i8 0, i64 4, i1 false)
; ORIGIN2-NEXT:    call void @llvm.va_start.p0(ptr [[VL]])
; ORIGIN2-NEXT:    [[TMP9:%.*]] = ptrtoint ptr [[VL]] to i32
; ORIGIN2-NEXT:    [[TMP10:%.*]] = inttoptr i32 [[TMP9]] to ptr
; ORIGIN2-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[TMP10]], align 4
; ORIGIN2-NEXT:    [[TMP12:%.*]] = call { ptr, ptr } @__msan_metadata_ptr_for_store_1(ptr [[TMP11]])
; ORIGIN2-NEXT:    [[TMP13:%.*]] = extractvalue { ptr, ptr } [[TMP12]], 0
; ORIGIN2-NEXT:    [[TMP14:%.*]] = extractvalue { ptr, ptr } [[TMP12]], 1
; ORIGIN2-NEXT:    call void @llvm.memcpy.p0.p0.i32(ptr align 4 [[TMP13]], ptr align 4 [[TMP4]], i32 [[TMP3]], i1 false)
; ORIGIN2-NEXT:    ret void
;
entry:
  %vl = alloca ptr, align 4
  call void @llvm.va_start(ptr %vl)
  ret void
}

declare void @llvm.va_start(ptr)
