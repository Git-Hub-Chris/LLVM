//===-- MeshOps.td - Mesh dialect operation definitions ----*- tablegen -*-===//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//

#ifndef MLIR_DIALECT_MESH_IR_MESHOPS_TD
#define MLIR_DIALECT_MESH_IR_MESHOPS_TD

include "mlir/Dialect/Mesh/IR/MeshBase.td"
include "mlir/Interfaces/InferTypeOpInterface.td"
include "mlir/Interfaces/SideEffectInterfaces.td"
include "mlir/IR/BuiltinTypes.td"
include "mlir/IR/CommonAttrConstraints.td"
include "mlir/IR/CommonTypeConstraints.td"
include "mlir/IR/SymbolInterfaces.td"

//===----------------------------------------------------------------------===//
// Mesh Dialect operations.
//===----------------------------------------------------------------------===//

class Mesh_Op<string mnemonic, list<Trait> traits = []> :
    Op<Mesh_Dialect, mnemonic, traits> {
}

def Mesh_ClusterOp : Mesh_Op<"cluster", [Symbol]> {
  let summary = "representing a mesh cluster";
  let description = [{
    The mesh.cluster operation is a symbol operation that identifies a specific
    mesh cluster. The operation has three attributes:

    1. `sym_name`: This attribute uniquely identifies the name of the mesh
    cluster. This name serves as a symbolic reference to the cluster throughout
    the MLIR module, allowing for consistent referencing and easier debugging.

    2. `rank`: This attribute specifies the number of axes of the cluster. The
    rank indicates the dimensionality of the mesh cluster and can be used to
    determine the layout and the addressing space of the computation distributed
    across the mesh.

    3. `dim_sizes`: This attribute represents the device assignment along the
    axes of the cluster. Each integer in the array corresponds to the number of
    devices along a specific axis. If an integer value is 0, it implies that the
    number of devices along that axis is unknown. This flexibility allows for
    dynamic device assignment or configurations where the exact number of
    devices might not be determined during compile time.

    Example:
    ```
    // A device mesh cluster with 3 axes, the total device number is 4 * 8 * 12
    // The dimension sizes are 4, 8, 12 
    mesh.cluster @mesh0(rank = 3, dim_sizes = [4, 8, 12])

    // A device mesh cluster with 2 axes, the total device number is unknown
    // The first dimension size is 4 and the second is unknown
    mesh.cluster @mesh1(rank = 2, dim_sizes = [4])

    // A device mesh cluster with 2 axes, the total device number is unknown
    // The first dimension size is unknown and the second is 4
    mesh.cluster @mesh2(rank = 2, dim_sizes = [0, 4])

    // A device mesh cluster with 2 axes, the number of devices along both axes
    // is unknown
    mesh.cluster @mesh3(rank = 2)

    // Used in the mesh sharding attribute to extend the standard tensor to
    // distributed
    tensor<4x8xf32, #mesh.shard<@mesh0, [[0]]>>
    ```
  }];
  let arguments = (ins
    SymbolNameAttr:$sym_name,
    I64Attr:$rank,
    DefaultValuedAttr<DenseI64ArrayAttr, "{}">:$dim_sizes
  );
  let assemblyFormat = [{
    $sym_name `(` `rank` `=` $rank (`,` `dim_sizes` `=` $dim_sizes^)? `)`
      attr-dict
  }];
  let extraClassDeclaration = [{
    ::mlir::SmallVector<int64_t> canonicalDimSizes();

    template <typename OutIt>
    void canonicalDimSizes(OutIt outIt) {
      std::copy(getDimSizes().begin(), getDimSizes().end(), outIt);
      std::fill_n(outIt, getRank() - getDimSizes().size(), 0);
    }
  }];
  let hasVerifier = 1;
}

def Mesh_ShardOp : Mesh_Op<"shard", [Pure, SameOperandsAndResultType]> {
  let summary = "Annotate on how a tensor is sharded across a mesh cluster.";
  let description = [{
    The mesh.shard operation is designed to specify and guide the sharding
    behavior of a tensor value across a mesh topology. This operation has one
    operand and two attributes:

    1. `input`: This operand represents the tensor value that needs to be
    annotated for sharding.

    2. `shard`: This attribute is type of `MeshSharding`, which is the core data
    structure to represent distributed tensor in mesh cluster.

    3. `annotate_for_users`: A unit attribute addressing the scenario when a
    tensor's sharding annotation differs based on its context of use (either as
    a result or an operand). If specified, the sharding pertains to specific
    users of the tensor value, indicating how it should be considered when used
    as an operand in subsequent operations. If not, the sharding applies to the
    operation that defines the tensor value.

    Example:
    ```
    func.func @only_result_annotated(%arg0 : tensor<4x8xf32>) -> () {
      %0 = mesh.shard %arg0 to <@mesh0, [[0]]> : tensor<4x8xf32>
      ...
    }

    func.func @only_operand_annotated(%arg0 : tensor<4x8xf32>) -> () {
      %0 = mesh.shard %arg0 to <@mesh0, [[0]]> annotate_for_users : tensor<4x8xf32>
      ...
    }

    // The first mesh.shard op applies to %arg0, the second mesh.shard op
    // applies for the operand of op0, the third mesh.shard op applies for the
    // operand of op2
    func.func @both_result_and_multi_operands_annotated(
        %arg0 : tensor<4x8xf32>) -> () {
      %0 = mesh.shard %arg0 to <@mesh0, [[0]]> : tensor<4x8xf32>
      %1 = mesh.shard %0 to <@mesh0, [[1]]> annotate_for_users : tensor<4x8xf32>
      %2 = mesh.shard %0 to <@mesh0, [[2]]> annotate_for_users : tensor<4x8xf32>
      "op0"(%1) : ...
      "op1"(%2) : ...
      ...
    }
    ```

    The following usages are undefined:
    ```
    func.func @annotate_on_same_result_with_different_sharding(
        %arg0 : tensor<4x8xf32>) -> () {
      %0 = mesh.shard %arg0 to <@mesh0, [[0]]> : tensor<4x8xf32>
      %1 = mesh.shard %0 to <@mesh0, [[1]]> : tensor<4x8xf32>
      ...
    }

    func.func @annotate_on_same_result_same_value_with_different_sharding(
        %arg0 : tensor<4x8xf32>) -> () {
      %0 = mesh.shard %arg0 to <@mesh0, [[0]]> : tensor<4x8xf32>
      %1 = mesh.shard %arg0 to <@mesh0, [[1]]> : tensor<4x8xf32>
      ...
    }

    func.func @annotate_on_same_operand_with_different_sharding(
        %arg0 : tensor<4x8xf32>) -> () {
      %0 = mesh.shard %arg0 to <@mesh0, [[0]]> annotate_for_users : tensor<4x8xf32>
      %1 = mesh.shard %0 to <@mesh0, [[1]]> annotate_for_users : tensor<4x8xf32>
      ...
    }

    func.func @result_annotated_after_operand(
        %arg0 : tensor<4x8xf32>) -> () {
      %0 = mesh.shard %arg0 to <@mesh0, [[0]]> annotate_for_users : tensor<4x8xf32>
      %1 = mesh.shard %0 to <@mesh0, [[1]]> : tensor<4x8xf32>
      ...
    }
    ```
  }];
  let arguments = (ins
    Builtin_RankedTensor:$src,
    MeshSharding:$shard,
    UnitAttr:$annotate_for_users
  );
  let results = (outs
    Builtin_RankedTensor:$result
  );
  let assemblyFormat = [{
    $src `to` $shard (`annotate_for_users` $annotate_for_users^)? attr-dict `:`
      type($result)
  }];
}

//===----------------------------------------------------------------------===//
// collective communication ops
//===----------------------------------------------------------------------===//

class Mesh_CollectiveCommunicationOpBase<
    string mnemonic, list<Trait> traits = []> :
    Mesh_Op<mnemonic,
      !listconcat(traits,
      [DeclareOpInterfaceMethods<SymbolUserOpInterface>])> {
  let assemblyFormat = "$input attr-dict `:` type($input) `->` type($result)";
  dag commonArgs = (ins
    FlatSymbolRefAttr:$mesh,
    DefaultValuedOptionalAttr<DenseI16ArrayAttr, "{}">:$mesh_axes
  );
}

def Mesh_AllGatherOp : Mesh_CollectiveCommunicationOpBase<"all_gather", [
    SameOperandsAndResultElementType,
    SameOperandsAndResultRank
  ]> {
  let summary = "All-gather over a device mesh.";
  let description = [{
    Gathers along the `gather_axis` tensor axis.
    The order of input tensors in the resulting tensor is the same as the
    order of the corresponding devices' multi-index in the mesh.

    Example:
    ```mlir
    mesh.cluster @mesh0(rank = 2, dim_sizes = [2, 2])
    ...
    %1 = mesh.all_gather %0 {
        mesh = @mesh0, mesh_axes = array<i16: 1>, gather_axis = 1 : index
      } : tensor<2x2xi8> -> tensor<2x4xi8>
    ```
    Input:
    ```
                     +-------+-------+
    device (0, 0) -> |  1  2 |  5  6 | <- device (0, 1)
                     |  3  4 |  7  8 |
                     +-------+-------+
    device (1, 0) -> |  9 10 | 13 14 | <- device (1, 1)
                     | 11 12 | 15 16 |
                     +-------+-------+
    ```
    Result:
    ```
    +-------------+
    |  1  2  5  6 | <- devices (0, 0) and (0, 1)
    |  3  4  7  8 |
    +-------------+
    |  9 10 13 14 | <- devices (1, 0) and (1, 1)
    | 11 12 15 16 |
    +-------------+
    ```
  }];
  let arguments = !con(commonArgs, (ins
    AnyNon0RankedTensor:$input,
    APIntAttr:$gather_axis
  ));
  let results = (outs
    AnyNon0RankedTensor:$result
  );
  let hasVerifier = 1;
}

def Mesh_AllReduceOp : Mesh_CollectiveCommunicationOpBase<"all_reduce", [
    SameOperandsAndResultShape]> {
  let summary = "All-reduce over a device mesh.";
  let description = [{
    The accumulation element type is specified by the result type and
    it does not need to match the input element type.
    The input element is converted to the result element type before
    performing the reduction.

    Attributes:
    `reduction`: Indicates the reduction method.

    Example:
    ```
    %1 = mesh.all_reduce %0 {
        mesh = @mesh0, mesh_axes = array<i16: 1, 0>, reduction = #mesh.partial<max>
      } : tensor<3x4xf32> -> tensor<3x4xf64>
    ```
  }];
  let arguments = !con(commonArgs, (ins
    AnyRankedTensor:$input,
    DefaultValuedOptionalAttr<Mesh_PartialAttr, "::mlir::mesh::Partial::Sum">:$reduction
  ));
  let results = (outs
    AnyRankedTensor:$result
  );
  let hasVerifier = 1;
}

def Mesh_AllToAllOp : Mesh_CollectiveCommunicationOpBase<"all_to_all", [
    SameOperandsAndResultElementType,
    SameOperandsAndResultRank]> {
  let summary = "All-to-all over a device mesh.";
  let description = [{
    Performs an all-to-all on tensor pieces split along `split_axis`.
    The resulting pieces are concatenated along `concat_axis` on ech device.
    Example:
    ```
    mesh.cluster @mesh0(rank = 1, dim_sizes = [3])
    ...
    %1 = mesh.all_to_all %0 {
        mesh = @mesh0, mesh_axes = array<i16: 0>, split_axis = 0, concat_axis = 0
      } : tensor<3x6xi8> -> tensor<3x6xi8>
    ```
    Input:
    ```
     device  device  device
     (0)     (1)     (2)
    +-------+-------+-------+
    | 11 12 | 21 22 | 31 32 |
    | 13 14 | 23 24 | 33 34 |
    | 15 16 | 25 26 | 35 36 |
    +-------+-------+-------+
    ```
    Result:
    ```
     device  device  device
     (0)     (1)     (2)
    +-------+-------+-------+
    | 11 12 | 13 14 | 15 16 |
    | 21 22 | 23 24 | 25 26 |
    | 31 32 | 33 34 | 35 36 |
    +-------+-------+-------+
    ```
  }];
  let arguments = !con(commonArgs, (ins
    AnyNon0RankedTensor:$input,
    APIntAttr:$split_axis,
    APIntAttr:$concat_axis
  ));
  let results = (outs
    AnyNon0RankedTensor:$result
  );
  let hasVerifier = 1;
}

def Mesh_ReduceScatterOp : Mesh_CollectiveCommunicationOpBase<"reduce_scatter", [
    SameOperandsAndResultRank]> {
  let summary = "Reduce-scatter over a device mesh.";
  let description = [{
    After the reduction scatters the result within each device group.
    The tensor is split along `scatter_axis` and the pieces distributed
    across the device group.
    Example:
    ```
    mesh.cluster @mesh0(rank = 1, dim_sizes = [2, 2])
    ...
    %1 = mesh.reduce_scatter %0 {
        mesh = @mesh0, mesh_axes = array<i16: 1>, reduction = #mesh.partial<max>, scatter_axis = 0
      } : tensor<3x4xf32> -> tensor<1x4xf64>
    ```
    Input:
    ```
                     +-------+-------+
    device (0, 0) -> |  1  2 |  5  6 | <- device (0, 1)
                     |  3  4 |  7  8 |
                     +-------+-------+
    device (1, 0) -> |  9 10 | 13 14 | <- device (1, 1)
                     | 11 12 | 15 16 |
                     +-------+-------+
    ```
    Result:
    ```
    +-------+
    |  6  8 | <- devices (0, 0)
    +-------+
    | 10 12 | <- devices (0, 1)
    +-------+
    | 22 24 | <- devices (1, 0)
    +-------+
    | 26 28 | <- devices (1, 1)
    +-------+
    ```
  }];
  let arguments = !con(commonArgs, (ins
    AnyNon0RankedTensor:$input,
    DefaultValuedOptionalAttr<Mesh_PartialAttr, "::mlir::mesh::Partial::Sum">:$reduction,
    APIntAttr:$scatter_axis
  ));
  let results = (outs
    AnyRankedTensor:$result
  );
  let hasVerifier = 1;
}

#endif // MLIR_DIALECT_MESH_IR_MESHOPS_TD
